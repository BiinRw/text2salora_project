"""
Monkey Patch PEFT LoRA Layer ä»¥æ”¯æŒ SaLoRA é£æ ¼çš„ç¡¬çº¦æŸ
ç›´æ¥ä¿®æ”¹ forward æ–¹æ³•ï¼Œåœ¨ LoRA è¾“å‡ºåæ·»åŠ  @ C^T æŠ•å½±
"""

import torch
import types
from typing import Optional


def patch_lora_linear_forward(lora_module, lora_C: Optional[torch.Tensor] = None):
    """
    Patch PEFT LoRA Linear çš„ forward æ–¹æ³•ï¼Œæ·»åŠ  SaLoRA é£æ ¼çš„æŠ•å½±
    
    ä¿®æ”¹é€»è¾‘:
    åŸå§‹: output = base(x) + lora_B(lora_A(x)) * scaling
    ä¿®æ”¹: output = base(x) + (lora_C @ (lora_B(lora_A(x)) * scaling)
    
    Args:
        lora_module: PEFT LoRA Linear æ¨¡å—
        lora_C: æŠ•å½±çŸ©é˜µ C (hidden_dim, hidden_dim)ï¼Œä¼šè‡ªåŠ¨è£å‰ªåˆ° (out_dim, out_dim)
    """
    # æ³¨å…¥ lora_C å±æ€§
    if lora_C is not None:
        # ğŸ”‘ å…³é”®ä¿®å¤: æ ¹æ®è¯¥å±‚çš„ out_dim è£å‰ª C çŸ©é˜µ
        # è·å–è¯¥å±‚çš„ out_dim
        out_dim = lora_module.base_layer.out_features
        hidden_dim = lora_C.shape[0]
        
        if out_dim == hidden_dim:
            # ç»´åº¦åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨
            # æ³¨å†Œä¸º buffer è€Œä¸æ˜¯æ™®é€šå±æ€§ï¼Œç¡®ä¿ä¸å‚ä¸æ¢¯åº¦è®¡ç®—
            #lora_module.register_buffer('lora_C', lora_C.clone(), persistent=False)
            lora_module.lora_C = lora_C.clone().detach()
            lora_module.lora_C.requires_grad_(False)
        elif out_dim < hidden_dim:
            # éœ€è¦è£å‰ªï¼šåªä½¿ç”¨å‰ out_dim ç»´åº¦
            # C_small = C[:out_dim, :out_dim]
            #lora_module.register_buffer('lora_C', lora_C[:out_dim, :out_dim].clone(), persistent=False)
            lora_module.lora_C = lora_C[:out_dim, :out_dim].clone().detach()
            lora_module.lora_C.requires_grad_(False)
            print(f"   ğŸ”§ è£å‰ª C: {hidden_dim}x{hidden_dim} â†’ {out_dim}x{out_dim}")
        else:
            # out_dim > hidden_dimï¼Œä¸åº”è¯¥å‘ç”Ÿ
            print(f"   âš ï¸  è­¦å‘Š: out_dim ({out_dim}) > hidden_dim ({hidden_dim})ï¼Œä½¿ç”¨å•ä½çŸ©é˜µ")
            #lora_module.register_buffer('lora_C', torch.eye(out_dim, dtype=lora_C.dtype, device=lora_C.device), persistent=False)
            # âœ… ç›´æ¥èµ‹å€¼å±æ€§ï¼Œä¸æ³¨å†Œ bufferï¼Œæ›´å®‰å…¨
            lora_module.lora_C = torch.eye(
                out_dim, dtype=lora_C.dtype, device=lora_C.device
            ).detach()
            lora_module.lora_C.requires_grad_(False)

        
    else:
        lora_module.lora_C = None
    
    # ä¿å­˜åŸå§‹ forward (å¦‚æœè¿˜æ²¡ä¿å­˜)
    if not hasattr(lora_module, '_original_lora_forward'):
        lora_module._original_lora_forward = lora_module.forward
    
    # å®šä¹‰æ–°çš„ forward æ–¹æ³•
    def patched_forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:
        """
        ä¿®æ”¹åçš„ forwardï¼Œæ”¯æŒ SaLoRA æŠ•å½±
        """
        # æ£€æŸ¥æ˜¯å¦ç¦ç”¨ adapter
        if self.disable_adapters:
            if self.merged:
                self.unmerge()
            result = self.base_layer(x, *args, **kwargs)
            return result
        
        # æ£€æŸ¥æ˜¯å¦å·²åˆå¹¶
        if self.merged:
            result = self.base_layer(x, *args, **kwargs)
            return result
        
        # æ­£å¸¸æƒ…å†µ: base + lora
        result = self.base_layer(x, *args, **kwargs)
        torch_result_dtype = result.dtype
        
        # éå†æ‰€æœ‰æ´»è·ƒçš„ adapter
        for active_adapter in self.active_adapters:
            if active_adapter not in self.lora_A.keys():
                continue
            
            lora_A = self.lora_A[active_adapter]
            lora_B = self.lora_B[active_adapter]
            dropout = self.lora_dropout[active_adapter]
            scaling = self.scaling[active_adapter]
            
            x_lora = x.to(lora_A.weight.dtype)
            
            # è®¡ç®— LoRA è¾“å‡º
            if not self.use_dora[active_adapter]:
                # æ ‡å‡† LoRA: lora_B(lora_A(dropout(x))) * scaling
                lora_output = lora_B(lora_A(dropout(x_lora))) * scaling
                
                # ğŸ”‘ å…³é”®ä¿®æ”¹: å¦‚æœæœ‰ lora_Cï¼ŒæŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´
                if hasattr(self, 'lora_C') and self.lora_C is not None:
                    out_dim = lora_output.size(-1)
                    lora_C = self.lora_C.detach().to(lora_output.device, dtype=lora_output.dtype)
                    C_block = lora_C

                    # âœ… è‡ªåŠ¨åŒ¹é… C çš„å°ºå¯¸åˆ° out_dim
                    if C_block.shape[0] != out_dim or C_block.shape[1] != out_dim:
                        if C_block.shape[0] > out_dim and C_block.shape[1] > out_dim:
                            # æ¯”å½“å‰å±‚å¤§ â†’ è£å‰ª
                            C_block = C_block[:out_dim, :out_dim]
                        elif C_block.shape[0] < out_dim or C_block.shape[1] < out_dim:
                            # æ¯”å½“å‰å±‚å° â†’ æ‰©å±•æˆ block å¯¹è§’é˜µï¼ˆé‡å¤Cï¼‰
                            repeat_factor = math.ceil(out_dim / C_block.shape[0])
                            C_block = torch.block_diag(*([C_block] * repeat_factor))[:out_dim, :out_dim]
                        else:
                            # å®Œå…¨ä¸åŒ¹é… â†’ å•ä½çŸ©é˜µ fallback
                            C_block = torch.eye(out_dim, device=C_block.device, dtype=C_block.dtype)

                    # âœ… å·¦ä¹˜ï¼ˆæ­£ç¡®æ–¹å‘ï¼‰
                    lora_output = torch.matmul(C_block, lora_output.T).T


                result = result + lora_output
            else:
                # DoRA
                x_lora = dropout(x_lora)
                result = result + self._apply_dora(x_lora, lora_A, lora_B, scaling, active_adapter)
        
        result = result.to(torch_result_dtype)
        return result
    
    # ç»‘å®šæ–°çš„ forward æ–¹æ³•
    lora_module.forward = types.MethodType(patched_forward, lora_module)


def inject_hard_constraint_to_model(
    model,
    lora_C: torch.Tensor,
    verbose: bool = True
) -> int:
    """
    ä¸ºæ¨¡å‹çš„æ‰€æœ‰ LoRA å±‚æ³¨å…¥ç¡¬çº¦æŸ
    
    Args:
        model: PEFT æ¨¡å‹
        lora_C: æŠ•å½±çŸ©é˜µ C (shape: hidden_dim, hidden_dim)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        patched_count: ä¿®æ”¹çš„å±‚æ•°
    """
    patched_count = 0
    
    if verbose:
        print(f"\nğŸ”§ æ³¨å…¥ SaLoRA ç¡¬çº¦æŸåˆ°æ¨¡å‹...")
        print(f"   æŠ•å½±çŸ©é˜µ C shape: {lora_C.shape}")
    
    for name, module in model.named_modules():
        # æŸ¥æ‰¾ PEFT LoRA Linear å±‚
        if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
            # Patch forward
            patch_lora_linear_forward(module, lora_C)
            patched_count += 1
            
            if verbose and patched_count <= 3:
                out_dim = module.base_layer.out_features
                print(f"   âœ“ {name}: out_dim={out_dim}, å·²æ³¨å…¥ lora_C")
    
    if verbose:
        print(f"   ğŸ“Š å…±ä¿®æ”¹ {patched_count} ä¸ª LoRA å±‚")
        print(f"   ğŸ”’ çº¦æŸ: LoRA è¾“å‡º @ C^T (ç¡¬çº¦æŸï¼Œ æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´)")
    
    return patched_count


if __name__ == '__main__':
    print("æµ‹è¯• PEFT LoRA Patch...")
    
    # æ¨¡æ‹Ÿæµ‹è¯•
    import torch.nn as nn
    
    class MockLoRALayer(nn.Module):
        def __init__(self, in_dim=1024, out_dim=1024):
            super().__init__()
            self.base_layer = nn.Linear(in_dim, out_dim)
            self.lora_A = {'default': nn.Linear(in_dim, 8, bias=False)}
            self.lora_B = {'default': nn.Linear(8, out_dim, bias=False)}
            self.lora_dropout = {'default': nn.Identity()}
            self.scaling = {'default': 1.0}
            self.active_adapters = ['default']
            self.disable_adapters = False
            self.merged = False
            self.use_dora = {'default': False}
        
        def forward(self, x):
            return self.base_layer(x) + self.lora_B['default'](
                self.lora_A['default'](x)
            ) * self.scaling['default']
    
    # åˆ›å»ºæµ‹è¯•å±‚
    layer = MockLoRALayer(in_dim=1024, out_dim=1024)
    
    # åˆ›å»ºæŠ•å½±çŸ©é˜µ
    C = torch.eye(1536)  # æ›´å¤§çš„çŸ©é˜µ
    
    # Patch
    patch_lora_linear_forward(layer, C)
    
    # æµ‹è¯•
    x = torch.randn(2, 10, 1024)
    output = layer(x)
    
    print(f"\nâœ… Patch æµ‹è¯•æˆåŠŸ")
    print(f"   è¾“å…¥ shape: {x.shape}")
    print(f"   è¾“å‡º shape: {output.shape}")
    print(f"   lora_C shape: {layer.lora_C.shape}")
    print(f"   lora_C å·²æ³¨å…¥: {hasattr(layer, 'lora_C')}")
