"""
LoRA SVD åˆå§‹åŒ–æ¨¡å—

å®ç° SaLoRA è®ºæ–‡ä¸­çš„åˆå§‹åŒ–æ–¹æ³•ï¼š
1. SVD åˆ†è§£åŸå§‹æƒé‡
2. æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´ï¼ˆæˆ–å­ç©ºé—´å†…ï¼‰
3. æ›´æ–°åŸºç¡€æƒé‡
"""

import torch
from typing import Optional
import sys


def initialize_lora_weights(
    model,
    constraint=None,
    rank: int = 16,
    method: str = 'random',
    niter: int = 30,
    verbose: bool = True
):
    """
    åˆå§‹åŒ– LoRA æƒé‡
    
    Args:
        model: PEFT LoRA æ¨¡å‹
        constraint: OrthogonalConstraint å¯¹è±¡ï¼ŒåŒ…å«å­ç©ºé—´æŠ•å½±çŸ©é˜µ C
        rank: LoRA çš„ç§©
        method: åˆå§‹åŒ–æ–¹æ³•
            - 'random': PEFT é»˜è®¤éšæœºåˆå§‹åŒ–ï¼ˆä¸åšä»»ä½•å¤„ç†ï¼‰
            - 'svd': PiSSA æ–¹æ³•ï¼ˆSVD åˆ†è§£ï¼Œä¸æŠ•å½±ï¼‰
            - 'svd_ortho': SVD + æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´ï¼ˆæ¨èï¼‰
            - 'svd_salora': SaLoRA åŸå§‹æ–¹æ³•ï¼ˆSVD + æŠ•å½±åˆ°å­ç©ºé—´å†…ï¼‰
        niter: SVD è¿­ä»£æ¬¡æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        åˆå§‹åŒ–çš„æ¨¡å—æ•°é‡
    """
    
    if method == 'random':
        if verbose:
            print("âœ… ä½¿ç”¨é»˜è®¤éšæœºåˆå§‹åŒ–")
        return 0
    
    if method in ['svd_ortho', 'svd_salora'] and constraint is None:
        raise ValueError(f"æ–¹æ³• '{method}' éœ€è¦æä¾› constraint å¯¹è±¡")
    
    if verbose:
        print(f"\nğŸ”§ ä½¿ç”¨ {method} æ–¹æ³•åˆå§‹åŒ– LoRA...")
        print(f"   Rank: {rank}, SVDè¿­ä»£æ¬¡æ•°: {niter}")
    
    initialized_count = 0
    
    for name, module in model.named_modules():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA æ¨¡å—
        if not (hasattr(module, 'lora_A') and hasattr(module, 'lora_B')):
            continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ base_layer
        if not hasattr(module, 'base_layer'):
            if verbose:
                print(f"âš ï¸ {name} æ²¡æœ‰ base_layerï¼Œè·³è¿‡")
            continue
        
        try:
            # è·å–åŸºç¡€æƒé‡
            base_weight = module.base_layer.weight.data
            d_out, d_in = base_weight.shape
            
            if verbose:
                print(f"\nå¤„ç†æ¨¡å—: {name}")
                print(f"  æƒé‡å½¢çŠ¶: {base_weight.shape}")
            
            # SVD åˆ†è§£
            if verbose:
                print(f"  æ‰§è¡Œ SVD åˆ†è§£ (rank={rank})...")
            
            U, S, V = torch.svd_lowrank(
                base_weight.float(), 
                q=min(rank, min(d_out, d_in)), 
                niter=niter
            )
            
            # è½¬æ¢å›åŸå§‹æ•°æ®ç±»å‹
            U = U.to(base_weight.dtype)
            S = S.to(base_weight.dtype)
            V = V.to(base_weight.dtype)
            
            # è®¡ç®— sqrt(S)
            sqrt_S = torch.sqrt(S)
            
            # åˆå§‹åŒ– B å’Œ A
            # B: (d_out, r)
            # A: (r, d_in)
            B_init = U @ torch.diag(sqrt_S)
            A_init = torch.diag(sqrt_S) @ V.T
            
            if verbose:
                print(f"  B_init å½¢çŠ¶: {B_init.shape}, A_init å½¢çŠ¶: {A_init.shape}")
            
            # æ ¹æ®æ–¹æ³•é€‰æ‹©æŠ•å½±æ–¹å¼
            if method == 'svd_ortho':
                # æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´: B' = (I - C) @ B
                if verbose:
                    print(f"  æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´...")
                
                # è·å–æŠ•å½±çŸ©é˜µ P = V @ V^T
                # OrthogonalConstraint.projection_matrices: {dimension: P} or {dimension: {layer_id: P}}
                dim = constraint.dimensions[0] if hasattr(constraint, 'dimensions') and constraint.dimensions else "safety"
                P_data = constraint.projection_matrices.get(dim) if hasattr(constraint, 'projection_matrices') else None
                
                # å¤„ç†åˆ†å±‚å’Œèåˆä¸¤ç§æƒ…å†µ
                if isinstance(P_data, dict):
                    # åˆ†å±‚çš„æƒ…å†µï¼šä»æ¨¡å—åæå–layer_id
                    import re
                    layer_match = re.search(r'\.layers\.(\d+)\.', name)
                    if layer_match:
                        layer_id = int(layer_match.group(1))
                        C = P_data.get(layer_id)
                    else:
                        C = None
                else:
                    # èåˆçš„æƒ…å†µï¼šç›´æ¥ä½¿ç”¨
                    C = P_data
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æŠ•å½±çŸ©é˜µï¼Œä½¿ç”¨å•ä½çŸ©é˜µï¼ˆç­‰ä»·äºä¸æŠ•å½±ï¼‰
                if C is None:
                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                else:
                    C = C.to(device=B_init.device, dtype=B_init.dtype)
                
                # ç¡®ä¿ C çš„å½¢çŠ¶ä¸ B_init åŒ¹é…
                if C.shape[0] != d_out:
                    if verbose:
                        print(f"  âš ï¸ C å½¢çŠ¶ {C.shape} ä¸ d_out {d_out} ä¸åŒ¹é…ï¼Œè·³è¿‡æŠ•å½±")
                else:
                    I_minus_C = torch.eye(d_out, device=C.device, dtype=C.dtype) - C
                    # SaLoRA åŸå§‹æ–¹æ³• B' = C @ Bï¼Œ æŠ•å½±åˆ°å®‰å…¨å­ç©ºé—´å†…éƒ¨
                    B_init = I_minus_C @ B_init
                    
                    if verbose:
                        print(f"  æŠ•å½±å B_init å½¢çŠ¶: {B_init.shape}")
            
            elif method == 'svd_salora':
                # SaLoRA åŸå§‹æ–¹æ³•ï¼šæŠ•å½±åˆ°å­ç©ºé—´å†…: B' = C @ B
                if verbose:
                    print(f"  æŠ•å½±åˆ°å­ç©ºé—´å†… (SaLoRA æ–¹æ³•)...")
                
                # è·å–æŠ•å½±çŸ©é˜µ P = V @ V^T
                # OrthogonalConstraint.projection_matrices: {dimension: P} or {dimension: {layer_id: P}}
                dim = constraint.dimensions[0] if hasattr(constraint, 'dimensions') and constraint.dimensions else "safety"
                P_data = constraint.projection_matrices.get(dim) if hasattr(constraint, 'projection_matrices') else None
                
                # å¤„ç†åˆ†å±‚å’Œèåˆä¸¤ç§æƒ…å†µ
                if isinstance(P_data, dict):
                    # åˆ†å±‚çš„æƒ…å†µï¼šä»æ¨¡å—åæå–layer_id
                    import re
                    layer_match = re.search(r'\.layers\.(\d+)\.', name)
                    if layer_match:
                        layer_id = int(layer_match.group(1))
                        C = P_data.get(layer_id)
                    else:
                        C = None
                else:
                    # èåˆçš„æƒ…å†µï¼šç›´æ¥ä½¿ç”¨
                    C = P_data
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æŠ•å½±çŸ©é˜µï¼Œä½¿ç”¨å•ä½çŸ©é˜µï¼ˆç­‰ä»·äºä¸æŠ•å½±ï¼‰
                if C is None:
                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                else:
                    C = C.to(device=B_init.device, dtype=B_init.dtype)
                
                if C.shape[0] != d_out:
                    if verbose:
                        print(f"  âš ï¸ C å½¢çŠ¶ {C.shape} ä¸ d_out {d_out} ä¸åŒ¹é…ï¼Œè·³è¿‡æŠ•å½±")
                else:
                    B_init = C @ B_init
                    
                    if verbose:
                        print(f"  æŠ•å½±å B_init å½¢çŠ¶: {B_init.shape}")
                    
                    # âœ… å…³é”®æ­¥éª¤ï¼šæ›´æ–° base_weight (ä¸åŸç‰ˆ SaLoRA ä¸€è‡´)
                    # åŸç†ï¼šä½¿ LoRA ä» 0 è´¡çŒ®å¼€å§‹
                    # è¾“å‡º = (W - B@A) @ x + B @ A @ x = W @ x
                    # æ³¨æ„ï¼šå› ä¸º C @ B = B (B åœ¨å­ç©ºé—´å†…)ï¼Œæ‰€ä»¥å®é™…å‡å» B @ A
                    base_layer = module.base_layer
                    if hasattr(base_layer, 'weight'):
                        with torch.no_grad():
                            # è®¡ç®—è¦å‡å»çš„éƒ¨åˆ†ï¼šB @ A
                            delta = (B_init @ A_init).to(base_layer.weight.dtype)
                            base_layer.weight.data.sub_(delta)
                            
                            if verbose:
                                print(f"  âœ… å·²æ›´æ–° base_weight: å‡å» B@A (å½¢çŠ¶: {delta.shape})")
                                print(f"     ç¡®ä¿åˆå§‹åŒ–åæ¨¡å‹è¾“å‡ºä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€è‡´")
            
            # èµ‹å€¼ç»™ LoRA å‚æ•°
            # æ³¨æ„ï¼šPEFT çš„ lora_B å’Œ lora_A çš„ weight éœ€è¦è½¬ç½®
            # lora_B: Linear(r, d_out) -> weight: (d_out, r)
            # lora_A: Linear(d_in, r) -> weight: (r, d_in)
            
            if 'default' in module.lora_A:
                adapter_name = 'default'
            else:
                adapter_name = list(module.lora_A.keys())[0]
            
            # B_init: (d_out, r) -> lora_B.weight: (d_out, r)
            # ä½¿ç”¨ with torch.no_grad() é¿å…åˆ›å»ºè®¡ç®—å›¾ï¼ŒåŒæ—¶ä¿æŒ requires_grad
            with torch.no_grad():
                module.lora_B[adapter_name].weight.copy_(B_init.detach())
            # ç¡®ä¿ requires_grad=True
            module.lora_B[adapter_name].weight.requires_grad_(True)
            
            # A_init: (r, d_in) -> lora_A.weight: (r, d_in)
            with torch.no_grad():
                module.lora_A[adapter_name].weight.copy_(A_init.detach())
            # ç¡®ä¿ requires_grad=True
            module.lora_A[adapter_name].weight.requires_grad_(True)
            
            if verbose:
                print(f"  âœ… å·²èµ‹å€¼ lora_A å’Œ lora_B")
            
            # æ›´æ–°åŸºç¡€æƒé‡: W' = W - B @ A
            # è¿™æ ·ä¿è¯ W' + B @ A = Wï¼ˆåˆå§‹è¾“å‡ºä¸å˜ï¼‰
            # è®¡ç®— BA ä¹˜ç§¯å¹¶ detachï¼Œç¡®ä¿ä¸å½±å“è®¡ç®—å›¾
            BA_product = (B_init @ A_init).detach()
            module.base_layer.weight.data.sub_(BA_product)
            
            if verbose:
                print(f"  âœ… å·²æ›´æ–°åŸºç¡€æƒé‡")
                # éªŒè¯
                reconstructed = module.base_layer.weight.data + BA_product
                error = torch.norm(reconstructed - base_weight) / torch.norm(base_weight)
                print(f"  é‡æ„è¯¯å·®: {error.item():.6f}")
                
                # éªŒè¯æ¢¯åº¦çŠ¶æ€
                print(f"  ğŸ“Š æ¢¯åº¦çŠ¶æ€:")
                print(f"     lora_B.requires_grad: {module.lora_B[adapter_name].weight.requires_grad}")
                print(f"     lora_A.requires_grad: {module.lora_A[adapter_name].weight.requires_grad}")
                print(f"     base_layer.requires_grad: {module.base_layer.weight.requires_grad}")
            
            initialized_count += 1
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– {name} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if verbose:
        print(f"\nâœ… LoRA åˆå§‹åŒ–å®Œæˆï¼å…±åˆå§‹åŒ– {initialized_count} ä¸ªæ¨¡å—")
    
    return initialized_count


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("LoRA SVD åˆå§‹åŒ–æ¨¡å—")
    print("æ”¯æŒçš„æ–¹æ³•:")
    print("  - random: é»˜è®¤éšæœºåˆå§‹åŒ–")
    print("  - svd: PiSSA æ–¹æ³•")
    print("  - svd_ortho: SVD + æ­£äº¤è¡¥æŠ•å½± (æ¨è)")
    print("  - svd_salora: SaLoRA åŸå§‹æ–¹æ³•")
