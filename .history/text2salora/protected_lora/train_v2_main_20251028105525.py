"""
ä¸»è®­ç»ƒæ–‡ä»¶ v2 - ä¿®å¤ç‰ˆ
åº”ç”¨äº†æ‰€æœ‰å‘ç°çš„ä¿®å¤:
1. æ¢¯åº¦é—®é¢˜ä¿®å¤
2. GPU é€‰æ‹©
3. å‘½ä»¤è¡Œæ¥å£
4. å¯é€‰çš„æ­£äº¤çº¦æŸ
"""

import torch
import torch.nn as nn
import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Union
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    TrainerCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from torch.utils.data import Dataset
import numpy as np
from training_monitor import create_training_callbacks
from dataset_loader import load_ultrafeedback_data

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
from protected_lora.orthogonal_constraints import (
    OrthogonalConstraint,
    collect_lora_AB_matrices
)
from protected_lora.peft_lora_patch import inject_hard_constraint_to_model


class SimpleDataset(Dataset):
    """ç®€å•æ–‡æœ¬æ•°æ®é›†"""
    def __init__(self, texts, tokenizer, max_length=512):
        self.encodings = tokenizer(
            texts,
            padding='max_length',
            truncation=True,
            max_length=max_length,
            return_tensors='pt'
        )
    
    def __len__(self):
        return len(self.encodings['input_ids'])
    
    def __getitem__(self, idx):
        return {
            'input_ids': self.encodings['input_ids'][idx],
            'attention_mask': self.encodings['attention_mask'][idx],
            'labels': self.encodings['input_ids'][idx]
        }


class OrthogonalLoRATrainer(Trainer):
    """æ”¯æŒæ­£äº¤çº¦æŸçš„ Trainer"""
    
    def __init__(
        self,
        constraint_calculator: Optional[OrthogonalConstraint] = None,
        lambda_orth: float = 0.1,
        dimension_weights: Optional[Dict[str, float]] = None,
        use_orthogonal: bool = False,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.constraint = constraint_calculator
        self.lambda_orth = lambda_orth
        self.dimension_weights = dimension_weights or {}
        self.use_orthogonal = use_orthogonal
        self.orth_loss_history = []
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """è®¡ç®—æŸå¤± = ä»»åŠ¡æŸå¤± + æ­£äº¤æŸå¤±"""

        # 1. è®¡ç®—ä»»åŠ¡æŸå¤±
        outputs = model(**inputs)

        if isinstance(outputs, dict):
            task_loss = outputs.get('loss')
        else:
            task_loss = outputs[0] if isinstance(outputs, tuple) else outputs.loss

        # 2. å¦‚æœä¸ä½¿ç”¨æ­£äº¤çº¦æŸ,ç›´æ¥è¿”å›
        if not self.use_orthogonal or self.constraint is None:
            return (task_loss, outputs) if return_outputs else task_loss

        # 3. è®¡ç®—æ­£äº¤æŸå¤±
        if model.training:
            lora_A, lora_B = collect_lora_AB_matrices(model)

            if len(lora_A) > 0:
                orth_loss, loss_details = self.constraint.compute_orthogonal_loss_efficient(
                    lora_A, lora_B,
                    lambda_orth=self.lambda_orth,
                    dimension_weights=self.dimension_weights
                )

                # è®°å½•
                self.orth_loss_history.append({
                    'step': self.state.global_step,
                    'task_loss': task_loss.item(),
                    'orth_loss': orth_loss.item(),
                    'details': loss_details
                })

                # æ€»æŸå¤±
                total_loss = task_loss + orth_loss
            else:
                total_loss = task_loss
        else:
            total_loss = task_loss

        return (total_loss, outputs) if return_outputs else total_loss

    def save_orth_loss_history(self, output_dir: str):
        """ä¿å­˜æ­£äº¤æŸå¤±å†å²"""
        if len(self.orth_loss_history) > 0:
            output_path = Path(output_dir) / 'orth_loss_history.json'
            with open(output_path, 'w') as f:
                json.dump(self.orth_loss_history, f, indent=2)
            print(f"âœ… æ­£äº¤æŸå¤±å†å²å·²ä¿å­˜: {output_path}")


def setup_lora_model(
    model_name: str,
    lora_rank: int = 8,
    lora_alpha: int = 16,
    lora_dropout: float = 0.1,
    target_modules: List[str] = None,
    device: str = 'cuda:0'
) -> tuple:
    """è®¾ç½® LoRA æ¨¡å‹
    
    Returns:
        (model, tokenizer)
    """
    if target_modules is None:
        target_modules = ['q_proj', 'v_proj', 'k_proj', 'o_proj']
    
    print(f"\nğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map=device,
        trust_remote_code=True
    )
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # é…ç½® LoRA
    print(f"\nğŸ”§ åº”ç”¨ LoRA (rank={lora_rank}, alpha={lora_alpha})")
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=lora_rank,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        init_lora_weights=True,
        bias='none'
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # âœ… ä¿®å¤: ç¡®ä¿ LoRA å‚æ•°éœ€è¦æ¢¯åº¦
    for name, param in model.named_parameters():
        if 'lora_' in name:
            param.requires_grad = True
    
    return model, tokenizer


def load_subspace_constraint(
    subspace_dir: str,
    dimensions: List[str],
    device: str = 'cuda:0',
    subspace_rank: int = None  # æ–°å¢: ä½¿ç”¨å­ç©ºé—´çš„å‰ k ä¸ªå‘é‡ï¼ŒNone=ä½¿ç”¨å…¨éƒ¨
) -> OrthogonalConstraint:
    """åŠ è½½å­ç©ºé—´çº¦æŸã€‚æ”¯æŒ .pt å’Œ .npy æ–‡ä»¶æ ¼å¼ã€‚
    ä¼˜å…ˆåŠ è½½ fused å­ç©ºé—´æ–‡ä»¶ï¼ˆå¦‚ safety_fused_subspace.ptï¼‰ã€‚
    
    Args:
        subspace_dir: å­ç©ºé—´æ–‡ä»¶ç›®å½•
        dimensions: åå¥½ç»´åº¦åˆ—è¡¨
        device: è®¾å¤‡
        subspace_rank: ä½¿ç”¨å­ç©ºé—´çš„å‰ k ä¸ªå‘é‡ï¼ˆNone=ä½¿ç”¨å…¨éƒ¨ï¼‰
    """
    from pathlib import Path
    
    print(f"\nğŸ“Š åŠ è½½åå¥½å­ç©ºé—´: {dimensions}")
    print(f"   å­ç©ºé—´ç›®å½•: {subspace_dir}")
    
    # åŠ è½½å­ç©ºé—´çŸ©é˜µ
    subspace_V = {}
    
    for dim in dimensions:
        # å°è¯•å¤šç§æ–‡ä»¶æ ¼å¼
        candidates = [
            Path(subspace_dir) / f"{dim}_fused_subspace.pt",  # PyTorch æ ¼å¼ï¼ˆä¼˜å…ˆï¼‰
            Path(subspace_dir) / f"{dim}.pt",
            Path(subspace_dir) / f"{dim}_subspace.pt",
            Path(subspace_dir) / f"{dim}.npy",  # NumPy æ ¼å¼
            Path(subspace_dir) / f"{dim}_V.npy",
            Path(subspace_dir) / f"{dim}_subspace.npy",
        ]
        
        loaded = False
        for p in candidates:
            if p.exists():
                print(f"   æ‰¾åˆ°æ–‡ä»¶: {p.name}")
                try:
                    if p.suffix == '.pt':
                        # åŠ è½½ PyTorch æ–‡ä»¶
                        V_tensor = torch.load(p, map_location=device)
                        if isinstance(V_tensor, dict):
                            # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æå–å­ç©ºé—´çŸ©é˜µ
                            if 'subspace' in V_tensor:
                                V_tensor = V_tensor['subspace']
                            elif 'V' in V_tensor:
                                V_tensor = V_tensor['V']
                            else:
                                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼
                                V_tensor = list(V_tensor.values())[0]
                        V_tensor = V_tensor.to(device)
                    elif p.suffix == '.npy':
                        # åŠ è½½ NumPy æ–‡ä»¶
                        arr = np.load(p)
                        if arr.ndim == 1:
                            arr = arr[:, None]
                        V_tensor = torch.from_numpy(arr).to(device)
                    else:
                        continue
                    
                    # ç¡®ä¿æ˜¯ 2D å¼ é‡
                    if V_tensor.ndim == 1:
                        V_tensor = V_tensor.unsqueeze(1)
                    
                    # æˆªæ–­å­ç©ºé—´ï¼ˆå¦‚æœæŒ‡å®šäº† subspace_rankï¼‰
                    if subspace_rank is not None and V_tensor.shape[1] > subspace_rank:
                        original_rank = V_tensor.shape[1]
                        V_tensor = V_tensor[:, :subspace_rank]
                        print(f"   ğŸ“Š æˆªæ–­å­ç©ºé—´: {original_rank} â†’ {subspace_rank}")
                    
                    subspace_V[dim] = V_tensor
                    print(f"   âœ… {dim}: shape={V_tensor.shape}")
                    loaded = True
                    break
                    
                except Exception as e:
                    print(f"   âš ï¸ åŠ è½½ {p} å¤±è´¥: {e}")
                    continue
        
        if not loaded:
            raise FileNotFoundError(
                f"æ— æ³•æ‰¾åˆ°å­ç©ºé—´æ–‡ä»¶ for dimension '{dim}' in {subspace_dir}.\n"
                f"å°è¯•çš„æ–‡ä»¶: {[str(c) for c in candidates]}"
            )
    
    # åˆ›å»ºæ­£äº¤çº¦æŸ
    # åˆ›å»º PreferenceSubspaceManager
    from utils.svd_utils import PreferenceSubspaceManager
    manager = PreferenceSubspaceManager(subspace_dir=subspace_dir, device=device)
    
    # å°†åŠ è½½çš„å­ç©ºé—´çŸ©é˜µæ”¾å…¥ manager
    for dim in dimensions:
        manager.subspaces[dim] = {'fused': subspace_V[dim]}
    
    # åˆ›å»ºæ­£äº¤çº¦æŸ
    constraint = OrthogonalConstraint(
        subspace_manager=manager,
        dimensions=dimensions,
        device=device
    )
    
    print(f"âœ… å­ç©ºé—´åŠ è½½å®Œæˆ")
    return constraint

def train(
    model_name: str,
    train_texts: List[str] = None,
    eval_texts: List[str] = None,
    
    # æ•°æ®é›†é…ç½®
    dataset_type: str = 'demo',
    dataset_size: str = '100',
    data_format: str = 'instruction',
    max_samples: int = None,
    output_dir: str = './output/train',
    
    # GPU & èµ„æº
    gpu_id: int = 1,
    
    # LoRA é…ç½®
    lora_rank: int = 8,
    lora_alpha: int = 16,
    lora_dropout: float = 0.1,
    target_modules: List[str] = None,
    
    # è®­ç»ƒé…ç½®
    num_epochs: int = 3,
    batch_size: int = 2,
    gradient_accumulation: int = 4,
    learning_rate: float = 1e-4,
    max_length: int = 512,
    use_gradient_checkpointing: bool = False,
    
    # æ­£äº¤çº¦æŸ
    use_orthogonal: bool = False,
    use_hard_constraint: bool = False,  # ğŸ”‘ æ–°å¢: ä½¿ç”¨ç¡¬çº¦æŸï¼ˆSaLoRA é£æ ¼ï¼‰
    subspace_dir: str = None,
    preference_dimensions: List[str] = None,
    lambda_orth: float = 0.1,
    dimension_weights: Dict[str, float] = None,
    subspace_rank: int = None,  # æ–°å¢: ä½¿ç”¨å­ç©ºé—´çš„å‰ k ä¸ªå‘é‡ï¼ˆNone=ä½¿ç”¨å…¨éƒ¨ï¼‰
    
    # è®­ç»ƒç›‘æ§
    use_swanlab: bool = True,
    swanlab_project: str = 'protected-lora',
    experiment_name: str = None,
    print_interval: int = 10,
    enable_console_logging: bool = True
):
    """ä¸»è®­ç»ƒå‡½æ•°
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        train_texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
        eval_texts: éªŒè¯æ–‡æœ¬åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        gpu_id: GPU ID
        
        LoRA é…ç½®:
        lora_rank: LoRA rank
        lora_alpha: LoRA alpha
        lora_dropout: LoRA dropout
        target_modules: ç›®æ ‡æ¨¡å—
        
        è®­ç»ƒé…ç½®:
        num_epochs: è®­ç»ƒè½®æ•°
        batch_size: batch size
        gradient_accumulation: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate: å­¦ä¹ ç‡
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        use_gradient_checkpointing: æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        
        æ­£äº¤çº¦æŸ:
        use_orthogonal: æ˜¯å¦ä½¿ç”¨æ­£äº¤çº¦æŸ
        subspace_dir: å­ç©ºé—´ç›®å½•
        preference_dimensions: åå¥½ç»´åº¦åˆ—è¡¨
        lambda_orth: æ­£äº¤æŸå¤±ç³»æ•°
        dimension_weights: ç»´åº¦æƒé‡
    """
    
    print("="*80)


    print("ğŸš€ å¼€å§‹è®­ç»ƒ - ä¸»è®­ç»ƒæ–‡ä»¶ v2")
    print("="*80)
    
    # âœ… ä¿®å¤: GPU é€‰æ‹©
    os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
    device = 'cuda:0'
    
    print(f"\nğŸ“Š é…ç½®:")
    print(f"  æ¨¡å‹: {model_name}")
    print(f"  GPU: {gpu_id}")
    if train_texts is not None:
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_texts)}")
    else:
        print(f"  æ•°æ®é›†ç±»å‹: {dataset_type}")
    print(f"  LoRA: rank={lora_rank}, alpha={lora_alpha}")
    print(f"  è®­ç»ƒ: epochs={num_epochs}, batch={batch_size}, lr={learning_rate}")
    if use_hard_constraint:
        print(f"  çº¦æŸæ¨¡å¼: ğŸ”’ ç¡¬çº¦æŸ (SaLoRA é£æ ¼ï¼Œè¡¨å¾ç©ºé—´)")
    elif use_orthogonal:
        print(f"  çº¦æŸæ¨¡å¼: ğŸ“Š è½¯çº¦æŸ (å‚æ•°ç©ºé—´ï¼Œlambda={lambda_orth})")
    else:
        print(f"  çº¦æŸæ¨¡å¼: âŒ æ— çº¦æŸ")
    
    # 1. è®¾ç½®æ¨¡å‹
    model, tokenizer = setup_lora_model(
        model_name=model_name,
        lora_rank=lora_rank,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        device=device
    )
    
    # 2. å‡†å¤‡æ•°æ®
    print(f"\nğŸ“š å‡†å¤‡æ•°æ®é›†")
    
    if dataset_type == 'ultrafeedback':
        # ä½¿ç”¨ UltraFeedback æ•°æ®é›†
        print(f"  ç±»å‹: UltraFeedback")
        print(f"  å¤§å°: {dataset_size}")
        print(f"  æ ¼å¼: {data_format}")
        
        from dataset_loader import load_ultrafeedback_data
        
        # åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        train_dataset, eval_dataset = load_ultrafeedback_data(
            dataset_size=dataset_size,
            tokenizer=tokenizer,
            max_length=max_length,
            use_chosen_only=True,
            format_type=data_format,
            split="both"
        )
        
        # é™åˆ¶æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        if max_samples and max_samples < len(train_dataset):
            print(f"  âš ï¸  é™åˆ¶è®­ç»ƒæ ·æœ¬: {max_samples}/{len(train_dataset)}")
            train_dataset.data = train_dataset.data[:max_samples]
        
    else:
        # ä½¿ç”¨ç®€å•æ–‡æœ¬æ•°æ®
        if train_texts is None:
            raise ValueError("demo æ¨¡å¼éœ€è¦æä¾› train_texts")
        train_dataset = SimpleDataset(train_texts, tokenizer, max_length)
        eval_dataset = SimpleDataset(eval_texts, tokenizer, max_length) if eval_texts else None
    print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    if eval_dataset:
        print(f"âœ… éªŒè¯é›†: {len(eval_dataset)} æ ·æœ¬")
    
    # 3. åŠ è½½çº¦æŸ (è½¯çº¦æŸæˆ–ç¡¬çº¦æŸ)
    constraint = None
    
    if use_hard_constraint:
        # ğŸ”’ ç¡¬çº¦æŸæ¨¡å¼ (SaLoRA é£æ ¼)
        if subspace_dir is None or preference_dimensions is None:
            print("âš ï¸  è­¦å‘Š: use_hard_constraint=True ä½†æœªæä¾› subspace_dir æˆ– preference_dimensions")
            print("âš ï¸  å°†å…³é—­ç¡¬çº¦æŸ")
            use_hard_constraint = False
        else:
            print(f"\nğŸ”’ åŠ è½½ç¡¬çº¦æŸ (SaLoRA é£æ ¼)...")
            # åŠ è½½å­ç©ºé—´
            constraint = load_subspace_constraint(
                subspace_dir=subspace_dir,
                dimensions=preference_dimensions,
                device=device,
                subspace_rank=subspace_rank
            )
            
            # è®¡ç®—æŠ•å½±çŸ©é˜µ C = V @ V^T
            C_combined = None
            for dim in preference_dimensions:
                V = constraint.manager.get_subspace(dim, layer_id=None)
                C = V @ V.T  # (hidden_dim, hidden_dim)
                if C_combined is None:
                    C_combined = C
                else:
                    C_combined = C_combined @ C  # å¤šä¸ªç»´åº¦å–äº¤é›†
            
            # æ³¨å…¥ç¡¬çº¦æŸåˆ°æ¨¡å‹
            patched_count = inject_hard_constraint_to_model(
                model=model,
                lora_C=C_combined,
                verbose=True
            )
            
            print(f"âœ… ç¡¬çº¦æŸæ³¨å…¥å®Œæˆ: {patched_count} ä¸ª LoRA å±‚")
            print(f"   çº¦æŸå…¬å¼: output = base(x) + (LoRA(x) @ C^T)")
            print(f"   C = V @ V^T (å›ºå®šä¸è®­ç»ƒ)")
            
            # ç¡¬çº¦æŸæ¨¡å¼ä¸‹ä¸éœ€è¦è½¯çº¦æŸ
            use_orthogonal = False
            constraint = None
    
    elif use_orthogonal:
        # ğŸ“Š è½¯çº¦æŸæ¨¡å¼ (å‚æ•°ç©ºé—´)
        if subspace_dir is None or preference_dimensions is None:
            print("âš ï¸  è­¦å‘Š: use_orthogonal=True ä½†æœªæä¾› subspace_dir æˆ– preference_dimensions")
            print("âš ï¸  å°†å…³é—­æ­£äº¤çº¦æŸ")
            use_orthogonal = False
        else:
            constraint = load_subspace_constraint(
                subspace_dir=subspace_dir,
                dimensions=preference_dimensions,
                device=device,
                subspace_rank=subspace_rank
            )
    
    # 4. è®¾ç½®è®­ç»ƒå‚æ•°
    print(f"\nâš™ï¸  è®¾ç½®è®­ç»ƒå‚æ•°")
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation,
        learning_rate=learning_rate,
        lr_scheduler_type='constant',  # ä½¿ç”¨æ’å®šå­¦ä¹ ç‡ï¼Œé¿å…çº¦æŸå¤±æ•ˆ
        warmup_steps=50,
        logging_steps=10,
        save_steps=100,
        eval_strategy='steps' if eval_dataset else 'no',
        eval_steps=100 if eval_dataset else None,
        save_total_limit=2,
        fp16=True,
        gradient_checkpointing=use_gradient_checkpointing,  # âœ… å¯é…ç½®
        report_to='none',
        remove_unused_columns=False
    )
    
    # 5. åˆ›å»º Trainer
    print(f"\nğŸ‹ï¸  åˆ›å»º Trainer")

    # å‡†å¤‡ SwanLab é…ç½®
    swanlab_config = {
        'model_name': model_name,
        'lora_rank': lora_rank,
        'lora_alpha': lora_alpha,
        'learning_rate': learning_rate,
        'batch_size': batch_size,
        'num_epochs': num_epochs,
        'use_orthogonal': use_orthogonal,
        'lambda_orth': lambda_orth if use_orthogonal else None,
        'gpu_id': gpu_id,
    }
    trainer = OrthogonalLoRATrainer(
        constraint_calculator=constraint,
        lambda_orth=lambda_orth,
        dimension_weights=dimension_weights,
        use_orthogonal=use_orthogonal,
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
    )
    
    # 6. è®­ç»ƒ

    # æ·»åŠ è®­ç»ƒç›‘æ§å›è°ƒ
    callbacks = create_training_callbacks(
        trainer=trainer,
        use_swanlab=use_swanlab,
        swanlab_project=swanlab_project,
        swanlab_experiment=experiment_name,
        swanlab_config=swanlab_config,
        print_interval=print_interval,
        enable_console_logging=enable_console_logging,
        monitor_orth_loss=use_orthogonal,
    )
    
    for callback in callbacks:
        trainer.add_callback(callback)


    trainer.train()
    
    # 7. ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹")
    trainer.save_model(output_dir)
    if use_orthogonal:
        trainer.save_orth_loss_history(output_dir)
    
    print(f"\n" + "="*80)
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("="*80)
    
    return trainer


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description='ä¸»è®­ç»ƒæ–‡ä»¶ v2')
    
    # åŸºç¡€é…ç½®
    parser.add_argument('--model_name', type=str, default='Qwen/Qwen2.5-1.5B-Instruct',
                        help='æ¨¡å‹åç§°æˆ–è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./output/train_v2',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--gpu_id', type=int, default=1,
                        help='GPU ID')
    
    # LoRA é…ç½®
    parser.add_argument('--lora_rank', type=int, default=8,
                        help='LoRA rank')
    parser.add_argument('--lora_alpha', type=int, default=16,
                        help='LoRA alpha')
    parser.add_argument('--lora_dropout', type=float, default=0.1,
                        help='LoRA dropout')
    
    # è®­ç»ƒé…ç½®
    parser.add_argument('--num_epochs', type=int, default=10,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=2,
                        help='Batch size')
    parser.add_argument('--gradient_accumulation', type=int, default=4,
                        help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--max_length', type=int, default=512,
                        help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--use_gradient_checkpointing', action='store_true',
                        help='ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹')
    
    # æ­£äº¤çº¦æŸ
    parser.add_argument('--use_orthogonal', action='store_true',
                        help='ä½¿ç”¨è½¯çº¦æŸï¼ˆå‚æ•°ç©ºé—´ï¼‰')
    parser.add_argument('--use_hard_constraint', action='store_true',
                        help='ğŸ”‘ ä½¿ç”¨ç¡¬çº¦æŸï¼ˆSaLoRA é£æ ¼ï¼Œè¡¨å¾ç©ºé—´ï¼‰')
    parser.add_argument('--subspace_dir', type=str,
                        default='../preference_subspace/output/qwen2.5_1.5b',
                        help='å­ç©ºé—´ç›®å½•')
    parser.add_argument('--preference_dimensions', type=str, nargs='+',
                        default=['safety', 'helpfulness'],
                        help='åå¥½ç»´åº¦')
    parser.add_argument('--lambda_orth', type=float, default=0.01,
                        help='æ­£äº¤æŸå¤±ç³»æ•°')
    parser.add_argument('--subspace_rank', type=int, default=None,
                        help='ä½¿ç”¨å­ç©ºé—´çš„å‰ k ä¸ªå‘é‡ï¼ˆNone=ä½¿ç”¨å…¨éƒ¨ï¼‰')
    
    # æ•°æ®
    parser.add_argument('--use_demo_data', action='store_true', default=True,
                        help='ä½¿ç”¨æ¼”ç¤ºæ•°æ®')

    # è®­ç»ƒç›‘æ§å‚æ•°
    
    # æ•°æ®é›†é…ç½®
    parser.add_argument('--dataset_type', type=str, default='demo',
                        choices=['demo', 'ultrafeedback'],
                        help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--dataset_size', type=str, default='100',
                        choices=['100', '1k', '3k', '1w', 'full'],
                        help='UltraFeedback æ•°æ®é›†å¤§å°')
    parser.add_argument('--data_format', type=str, default='instruction',
                        choices=['instruction', 'conversation'],
                        help='æ•°æ®æ ¼å¼åŒ–ç±»å‹')
    parser.add_argument('--max_samples', type=int, default=None,
                        help='é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')

    parser.add_argument('--use_swanlab', type=lambda x: x.lower() == 'true', default=True,
                        help='ä½¿ç”¨ SwanLab è¿›è¡Œå®éªŒè¿½è¸ª (true/false)')
    parser.add_argument('--swanlab_project', type=str, default='protected-lora',
                        help='SwanLab é¡¹ç›®åç§°')
    parser.add_argument('--experiment_name', type=str, default=None,
                        help='è®­ç»ƒå®éªŒåç§°ï¼Œç”¨äº SwanLab è®°å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--print_interval', type=int, default=10,
                        help='ç»ˆç«¯æ‰“å°é—´éš”ï¼ˆæ­¥æ•°ï¼‰')
    parser.add_argument('--disable_console_log', action='store_true',
                        help='ç¦ç”¨ç»ˆç«¯è¯¦ç»†æ—¥å¿—')
    
    args = parser.parse_args()
    
    # å‡†å¤‡æ•°æ®
    # å‡†å¤‡æ•°æ®
    if args.dataset_type == 'demo':
        print("ğŸ“š ä½¿ç”¨æ¼”ç¤ºæ•°æ®")
        train_texts = [
            "What is machine learning? Machine learning is a subset of artificial intelligence.",
            "Explain neural networks. Neural networks are computing systems inspired by biology.",
            "What is deep learning? Deep learning uses neural networks with multiple layers.",
            "Describe NLP. Natural language processing helps computers understand human language.",
            "What is reinforcement learning? RL trains agents through rewards and penalties.",
            "Explain computer vision. CV enables computers to derive information from images.",
            "What is supervised learning? Supervised learning uses labeled training data.",
            "Describe unsupervised learning. Unsupervised learning finds patterns in unlabeled data.",
        ] * 20  # 160 æ ·æœ¬
        eval_texts = train_texts[:10]
    
    elif args.dataset_type == 'ultrafeedback':
        print(f"ğŸ“š åŠ è½½ UltraFeedback æ•°æ®é›† (å¤§å°: {args.dataset_size})")
        
        # è¿™é‡Œå…ˆä¸å®é™…åŠ è½½ï¼Œåœ¨ train() å‡½æ•°ä¸­åŠ è½½
        # å› ä¸ºéœ€è¦ tokenizer
        train_texts = None  # æ ‡è®°ä¸ºä½¿ç”¨æ•°æ®é›†
        eval_texts = None
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {args.dataset_type}")
    
    # è®­ç»ƒ
    trainer = train(
        model_name=args.model_name,
        train_texts=train_texts,
        eval_texts=eval_texts,
        
        # æ•°æ®é›†é…ç½®
        dataset_type=args.dataset_type,
        dataset_size=args.dataset_size,
        data_format=args.data_format,
        max_samples=args.max_samples,
        
        output_dir=args.output_dir,
        gpu_id=args.gpu_id,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        gradient_accumulation=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        max_length=args.max_length,
        use_gradient_checkpointing=args.use_gradient_checkpointing,
        use_orthogonal=args.use_orthogonal,
        use_hard_constraint=args.use_hard_constraint,  # ğŸ”‘ æ–°å¢
        subspace_dir=args.subspace_dir if (args.use_orthogonal or args.use_hard_constraint) else None,
        preference_dimensions=args.preference_dimensions if (args.use_orthogonal or args.use_hard_constraint) else None,
        lambda_orth=args.lambda_orth,
        subspace_rank=args.subspace_rank if (args.use_orthogonal or args.use_hard_constraint) else None,
        # è®­ç»ƒç›‘æ§
        experiment_name=args.experiment_name,
        use_swanlab=args.use_swanlab,
        swanlab_project=args.swanlab_project,
        print_interval=args.print_interval,
        enable_console_logging=not args.disable_console_log
    )
    
    print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    print(f"  æ€»æ­¥æ•°: {trainer.state.global_step}")
    if trainer.state.log_history:
        print(f"  æœ€ç»ˆæŸå¤±: {trainer.state.log_history[-1].get('loss', 'N/A')}")


if __name__ == '__main__':
    main()