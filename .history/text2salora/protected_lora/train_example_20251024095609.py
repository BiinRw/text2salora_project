"""
å®Œæ•´çš„è®­ç»ƒç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ­£äº¤çº¦æŸè®­ç»ƒ LoRA
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from utils.svd_utils import PreferenceSubspaceManager
from protected_lora.orthogonal_constraints import OrthogonalConstraint
from protected_lora.train_lora_orthogonal import OrthogonalLoRATrainer


def main():
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # é…ç½®å‚æ•°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    MODEL_PATH = 'Qwen/Qwen2.5-1.5B-Instruct'
    SUBSPACE_DIR = '../preference_subspace/saved_subspaces'
    OUTPUT_DIR = './output/lora_with_orthogonal_constraint'
    
    # åå¥½çº¦æŸé…ç½®
    PREFERENCE_DIMENSIONS = ['safety', 'helpfulness']  # è¦ä¿æŠ¤çš„åå¥½
    LAMBDA_ORTH = 0.1  # æ­£äº¤çº¦æŸç³»æ•° (0.01~0.5)
    DIMENSION_WEIGHTS = {
        'safety': 1.0,      # safety æƒé‡æœ€é«˜
        'helpfulness': 0.5  # helpfulness æƒé‡è¾ƒä½
    }
    
    # LoRA é…ç½®
    LORA_CONFIG = {
        'rank': 8,
        'alpha': 16,
        'dropout': 0.1,
        'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj']
    }
    
    print("=" * 70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ: æ­£äº¤çº¦æŸ LoRA")
    print("=" * 70)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 1: åŠ è½½åå¥½å­ç©ºé—´
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ“¦ æ­¥éª¤ 1: åŠ è½½åå¥½å­ç©ºé—´")
    
    manager = PreferenceSubspaceManager(
        subspace_dir=SUBSPACE_DIR,
        device='cuda:1'
    )
    
    manager.load_all_dimensions(
        dimensions=PREFERENCE_DIMENSIONS,
        use_fused=True  # ä½¿ç”¨èåˆå­ç©ºé—´
    )
    
    manager.print_info()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 2: åˆ›å»ºæ­£äº¤çº¦æŸè®¡ç®—å™¨
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ”§ æ­¥éª¤ 2: åˆ›å»ºæ­£äº¤çº¦æŸè®¡ç®—å™¨")
    
    constraint = OrthogonalConstraint(
        subspace_manager=manager,
        dimensions=PREFERENCE_DIMENSIONS,
        use_fused=True,
        device='cuda:0'
    )
    
    print(f"âœ… æ­£äº¤çº¦æŸè®¡ç®—å™¨åˆ›å»ºå®Œæˆ")
    print(f"   â€¢ çº¦æŸç»´åº¦: {PREFERENCE_DIMENSIONS}")
    print(f"   â€¢ Lambda: {LAMBDA_ORTH}")
    print(f"   â€¢ ç»´åº¦æƒé‡: {DIMENSION_WEIGHTS}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 3: åŠ è½½æ¨¡å‹å’Œ tokenizer
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ¤– æ­¥éª¤ 3: åŠ è½½æ¨¡å‹å’Œ tokenizer")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,
        device_map='cuda:0'
    )
    
    print(f"âœ… åŸºåº§æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 4: åº”ç”¨ LoRA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ”„ æ­¥éª¤ 4: åº”ç”¨ LoRA")
    
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_CONFIG['rank'],
        lora_alpha=LORA_CONFIG['alpha'],
        lora_dropout=LORA_CONFIG['dropout'],
        target_modules=LORA_CONFIG['target_modules'],
        bias='none'
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 5: å‡†å¤‡è®­ç»ƒæ•°æ®
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ“š æ­¥éª¤ 5: å‡†å¤‡è®­ç»ƒæ•°æ®")
    
    # è¿™é‡Œéœ€è¦ä½ è‡ªå·±å‡†å¤‡æ•°æ®
    # ç¤ºä¾‹: ä½¿ç”¨ Hugging Face æ•°æ®é›†
    # dataset = load_dataset('your_dataset_name')
    
    # ä¸´æ—¶ç¤ºä¾‹: åˆ›å»ºä¸€ä¸ªå°çš„æµ‹è¯•æ•°æ®é›†
    print("âš ï¸  è­¦å‘Š: ä½¿ç”¨æµ‹è¯•æ•°æ®é›†,å®é™…è®­ç»ƒéœ€è¦æ›¿æ¢ä¸ºçœŸå®æ•°æ®")
    
    test_texts = [
        "What is artificial intelligence?",
        "Explain machine learning.",
        "How does a neural network work?"
    ] * 10  # é‡å¤ä»¥å¢åŠ æ•°æ®é‡
    
    def tokenize_function(examples):
        return tokenizer(
            examples,
            padding='max_length',
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
    
    # åˆ›å»ºç®€å•æ•°æ®é›†
    from torch.utils.data import Dataset
    
    class SimpleDataset(Dataset):
        def __init__(self, texts, tokenizer):
            self.encodings = tokenizer(
                texts,
                padding='max_length',
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )
        
        def __len__(self):
            return len(self.encodings['input_ids'])
        
        def __getitem__(self, idx):
            return {
                'input_ids': self.encodings['input_ids'][idx],
                'attention_mask': self.encodings['attention_mask'][idx],
                'labels': self.encodings['input_ids'][idx]
            }
    
    train_dataset = SimpleDataset(test_texts, tokenizer)
    eval_dataset = SimpleDataset(test_texts[:5], tokenizer)
    
    print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ")
    print(f"   â€¢ è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"   â€¢ éªŒè¯æ ·æœ¬: {len(eval_dataset)}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 6: è®¾ç½®è®­ç»ƒå‚æ•°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nâš™ï¸  æ­¥éª¤ 6: è®¾ç½®è®­ç»ƒå‚æ•°")
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=2,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=5e-5,
        warmup_steps=50,
        logging_steps=5,
        save_steps=50,
        eval_steps=50,
        evaluation_strategy='steps',
        save_total_limit=2,
        fp16=True,
        max_grad_norm=1.0,
        report_to='none'  # ä¸ä¸ŠæŠ¥åˆ° wandb ç­‰
    )
    
    print(f"âœ… è®­ç»ƒå‚æ•°è®¾ç½®å®Œæˆ")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 7: åˆ›å»º Trainer å¹¶å¼€å§‹è®­ç»ƒ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸƒ æ­¥éª¤ 7: åˆ›å»º Trainer å¹¶å¼€å§‹è®­ç»ƒ")
    
    trainer = OrthogonalLoRATrainer(
        constraint_calculator=constraint,
        lambda_orth=LAMBDA_ORTH,
        dimension_weights=DIMENSION_WEIGHTS,
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False
        )
    )
    
    print(f"âœ… Trainer åˆ›å»ºå®Œæˆ")
    print(f"\n{'=' * 70}")
    print("å¼€å§‹è®­ç»ƒ...")
    print(f"{'=' * 70}\n")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 8: ä¿å­˜æ¨¡å‹å’Œå†å²
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ’¾ æ­¥éª¤ 8: ä¿å­˜ç»“æœ")
    
    trainer.save_model(OUTPUT_DIR)
    trainer.save_orth_loss_history(OUTPUT_DIR)
    
    print(f"\n{'=' * 70}")
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"{'=' * 70}")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {OUTPUT_DIR}")
    print(f"æ­£äº¤æŸå¤±å†å²: {OUTPUT_DIR}/orth_loss_history.json")
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"  1. æŸ¥çœ‹æ­£äº¤æŸå¤±å†å²")
    print(f"  2. ä½¿ç”¨ eval_preference_retention.py è¯„ä¼°åå¥½ä¿ç•™")
    print(f"  3. å¯¹æ¯”æœ‰/æ— çº¦æŸçš„æ•ˆæœå·®å¼‚")
    print(f"{'=' * 70}\n")


if __name__ == '__main__':
    main()
