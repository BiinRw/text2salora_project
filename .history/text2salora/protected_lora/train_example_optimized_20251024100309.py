"""
æ˜¾å­˜ä¼˜åŒ–ç‰ˆæœ¬çš„è®­ç»ƒç¤ºä¾‹ (é€‚é… 24GB æ˜¾å­˜)
ä¸»è¦ä¼˜åŒ–:
1. é¿å…é¢„è®¡ç®—å®Œæ•´æŠ•å½±çŸ©é˜µ P = V @ V^T
2. ä½¿ç”¨ 8-bit é‡åŒ–åŠ è½½æ¨¡å‹
3. å‡å° batch size å’Œ sequence length
4. ä»…çº¦æŸ q_proj å’Œ v_proj
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
import sys
from pathlib import Path
import gc

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from utils.svd_utils import PreferenceSubspaceManager
from protected_lora.train_lora_orthogonal import OrthogonalLoRATrainer


class MemoryEfficientOrthogonalConstraint:
    """æ˜¾å­˜ä¼˜åŒ–çš„æ­£äº¤çº¦æŸè®¡ç®—å™¨
    
    ä¸é¢„è®¡ç®— P = V @ V^T,è€Œæ˜¯åœ¨éœ€è¦æ—¶åŠ¨æ€è®¡ç®— V @ (V^T @ x)
    """
    
    def __init__(
        self,
        subspace_manager: PreferenceSubspaceManager,
        dimensions: list,
        use_fused: bool = True,
        device: str = 'cuda:0'
    ):
        self.manager = subspace_manager
        self.dimensions = dimensions
        self.use_fused = use_fused
        self.device = device
        
        # åªå­˜å‚¨ V çŸ©é˜µ,ä¸è®¡ç®— P
        self.subspace_V = {}
        for dim in dimensions:
            V = self.manager.get_subspace(dim, layer_id=None if use_fused else 0)
            self.subspace_V[dim] = V
            print(f"   {dim}: V shape={V.shape} (ä¸é¢„è®¡ç®— P,èŠ‚çœæ˜¾å­˜)")
    
    def compute_orthogonal_loss_efficient(
        self,
        lora_A: dict,
        lora_B: dict,
        lambda_orth: float = 0.1,
        dimension_weights: dict = None
    ):
        """æ˜¾å­˜ä¼˜åŒ–çš„æ­£äº¤æŸå¤±è®¡ç®—
        
        åŸå§‹: BAP = B @ A @ (V @ V^T)
        ä¼˜åŒ–: BAP = B @ A @ V @ V^T = (B @ A @ V) @ V^T
        
        è¿›ä¸€æ­¥ä¼˜åŒ–: 
        L = ||BAP||Â² = ||B @ A @ V @ V^T||Â²
          = trace((B @ A @ V @ V^T) @ (B @ A @ V @ V^T)^T)
          = trace((B @ A @ V) @ V^T @ V @ (B @ A @ V)^T)
          = trace((B @ A @ V) @ (B @ A @ V)^T)  (å› ä¸º V^T @ V = I)
          = ||B @ A @ V||Â²
        """
        if dimension_weights is None:
            dimension_weights = {dim: 1.0 for dim in self.dimensions}
        
        total_loss = 0.0
        loss_details = {}
        
        for dim in self.dimensions:
            dim_weight = dimension_weights.get(dim, 1.0)
            dim_loss = 0.0
            
            V = self.subspace_V[dim]  # (d, k)
            
            for layer_name in lora_A.keys():
                A = lora_A[layer_name]  # (rank, d)
                B = lora_B[layer_name]  # (out, rank)
                
                # è®¡ç®— B @ A @ V
                AV = A @ V  # (rank, d) @ (d, k) = (rank, k)
                BAV = B @ AV  # (out, rank) @ (rank, k) = (out, k)
                
                # ||BAV||Â²
                loss_term = torch.sum(BAV ** 2)
                dim_loss += loss_term
            
            dim_loss = dim_weight * dim_loss
            loss_details[dim] = dim_loss.item()
            total_loss += dim_loss
        
        total_loss = lambda_orth * total_loss
        
        return total_loss, loss_details


class MemoryEfficientTrainer(OrthogonalLoRATrainer):
    """æ˜¾å­˜ä¼˜åŒ–çš„ Trainer"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """è®¡ç®—æŸå¤±,å®šæœŸæ¸…ç†æ˜¾å­˜"""
        
        # 1. è®¡ç®—ä»»åŠ¡æŸå¤±
        outputs = model(**inputs)
        
        if isinstance(outputs, dict):
            task_loss = outputs.get('loss')
        else:
            task_loss = outputs[0] if isinstance(outputs, tuple) else outputs.loss
        
        # 2. è®¡ç®—æ­£äº¤æŸå¤±
        if model.training:
            lora_A = {}
            lora_B = {}
            
            for name, module in model.named_modules():
                if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                    lora_A[name] = module.lora_A['default'].weight
                    lora_B[name] = module.lora_B['default'].weight
            
            if len(lora_A) > 0:
                orth_loss, loss_details = self.constraint.compute_orthogonal_loss_efficient(
                    lora_A, lora_B,
                    lambda_orth=self.lambda_orth,
                    dimension_weights=self.dimension_weights
                )
                
                total_loss = task_loss + orth_loss
                
                # è®°å½•
                if self.state.global_step % 10 == 0:
                    self.orth_loss_history.append({
                        'step': self.state.global_step,
                        'task_loss': task_loss.item(),
                        'orth_loss': orth_loss.item(),
                        'details': loss_details
                    })
            else:
                total_loss = task_loss
        else:
            total_loss = task_loss
        
        # å®šæœŸæ¸…ç†æ˜¾å­˜
        if self.state.global_step % 20 == 0:
            torch.cuda.empty_cache()
        
        return (total_loss, outputs) if return_outputs else total_loss


def main():
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # é…ç½®å‚æ•° (æ˜¾å­˜ä¼˜åŒ–)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    MODEL_PATH = '/var/models/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/f347a08eb880e0a3c87089c8c45043775c338c9c'
    SUBSPACE_DIR = '../preference_subspace/saved_subspaces'
    OUTPUT_DIR = './output/lora_orthogonal_24gb'
    
    # åå¥½çº¦æŸé…ç½®
    PREFERENCE_DIMENSIONS = ['safety', 'helpfulness']
    LAMBDA_ORTH = 0.1
    DIMENSION_WEIGHTS = {
        'safety': 1.0,
        'helpfulness': 0.5
    }
    
    # LoRA é…ç½® (æ˜¾å­˜ä¼˜åŒ–)
    LORA_CONFIG = {
        'rank': 8,
        'alpha': 16,
        'dropout': 0.1,
        'target_modules': ['q_proj', 'v_proj']  # åªçº¦æŸ QV,èŠ‚çœæ˜¾å­˜
    }
    
    print("=" * 70)
    print("ğŸš€ æ˜¾å­˜ä¼˜åŒ–è®­ç»ƒ: æ­£äº¤çº¦æŸ LoRA (24GB æ˜¾å­˜)")
    print("=" * 70)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 1: åŠ è½½åå¥½å­ç©ºé—´
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ“¦ æ­¥éª¤ 1: åŠ è½½åå¥½å­ç©ºé—´")
    
    manager = PreferenceSubspaceManager(
        subspace_dir=SUBSPACE_DIR,
        device='cuda:0'
    )
    
    manager.load_all_dimensions(
        dimensions=PREFERENCE_DIMENSIONS,
        use_fused=True
    )
    
    manager.print_info()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 2: åˆ›å»ºæ˜¾å­˜ä¼˜åŒ–çš„çº¦æŸè®¡ç®—å™¨
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ”§ æ­¥éª¤ 2: åˆ›å»ºæ˜¾å­˜ä¼˜åŒ–çš„çº¦æŸè®¡ç®—å™¨")
    
    constraint = MemoryEfficientOrthogonalConstraint(
        subspace_manager=manager,
        dimensions=PREFERENCE_DIMENSIONS,
        use_fused=True,
        device='cuda:0'
    )
    
    print(f"âœ… çº¦æŸè®¡ç®—å™¨åˆ›å»ºå®Œæˆ (ä¸é¢„è®¡ç®— P çŸ©é˜µ)")
    
    # æ¸…ç†æ˜¾å­˜
    gc.collect()
    torch.cuda.empty_cache()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 3: åŠ è½½æ¨¡å‹ (ä½¿ç”¨ float16)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ¤– æ­¥éª¤ 3: åŠ è½½æ¨¡å‹ (float16 ä¼˜åŒ–)")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,
        device_map='auto',  # è‡ªåŠ¨åˆ†é…æ˜¾å­˜
        low_cpu_mem_usage=True
    )
    
    print(f"âœ… åŸºåº§æ¨¡å‹åŠ è½½å®Œæˆ (float16)")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 4: åº”ç”¨ LoRA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ”„ æ­¥éª¤ 4: åº”ç”¨ LoRA")
    
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_CONFIG['rank'],
        lora_alpha=LORA_CONFIG['alpha'],
        lora_dropout=LORA_CONFIG['dropout'],
        target_modules=LORA_CONFIG['target_modules'],
        bias='none'
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # æ¸…ç†æ˜¾å­˜
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"âœ… LoRA åº”ç”¨å®Œæˆ")
    print(f"ğŸ“Š æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / 24 GB")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 5: å‡†å¤‡è®­ç»ƒæ•°æ® (å°æ•°æ®é›†)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ“š æ­¥éª¤ 5: å‡†å¤‡è®­ç»ƒæ•°æ®")
    
    test_texts = [
        "What is artificial intelligence?",
        "Explain machine learning.",
        "How does a neural network work?",
        "What is deep learning?",
        "Describe natural language processing."
    ] * 4  # 20 ä¸ªæ ·æœ¬
    
    from torch.utils.data import Dataset
    
    class SimpleDataset(Dataset):
        def __init__(self, texts, tokenizer, max_length=256):  # å‡å° max_length
            self.encodings = tokenizer(
                texts,
                padding='max_length',
                truncation=True,
                max_length=max_length,
                return_tensors='pt'
            )
        
        def __len__(self):
            return len(self.encodings['input_ids'])
        
        def __getitem__(self, idx):
            return {
                'input_ids': self.encodings['input_ids'][idx],
                'attention_mask': self.encodings['attention_mask'][idx],
                'labels': self.encodings['input_ids'][idx]
            }
    
    train_dataset = SimpleDataset(test_texts, tokenizer, max_length=256)
    eval_dataset = SimpleDataset(test_texts[:3], tokenizer, max_length=256)
    
    print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ")
    print(f"   â€¢ è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"   â€¢ éªŒè¯æ ·æœ¬: {len(eval_dataset)}")
    print(f"   â€¢ Max Length: 256 (èŠ‚çœæ˜¾å­˜)")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 6: è®¾ç½®è®­ç»ƒå‚æ•° (æ˜¾å­˜ä¼˜åŒ–)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nâš™ï¸  æ­¥éª¤ 6: è®¾ç½®è®­ç»ƒå‚æ•° (æ˜¾å­˜ä¼˜åŒ–)")
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=2,
        per_device_train_batch_size=1,  # å‡å° batch size
        gradient_accumulation_steps=8,   # å¢åŠ æ¢¯åº¦ç´¯ç§¯
        learning_rate=5e-5,
        warmup_steps=10,
        logging_steps=2,
        save_steps=20,
        eval_steps=20,
        evaluation_strategy='steps',
        save_total_limit=1,
        fp16=True,
        max_grad_norm=1.0,
        gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        optim='adamw_torch',
        report_to='none'
    )
    
    print(f"âœ… è®­ç»ƒå‚æ•°è®¾ç½®å®Œæˆ")
    print(f"   â€¢ Batch Size: 1")
    print(f"   â€¢ Gradient Accumulation: 8")
    print(f"   â€¢ Effective Batch Size: 8")
    print(f"   â€¢ Gradient Checkpointing: True")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ­¥éª¤ 7: åˆ›å»º Trainer å¹¶è®­ç»ƒ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸƒ æ­¥éª¤ 7: åˆ›å»º Trainer å¹¶å¼€å§‹è®­ç»ƒ")
    
    trainer = MemoryEfficientTrainer(
        constraint_calculator=constraint,
        lambda_orth=LAMBDA_ORTH,
        dimension_weights=DIMENSION_WEIGHTS,
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False
        )
    )
    
    print(f"âœ… Trainer åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / 24 GB")
    print(f"\n{'=' * 70}")
    print("å¼€å§‹è®­ç»ƒ...")
    print(f"{'=' * 70}\n")
    
    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train()
        
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœ")
        trainer.save_model(OUTPUT_DIR)
        trainer.save_orth_loss_history(OUTPUT_DIR)
        
        print(f"\n{'=' * 70}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"{'=' * 70}")
        print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {OUTPUT_DIR}")
        print(f"æ­£äº¤æŸå¤±å†å²: {OUTPUT_DIR}/orth_loss_history.json")
        print(f"ğŸ“Š æœ€ç»ˆæ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / 24 GB")
        
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"\nâŒ æ˜¾å­˜ä¸è¶³é”™è¯¯: {e}")
            print(f"\nå»ºè®®:")
            print(f"  1. è¿›ä¸€æ­¥å‡å° batch_size æˆ– max_length")
            print(f"  2. å‡å°‘ LoRA target_modules (åªç”¨ ['q_proj'])")
            print(f"  3. å‡å°å­ç©ºé—´ç»´åº¦ (é‡æ–°è¿è¡Œ compute_svd.py --top_k 32)")
            print(f"  4. åªçº¦æŸä¸€ä¸ªåå¥½ç»´åº¦ (PREFERENCE_DIMENSIONS = ['safety'])")
        raise


if __name__ == '__main__':
    main()
