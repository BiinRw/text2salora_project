#!/bin/bash

# ä¿®æ­£ç‰ˆ: è®­ç»ƒæ‰€æœ‰ç»´åº¦çš„æ¢é’ˆ
# å…³é”®ä¿®æ”¹: ä½¿ç”¨0/1æ ‡ç­¾çš„æ•°æ®

MODEL_NAME="Qwen/Qwen2.5-1.5B-Instruct"
DEVICE="cuda:3"

DATA_DIR="data"
RESULTS_DIR="results_all_dimensions"
PROBES_DIR="trained_probes"

mkdir -p $RESULTS_DIR
mkdir -p $PROBES_DIR

echo "========================================="
echo "ğŸš€ ä¿®æ­£ç‰ˆ: è®­ç»ƒæ‰€æœ‰ç»´åº¦æ¢é’ˆ"
echo "========================================="
echo "æ¨¡å‹: $MODEL_NAME"
echo "è®¾å¤‡: $DEVICE"
echo "æ•°æ®: 0/1æ ‡ç­¾é…å¯¹æ•°æ®"
echo "========================================="

# 1. è®­ç»ƒå®‰å…¨æ€§æ¢é’ˆ
echo ""
echo "ğŸ“Š [1/6] è®­ç»ƒå®‰å…¨æ€§æ¢é’ˆ..."
python train_probe_paired_improved.py \
    --model_name $MODEL_NAME \
    --device $DEVICE \
    --safe_pairs ${DATA_DIR}/safety_paired/safe_pairs_large.json \
    --harmful_pairs ${DATA_DIR}/safety_paired/harmful_pairs_large.json \
    --max_samples 5000 \
    --test_split 0.2 \
    --cv_folds 5 \
    --max_iter 2000 \
    --reg_C 1.0 \
    --output_file ${RESULTS_DIR}/probe_safety.json \
    --probe_dir ${PROBES_DIR}/safety

# 2-6. è®­ç»ƒHelpSteerå„ç»´åº¦æ¢é’ˆ
dimensions=("helpfulness" "correctness" "coherence" "complexity" "verbosity")
dim_names=("æœ‰ç”¨æ€§" "æ­£ç¡®æ€§" "è¿è´¯æ€§" "å¤æ‚æ€§" "å†—é•¿åº¦")

for i in "${!dimensions[@]}"; do
    dim=${dimensions[$i]}
    name=${dim_names[$i]}
    idx=$((i+2))
    
    echo ""
    echo "ğŸ“Š [$idx/6] è®­ç»ƒ${name}æ¢é’ˆ ($dim)..."
    python train_helpsteer_dimension.py \
        --model_name $MODEL_NAME \
        --device $DEVICE \
        --good_pairs ${DATA_DIR}/helpsteer_paired/${dim}_good_pairs.json \
        --bad_pairs ${DATA_DIR}/helpsteer_paired/${dim}_bad_pairs.json \
        --max_samples 5000 \
        --test_split 0.2 \
        --cv_folds 5 \
        --max_iter 2000 \
        --reg_C 1.0 \
        --output_file ${RESULTS_DIR}/probe_${dim}.json \
        --probe_dir ${PROBES_DIR}/${dim}
done

echo ""
echo "========================================="
echo "âœ… æ‰€æœ‰ç»´åº¦è®­ç»ƒå®Œæˆ!"
echo "========================================="
echo "ğŸ“‚ ç»“æœç›®å½•: $RESULTS_DIR/"
echo "ğŸ“‚ æ¢é’ˆç›®å½•: $PROBES_DIR/"
echo "========================================="

# ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
echo ""
echo "ğŸ“ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š..."
python << 'EOFREPORT'
import json
import os
import numpy as np

results_dir = "results_all_dimensions"
output_file = os.path.join(results_dir, "training_summary.txt")

dimensions = [
    ('safety', 'å®‰å…¨æ€§'),
    ('helpfulness', 'æœ‰ç”¨æ€§'),
    ('correctness', 'æ­£ç¡®æ€§'),
    ('coherence', 'è¿è´¯æ€§'),
    ('complexity', 'å¤æ‚æ€§'),
    ('verbosity', 'å†—é•¿åº¦')
]

with open(output_file, 'w') as f:
    f.write("=" * 80 + "\n")
    f.write("ğŸ“Š æ‰€æœ‰ç»´åº¦æ¢é’ˆè®­ç»ƒæ±‡æ€»æŠ¥å‘Š\n")
    f.write("=" * 80 + "\n\n")
    
    all_stats = []
    
    for dim_key, dim_name in dimensions:
        result_file = os.path.join(results_dir, f"probe_{dim_key}.json")
        
        if not os.path.exists(result_file):
            f.write(f"âŒ {dim_name} ({dim_key}): ç»“æœæ–‡ä»¶ä¸å­˜åœ¨\n\n")
            continue
        
        with open(result_file, 'r') as rf:
            results = json.load(rf)
        
        test_accs = [r['test_accuracy'] for r in results.values()]
        cv_means = [r['cv_mean'] for r in results.values()]
        
        stats = {
            'dimension': dim_name,
            'key': dim_key,
            'total_heads': len(results),
            'avg_test': np.mean(test_accs),
            'max_test': np.max(test_accs),
            'min_test': np.min(test_accs),
            'avg_cv': np.mean(cv_means),
            'max_cv': np.max(cv_means),
            'ge_80': sum(1 for a in test_accs if a >= 0.8),
            'ge_90': sum(1 for a in test_accs if a >= 0.9)
        }
        all_stats.append(stats)
        
        f.write(f"ğŸ“Œ {dim_name} ({dim_key})\n")
        f.write("-" * 80 + "\n")
        f.write(f"æ€»æ³¨æ„åŠ›å¤´æ•°: {stats['total_heads']}\n")
        f.write(f"å¹³å‡æµ‹è¯•å‡†ç¡®ç‡: {stats['avg_test']:.4f}\n")
        f.write(f"æœ€é«˜æµ‹è¯•å‡†ç¡®ç‡: {stats['max_test']:.4f}\n")
        f.write(f"æœ€ä½æµ‹è¯•å‡†ç¡®ç‡: {stats['min_test']:.4f}\n")
        f.write(f"å¹³å‡CVå‡†ç¡®ç‡: {stats['avg_cv']:.4f}\n")
        f.write(f"æœ€é«˜CVå‡†ç¡®ç‡: {stats['max_cv']:.4f}\n")
        f.write(f"å‡†ç¡®ç‡ >= 0.8: {stats['ge_80']} ä¸ª\n")
        f.write(f"å‡†ç¡®ç‡ >= 0.9: {stats['ge_90']} ä¸ª\n")
        
        # Top 5
        f.write(f"\nğŸ† Top 5 æœ€ä½³æ³¨æ„åŠ›å¤´:\n")
        top_5 = sorted(results.items(), key=lambda x: x[1]['test_accuracy'], reverse=True)[:5]
        for i, (head, metrics) in enumerate(top_5, 1):
            f.write(f"  {i}. {head}: test={metrics['test_accuracy']:.4f}, ")
            f.write(f"cv={metrics['cv_mean']:.4f}Â±{metrics['cv_std']:.4f}\n")
        
        f.write("\n")
    
    # æ€»ä½“ç»Ÿè®¡
    if all_stats:
        f.write("=" * 80 + "\n")
        f.write("ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
        f.write("=" * 80 + "\n")
        
        avg_of_avgs = np.mean([s['avg_test'] for s in all_stats])
        best_dim = max(all_stats, key=lambda x: x['avg_test'])
        worst_dim = min(all_stats, key=lambda x: x['avg_test'])
        
        f.write(f"è®­ç»ƒç»´åº¦æ•°: {len(all_stats)}\n")
        f.write(f"å¹³å‡å‡†ç¡®ç‡: {avg_of_avgs:.4f}\n")
        f.write(f"æœ€ä½³ç»´åº¦: {best_dim['dimension']} (avg={best_dim['avg_test']:.4f})\n")
        f.write(f"æœ€å·®ç»´åº¦: {worst_dim['dimension']} (avg={worst_dim['avg_test']:.4f})\n")
        f.write(f"\næ€»>=0.8å¤´æ•°: {sum(s['ge_80'] for s in all_stats)}\n")
        f.write(f"æ€»>=0.9å¤´æ•°: {sum(s['ge_90'] for s in all_stats)}\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("âœ… è®­ç»ƒå®Œæˆ!\n")
        f.write(f"ğŸ’¾ æ¢é’ˆæ¨¡å‹å·²ä¿å­˜åˆ°: trained_probes/\n")
        f.write("=" * 80 + "\n")

print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
EOFREPORT

echo ""
echo "ï¿½ï¿½ æŸ¥çœ‹æ±‡æ€»æŠ¥å‘Š:"
cat ${RESULTS_DIR}/training_summary.txt

