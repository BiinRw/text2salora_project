"""
æ¢é’ˆå‡†ç¡®åº¦æµ‹è¯•è„šæœ¬

åŠŸèƒ½:
1. åŠ è½½ä¸åŒç±»å‹çš„æ¨¡å‹(åŸºæ¨¡å‹/å¾®è°ƒæ¨¡å‹/LoRAæ¨¡å‹)
2. åŠ è½½å·²è®­ç»ƒçš„æ¢é’ˆ
3. åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°æ¯å±‚æ¢é’ˆçš„å‡†ç¡®åº¦
4. ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
# æµ‹è¯•åŸºæ¨¡å‹
python test_probe_accuracy.py \
    --model_path meta-llama/Meta-Llama-3.1-8B-Instruct \
    --probe_dir ../trained_probes_paired/helpfulness \
    --test_data ../data/helpsteer_merged_paired \
    --dimension helpfulness \
    --output_dir results/base_model

# æµ‹è¯•å¾®è°ƒæ¨¡å‹
python test_probe_accuracy.py \
    --model_path /path/to/finetuned/model \
    --probe_dir ../trained_probes_paired/helpfulness \
    --test_data ../data/helpsteer_merged_paired \
    --dimension helpfulness \
    --output_dir results/finetuned_model

# æµ‹è¯•LoRAæ¨¡å‹
python test_probe_accuracy.py \
    --model_path meta-llama/Meta-Llama-3.1-8B-Instruct \
    --lora_path /path/to/lora/adapter \
    --probe_dir ../trained_probes_paired/helpfulness \
    --test_data ../data/helpsteer_merged_paired \
    --dimension helpfulness \
    --output_dir results/lora_model
"""

import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm
import json
import argparse
import os
import pickle
from pathlib import Path
from datetime import datetime


class ActivationExtractor:
    """æå–æ¨¡å‹æ¿€æ´»å€¼çš„å·¥å…·ç±»"""
    
    def __init__(self, model, tokenizer, device='cuda:0'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å±‚çš„æ­£ç¡®è·¯å¾„
        # ä¸åŒæ¨¡å‹å’ŒåŠ è½½æ–¹å¼(åŸºæ¨¡å‹ vs LoRA)çš„å±‚è·¯å¾„ä¸åŒ
        self.model_layers = self._get_model_layers()
        self.num_layers = model.config.num_hidden_layers
        
        self.activations = {}
        self.hooks = []
    
    def _get_model_layers(self):
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å±‚çš„æ­£ç¡®è®¿é—®è·¯å¾„
        
        ä¸åŒæƒ…å†µä¸‹çš„å±‚è·¯å¾„:
        - åŸºæ¨¡å‹: model.model.layers
        - LoRAæ¨¡å‹: model.model.model.layers æˆ– model.base_model.model.model.layers
        """
        # å°è¯•ä¸åŒçš„è·¯å¾„
        possible_paths = [
            ('model.model.layers', lambda m: m.model.layers),
            ('model.model.model.layers', lambda m: m.model.model.layers),
            ('model.base_model.model.model.layers', lambda m: m.base_model.model.model.layers),
        ]
        
        for path_name, path_fn in possible_paths:
            try:
                layers = path_fn(self.model)
                if layers is not None and len(layers) > 0:
                    print(f"   âœ… æ£€æµ‹åˆ°æ¨¡å‹å±‚è·¯å¾„: {path_name}")
                    return layers
            except (AttributeError, TypeError):
                continue
        
        raise RuntimeError("æ— æ³•æ‰¾åˆ°æ¨¡å‹çš„å±‚ç»“æ„! è¯·æ£€æŸ¥æ¨¡å‹ç±»å‹ã€‚")
    
    def _get_activation_hook(self, layer_id):
        """åˆ›å»ºhookå‡½æ•°æ¥æ•è·æ¿€æ´»å€¼"""
        def hook(module, input, output):
            key = f"layer-{layer_id}"
            if key not in self.activations:
                self.activations[key] = []
            # æå–æœ€åä¸€ä¸ªtokençš„æ¿€æ´»å€¼
            self.activations[key].append(output[:, -1, :].detach().cpu())
        return hook
    
    def register_hooks(self):
        """æ³¨å†Œhooksåˆ°æ‰€æœ‰å±‚çš„QæŠ•å½±"""
        for layer_id in range(self.num_layers):
            layer = self.model_layers[layer_id]
            hook = layer.self_attn.q_proj.register_forward_hook(
                self._get_activation_hook(layer_id)
            )
            self.hooks.append(hook)
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
    
    def format_conversation(self, prompt, response):
        """æ ¼å¼åŒ–å¯¹è¯ä¸ºæ¨¡å‹è¾“å…¥"""
        messages = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response}
        ]
        
        try:
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=False
            )
        except:
            text = f"User: {prompt}\nAssistant: {response}"
        
        return text
    
    def extract_activations(self, data_samples, max_samples=None):
        """æå–æµ‹è¯•æ•°æ®çš„æ¿€æ´»å€¼
        
        Args:
            data_samples: æ•°æ®æ ·æœ¬åˆ—è¡¨,æ¯ä¸ªæ ·æœ¬åŒ…å« prompt å’Œ response
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡é™åˆ¶
            
        Returns:
            dict: {layer_id: numpy_array} æ¯å±‚çš„æ¿€æ´»å€¼
        """
        self.activations = {}
        self.register_hooks()
        
        if max_samples:
            data_samples = data_samples[:max_samples]
        
        print(f"ğŸ“¥ æå– {len(data_samples)} ä¸ªæ ·æœ¬çš„æ¿€æ´»å€¼...")
        self.model.eval()
        
        with torch.no_grad():
            for sample in tqdm(data_samples, desc="æå–æ¿€æ´»"):
                text = self.format_conversation(sample['prompt'], sample['response'])
                inputs = self.tokenizer(
                    text,
                    return_tensors='pt',
                    truncation=True,
                    max_length=512
                ).to(self.device)
                self.model(**inputs)
        
        self.remove_hooks()
        
        # æ•´ç†æ¿€æ´»å€¼ä¸ºnumpyæ•°ç»„,å¹¶æŒ‰æ³¨æ„åŠ›å¤´åˆ†å‰²
        head_activations = {}
        
        for layer_id in range(self.num_layers):
            layer_key = f"layer-{layer_id}"
            if layer_key in self.activations:
                # åˆå¹¶è¯¥å±‚æ‰€æœ‰æ ·æœ¬çš„æ¿€æ´»å€¼
                layer_acts = torch.cat(self.activations[layer_key], dim=0).numpy()
                
                # è®¡ç®—æ¯ä¸ªå¤´çš„ç»´åº¦
                num_heads = self.model.config.num_attention_heads
                head_dim = self.model.config.hidden_size // num_heads
                
                # æŒ‰å¤´åˆ†å‰²æ¿€æ´»å€¼
                for head_id in range(num_heads):
                    start_idx = head_id * head_dim
                    end_idx = (head_id + 1) * head_dim
                    head_key = f"layer-{layer_id}-head-{head_id}"
                    head_activations[head_key] = layer_acts[:, start_idx:end_idx]
        
        return head_activations


def load_model(model_path, lora_path=None, device='cuda:0'):
    """åŠ è½½æ¨¡å‹(åŸºæ¨¡å‹/å¾®è°ƒæ¨¡å‹/LoRAæ¨¡å‹)
    
    Args:
        model_path: åŸºæ¨¡å‹è·¯å¾„
        lora_path: LoRAé€‚é…å™¨è·¯å¾„(å¯é€‰)
        device: è®¾å¤‡
        
    Returns:
        model, tokenizer, model_type
    """
    print(f"\nğŸ”§ åŠ è½½æ¨¡å‹...")
    print(f"   åŸºæ¨¡å‹è·¯å¾„: {model_path}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½åŸºæ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map=device
    )
    
    model_type = "base"
    
    # å¦‚æœæä¾›äº†LoRAè·¯å¾„,åŠ è½½LoRAé€‚é…å™¨
    if lora_path:
        print(f"   LoRAè·¯å¾„: {lora_path}")
        model = PeftModel.from_pretrained(model, lora_path)
        model_type = "lora"
        print(f"   âœ… LoRAé€‚é…å™¨å·²åŠ è½½")
    elif "checkpoint" in model_path or "finetuned" in model_path.lower():
        model_type = "finetuned"
    
    print(f"   âœ… æ¨¡å‹ç±»å‹: {model_type}")
    print(f"   âœ… æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
    
    return model, tokenizer, model_type


def load_probes(probe_dir):
    """åŠ è½½å·²è®­ç»ƒçš„æ¢é’ˆ
    
    Args:
        probe_dir: æ¢é’ˆç›®å½•è·¯å¾„
        
    Returns:
        dict: {layer_id: LogisticRegressionæ¨¡å‹}
    """
    probe_file = os.path.join(probe_dir, 'linear_probes.pkl')
    
    if not os.path.exists(probe_file):
        raise FileNotFoundError(f"æ¢é’ˆæ–‡ä»¶ä¸å­˜åœ¨: {probe_file}")
    
    print(f"\nğŸ“‚ åŠ è½½æ¢é’ˆ...")
    print(f"   æ¢é’ˆæ–‡ä»¶: {probe_file}")
    
    with open(probe_file, 'rb') as f:
        probes = pickle.load(f)
    
    print(f"   âœ… å·²åŠ è½½ {len(probes)} ä¸ªå±‚çš„æ¢é’ˆ")
    
    return probes


def load_test_data(test_data_dir, dimension):
    """åŠ è½½æµ‹è¯•æ•°æ®
    
    Args:
        test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•
        dimension: ç»´åº¦åç§°(å¦‚ helpfulness, correctness, safetyç­‰)
        
    Returns:
        good_samples, bad_samples: å¥½æ ·æœ¬å’Œåæ ·æœ¬åˆ—è¡¨
    """
    # Safetyç»´åº¦ä½¿ç”¨ä¸åŒçš„æ–‡ä»¶å‘½å
    if dimension == 'safety':
        good_file = os.path.join(test_data_dir, "safe_pairs.json")
        bad_file = os.path.join(test_data_dir, "harmful_pairs.json")
    else:
        good_file = os.path.join(test_data_dir, f"{dimension}_good_pairs.json")
        bad_file = os.path.join(test_data_dir, f"{dimension}_bad_pairs.json")
    
    if not os.path.exists(good_file) or not os.path.exists(bad_file):
        raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {good_file} æˆ– {bad_file}")
    
    print(f"
ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
    print(f"   ç»´åº¦: {dimension}")
    if dimension == 'safety':
        print(f"   Safeæ•°æ®: {good_file}")
        print(f"   Harmfulæ•°æ®: {bad_file}")
    else:
        print(f"   Goodæ•°æ®: {good_file}")
        print(f"   Badæ•°æ®: {bad_file}")
    
    with open(good_file, 'r') as f:
        good_samples = json.load(f)
    
    with open(bad_file, 'r') as f:
        bad_samples = json.load(f)
    
    if dimension == 'safety':
        print(f"   âœ… Safeæ ·æœ¬: {len(good_samples)}")
        print(f"   âœ… Harmfulæ ·æœ¬: {len(bad_samples)}")
    else:
        print(f"   âœ… Goodæ ·æœ¬: {len(good_samples)}")
        print(f"   âœ… Badæ ·æœ¬: {len(bad_samples)}")
    
    return good_samples, bad_samples


def evaluate_probes(probes, good_activations, bad_activations):
    """è¯„ä¼°æ¯å±‚æ¢é’ˆçš„å‡†ç¡®åº¦
    
    Args:
        probes: æ¢é’ˆå­—å…¸ {layer_id: LogisticRegression}
        good_activations: Goodæ ·æœ¬çš„æ¿€æ´»å€¼ {layer_id: numpy_array}
        bad_activations: Badæ ·æœ¬çš„æ¿€æ´»å€¼ {layer_id: numpy_array}
        
    Returns:
        dict: æ¯å±‚çš„è¯„ä¼°ç»“æœ
    """
    print(f"\nğŸ¯ è¯„ä¼°æ¢é’ˆå‡†ç¡®åº¦...")
    
    results = {}
    
    for layer_id, probe in tqdm(probes.items(), desc="è¯„ä¼°å±‚"):
        # è·å–è¯¥å±‚çš„æ¿€æ´»å€¼
        X_good = good_activations[layer_id]
        X_bad = bad_activations[layer_id]
        
        # åˆå¹¶æ•°æ®å’Œæ ‡ç­¾
        X = np.vstack([X_good, X_bad])
        y = np.array([1] * len(X_good) + [0] * len(X_bad))
        
        # é¢„æµ‹
        y_pred = probe.predict(X)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y, y_pred, average='binary', zero_division=0
        )
        
        results[layer_id] = {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1),
            'n_samples': len(X),
            'n_good': len(X_good),
            'n_bad': len(X_bad)
        }
    
    return results


def print_results(results):
    """æ‰“å°è¯„ä¼°ç»“æœ
    
    Args:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š æ¢é’ˆå‡†ç¡®åº¦æµ‹è¯•ç»“æœ")
    print(f"=" * 80)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    accuracies = [r['accuracy'] for r in results.values()]
    f1_scores = [r['f1'] for r in results.values()]
    
    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"   å±‚æ•°: {len(results)}")
    print(f"   å¹³å‡å‡†ç¡®åº¦: {np.mean(accuracies):.4f} Â± {np.std(accuracies):.4f}")
    print(f"   æœ€é«˜å‡†ç¡®åº¦: {np.max(accuracies):.4f}")
    print(f"   æœ€ä½å‡†ç¡®åº¦: {np.min(accuracies):.4f}")
    print(f"   å¹³å‡F1åˆ†æ•°: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}")
    
    # å‡†ç¡®åº¦åˆ†å¸ƒ
    acc_ge_80 = sum(1 for a in accuracies if a >= 0.80)
    acc_ge_85 = sum(1 for a in accuracies if a >= 0.85)
    acc_ge_90 = sum(1 for a in accuracies if a >= 0.90)
    
    print(f"\nå‡†ç¡®åº¦åˆ†å¸ƒ:")
    print(f"   >= 0.80: {acc_ge_80}/{len(results)} ({acc_ge_80/len(results)*100:.1f}%)")
    print(f"   >= 0.85: {acc_ge_85}/{len(results)} ({acc_ge_85/len(results)*100:.1f}%)")
    print(f"   >= 0.90: {acc_ge_90}/{len(results)} ({acc_ge_90/len(results)*100:.1f}%)")
    
    # Top 10 å±‚
    print(f"\nï¿½ï¿½ Top 10 å‡†ç¡®åº¦æœ€é«˜çš„å±‚:")
    sorted_results = sorted(
        results.items(), 
        key=lambda x: x[1]['accuracy'], 
        reverse=True
    )
    for i, (layer_id, metrics) in enumerate(sorted_results[:10], 1):
        print(f"   {i}. {layer_id}: "
              f"Acc={metrics['accuracy']:.4f}, "
              f"F1={metrics['f1']:.4f}, "
              f"Precision={metrics['precision']:.4f}, "
              f"Recall={metrics['recall']:.4f}")
    
    # Bottom 5 å±‚
    print(f"\nâš ï¸ å‡†ç¡®åº¦æœ€ä½çš„5å±‚:")
    for i, (layer_id, metrics) in enumerate(sorted_results[-5:], 1):
        print(f"   {i}. {layer_id}: "
              f"Acc={metrics['accuracy']:.4f}, "
              f"F1={metrics['f1']:.4f}")
    
    print(f"\n" + "=" * 80)


def save_results(results, output_dir, model_type, dimension):
    """ä¿å­˜è¯„ä¼°ç»“æœ
    
    Args:
        results: è¯„ä¼°ç»“æœå­—å…¸
        output_dir: è¾“å‡ºç›®å½•
        model_type: æ¨¡å‹ç±»å‹
        dimension: ç»´åº¦åç§°
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    save_data = {
        'model_type': model_type,
        'dimension': dimension,
        'timestamp': timestamp,
        'layer_results': results,
        'summary': {
            'n_layers': len(results),
            'mean_accuracy': float(np.mean([r['accuracy'] for r in results.values()])),
            'std_accuracy': float(np.std([r['accuracy'] for r in results.values()])),
            'max_accuracy': float(np.max([r['accuracy'] for r in results.values()])),
            'min_accuracy': float(np.min([r['accuracy'] for r in results.values()])),
            'mean_f1': float(np.mean([r['f1'] for r in results.values()])),
            'layers_ge_80': sum(1 for r in results.values() if r['accuracy'] >= 0.80),
            'layers_ge_85': sum(1 for r in results.values() if r['accuracy'] >= 0.85),
            'layers_ge_90': sum(1 for r in results.values() if r['accuracy'] >= 0.90),
        }
    }
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(
        output_dir, 
        f"probe_test_{model_type}_{dimension}_{timestamp}.json"
    )
    with open(results_file, 'w') as f:
        json.dump(save_data, f, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ä¿å­˜ç®€æ´ç‰ˆæœ¬(ä»…å‡†ç¡®åº¦)
    accuracy_file = os.path.join(
        output_dir,
        f"accuracy_{model_type}_{dimension}_{timestamp}.txt"
    )
    with open(accuracy_file, 'w') as f:
        f.write(f"Model Type: {model_type}\n")
        f.write(f"Dimension: {dimension}\n")
        f.write(f"Timestamp: {timestamp}\n")
        f.write(f"\nSummary:\n")
        f.write(f"  Mean Accuracy: {save_data['summary']['mean_accuracy']:.4f}\n")
        f.write(f"  Max Accuracy: {save_data['summary']['max_accuracy']:.4f}\n")
        f.write(f"  Layers >= 0.80: {save_data['summary']['layers_ge_80']}\n")
        f.write(f"  Layers >= 0.85: {save_data['summary']['layers_ge_85']}\n")
        f.write(f"  Layers >= 0.90: {save_data['summary']['layers_ge_90']}\n")
        f.write(f"\nPer-Layer Accuracy:\n")
        for layer_id in sorted(results.keys(), key=lambda x: int(x.split('-')[1])):
            acc = results[layer_id]['accuracy']
            f.write(f"  {layer_id}: {acc:.4f}\n")
    
    print(f"   ç®€æ´ç‰ˆæœ¬: {accuracy_file}")


def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ¢é’ˆåœ¨ä¸åŒæ¨¡å‹ä¸Šçš„å‡†ç¡®åº¦')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_path', type=str, required=True,
                        help='åŸºæ¨¡å‹è·¯å¾„')
    parser.add_argument('--lora_path', type=str, default=None,
                        help='LoRAé€‚é…å™¨è·¯å¾„(å¯é€‰)')
    
    # æ¢é’ˆå’Œæ•°æ®å‚æ•°
    parser.add_argument('--probe_dir', type=str, required=True,
                        help='æ¢é’ˆç›®å½•è·¯å¾„')
    parser.add_argument('--test_data', type=str, required=True,
                        help='æµ‹è¯•æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--dimension', type=str, required=True,
                        help='ç»´åº¦åç§°(å¦‚ helpfulness, correctnessç­‰)')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, required=True,
                        help='ç»“æœè¾“å‡ºç›®å½•')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', type=str, default='cuda:0',
                        help='è®¾å¤‡(é»˜è®¤: cuda:0)')
    parser.add_argument('--max_samples', type=int, default=None,
                        help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°(ç”¨äºå¿«é€Ÿæµ‹è¯•)')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ§ª æ¢é’ˆå‡†ç¡®åº¦æµ‹è¯•")
    print("=" * 80)
    
    # 1. åŠ è½½æ¨¡å‹
    model, tokenizer, model_type = load_model(
        args.model_path, 
        args.lora_path, 
        args.device
    )
    
    # 2. åŠ è½½æ¢é’ˆ
    probes = load_probes(args.probe_dir)
    
    # 3. åŠ è½½æµ‹è¯•æ•°æ®
    good_samples, bad_samples = load_test_data(args.test_data, args.dimension)
    
    # é™åˆ¶æ ·æœ¬æ•°é‡(ç”¨äºå¿«é€Ÿæµ‹è¯•)
    if args.max_samples:
        good_samples = good_samples[:args.max_samples]
        bad_samples = bad_samples[:args.max_samples]
        print(f"\nâš ï¸ é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°: {args.max_samples}")
    
    # 4. æå–æ¿€æ´»å€¼
    extractor = ActivationExtractor(model, tokenizer, args.device)
    
    print(f"\næå–Goodæ ·æœ¬æ¿€æ´»å€¼...")
    good_activations = extractor.extract_activations(good_samples, args.max_samples)
    
    print(f"\næå–Badæ ·æœ¬æ¿€æ´»å€¼...")
    bad_activations = extractor.extract_activations(bad_samples, args.max_samples)
    
    # 5. è¯„ä¼°æ¢é’ˆ
    results = evaluate_probes(probes, good_activations, bad_activations)
    
    # 6. æ‰“å°ç»“æœ
    print_results(results)
    
    # 7. ä¿å­˜ç»“æœ
    save_results(results, args.output_dir, model_type, args.dimension)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
