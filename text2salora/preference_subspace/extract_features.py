"""
åå¥½å­ç©ºé—´ç‰¹å¾æå–
æå– chosen/rejected æ ·æœ¬çš„æ¿€æ´»å€¼,è®¡ç®—ç‰¹å¾å·®åˆ†
"""

import os
import json
import torch
import numpy as np
from tqdm import tqdm
from typing import Dict, List, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer
from pathlib import Path


class ActivationExtractor:
    """æå–æ¨¡å‹æ¿€æ´»å€¼çš„å·¥å…·ç±» (å¤ç”¨ probing å®ç°)"""
    
    def __init__(self, model, tokenizer, device='cuda:0'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å±‚çš„æ­£ç¡®è·¯å¾„
        self.model_layers = self._get_model_layers()
        self.num_layers = model.config.num_hidden_layers
        
        self.activations = {}
        self.hooks = []
    
    def _get_model_layers(self):
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å±‚çš„æ­£ç¡®è®¿é—®è·¯å¾„"""
        possible_paths = [
            ('model.model.layers', lambda m: m.model.layers),
            ('model.model.model.layers', lambda m: m.model.model.layers),
            ('model.base_model.model.model.layers', lambda m: m.base_model.model.model.layers),
        ]
        
        for path_name, path_fn in possible_paths:
            try:
                layers = path_fn(self.model)
                if layers is not None and len(layers) > 0:
                    print(f"   âœ… æ£€æµ‹åˆ°æ¨¡å‹å±‚è·¯å¾„: {path_name}")
                    return layers
            except (AttributeError, TypeError):
                continue
        
        raise RuntimeError("æ— æ³•æ‰¾åˆ°æ¨¡å‹çš„å±‚ç»“æ„!")
    
    def _get_activation_hook(self, layer_id):
        """åˆ›å»ºhookå‡½æ•°æ¥æ•è·æ¿€æ´»å€¼"""
        def hook(module, input, output):
            key = f"layer-{layer_id}"
            if key not in self.activations:
                self.activations[key] = []
            # æå–æœ€åä¸€ä¸ªtokençš„æ¿€æ´»å€¼
            self.activations[key].append(output[:, -1, :].detach().cpu())
        return hook
    
    def register_hooks(self):
        """æ³¨å†Œhooksåˆ°æ‰€æœ‰å±‚çš„QæŠ•å½±"""
        for layer_id in range(self.num_layers):
            layer = self.model_layers[layer_id]
            hook = layer.self_attn.q_proj.register_forward_hook(
                self._get_activation_hook(layer_id)
            )
            self.hooks.append(hook)
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
    
    def format_conversation(self, prompt, response):
        """æ ¼å¼åŒ–å¯¹è¯ä¸ºæ¨¡å‹è¾“å…¥"""
        messages = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response}
        ]
        
        try:
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=False
            )
        except:
            text = f"User: {prompt}\nAssistant: {response}"
        
        return text
    
    def extract_activations(self, data_samples, max_samples=None):
        """æå–æ•°æ®æ ·æœ¬çš„æ¿€æ´»å€¼
        
        Args:
            data_samples: æ•°æ®æ ·æœ¬åˆ—è¡¨,æ¯ä¸ªæ ·æœ¬åŒ…å« prompt å’Œ response
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡é™åˆ¶
            
        Returns:
            dict: {layer_id: numpy_array} æ¯å±‚çš„æ¿€æ´»å€¼
        """
        self.activations = {}
        self.register_hooks()
        
        if max_samples:
            data_samples = data_samples[:max_samples]
        
        print(f"ğŸ“¥ æå– {len(data_samples)} ä¸ªæ ·æœ¬çš„æ¿€æ´»å€¼...")
        self.model.eval()
        
        with torch.no_grad():
            for sample in tqdm(data_samples, desc="æå–æ¿€æ´»"):
                text = self.format_conversation(sample['prompt'], sample['response'])
                inputs = self.tokenizer(
                    text,
                    return_tensors='pt',
                    truncation=True,
                    max_length=512
                ).to(self.device)
                self.model(**inputs)
        
        self.remove_hooks()
        
        # æ•´ç†æ¿€æ´»å€¼ä¸ºnumpyæ•°ç»„,å¹¶æŒ‰æ³¨æ„åŠ›å¤´åˆ†å‰²
        head_activations = {}
        
        for layer_id in range(self.num_layers):
            layer_key = f"layer-{layer_id}"
            if layer_key in self.activations:
                # åˆå¹¶è¯¥å±‚æ‰€æœ‰æ ·æœ¬çš„æ¿€æ´»å€¼
                layer_acts = torch.cat(self.activations[layer_key], dim=0).numpy()
                
                # è®¡ç®—æ¯ä¸ªå¤´çš„ç»´åº¦
                num_heads = self.model.config.num_attention_heads
                head_dim = self.model.config.hidden_size // num_heads
                
                # æŒ‰å¤´åˆ†å‰²æ¿€æ´»å€¼
                for head_id in range(num_heads):
                    start_idx = head_id * head_dim
                    end_idx = (head_id + 1) * head_dim
                    head_key = f"layer-{layer_id}-head-{head_id}"
                    head_activations[head_key] = layer_acts[:, start_idx:end_idx]
        
        return head_activations


def load_preference_data(data_dir: str, dimension: str) -> Tuple[List, List]:
    """åŠ è½½åå¥½æ•°æ®å¯¹
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        dimension: ç»´åº¦åç§° (safety, helpfulness, correctness, coherence)
        
    Returns:
        chosen_samples, rejected_samples
    """
    data_dir = Path(data_dir)
    
    if dimension == 'safety':
        # Safety ç»´åº¦: safe=chosen, harmful=rejected
        chosen_file = data_dir / 'safety_paired' / 'safe_pairs.json'
        rejected_file = data_dir / 'safety_paired' / 'harmful_pairs.json'
    else:
        # å…¶ä»–ç»´åº¦: good=chosen, bad=rejected
        chosen_file = data_dir / 'helpsteer_paired' / f'{dimension}_good_pairs.json'
        rejected_file = data_dir / 'helpsteer_paired' / f'{dimension}_bad_pairs.json'
    
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®:")
    print(f"   Chosen:   {chosen_file}")
    print(f"   Rejected: {rejected_file}")
    
    with open(chosen_file, 'r') as f:
        chosen_samples = json.load(f)
    
    with open(rejected_file, 'r') as f:
        rejected_samples = json.load(f)
    
    print(f"   âœ… Chosen: {len(chosen_samples)} æ ·æœ¬")
    print(f"   âœ… Rejected: {len(rejected_samples)} æ ·æœ¬")
    
    return chosen_samples, rejected_samples


def extract_and_save_features(
    model_name: str,
    data_dir: str,
    dimension: str,
    output_dir: str,
    max_samples: int = None,
    device: str = 'cuda:0'
):
    """æå–å¹¶ä¿å­˜ç‰¹å¾å·®åˆ†
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        data_dir: æ•°æ®ç›®å½•
        dimension: åå¥½ç»´åº¦
        output_dir: è¾“å‡ºç›®å½•
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        device: è®¾å¤‡
    """
    print("=" * 80)
    print(f"ğŸš€ å¼€å§‹æå– {dimension} ç»´åº¦çš„ç‰¹å¾")
    print("=" * 80)
    
    # 1. åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map=device,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # 2. åŠ è½½æ•°æ®
    chosen_samples, rejected_samples = load_preference_data(data_dir, dimension)
    
    # é™åˆ¶æ ·æœ¬æ•°
    if max_samples:
        chosen_samples = chosen_samples[:max_samples]
        rejected_samples = rejected_samples[:max_samples]
        print(f"\nâš ï¸  é™åˆ¶æ ·æœ¬æ•°: {max_samples}")
    
    # 3. æå–æ¿€æ´»å€¼
    extractor = ActivationExtractor(model, tokenizer, device)
    
    print(f"\nğŸ“Š æå– Chosen æ ·æœ¬æ¿€æ´»å€¼:")
    h_chosen = extractor.extract_activations(chosen_samples, max_samples)
    
    print(f"\nğŸ“Š æå– Rejected æ ·æœ¬æ¿€æ´»å€¼:")
    h_rejected = extractor.extract_activations(rejected_samples, max_samples)
    
    # 4. è®¡ç®—ç‰¹å¾å·®åˆ†å¹¶æŒ‰å±‚ä¿å­˜
    print(f"\nğŸ’¾ è®¡ç®—å¹¶ä¿å­˜ç‰¹å¾å·®åˆ†...")
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æŒ‰å±‚ç»„ç»‡ç‰¹å¾å·®åˆ†
    layer_diffs = {}
    
    num_layers = model.config.num_hidden_layers
    num_heads = model.config.num_attention_heads
    
    for layer_id in range(num_layers):
        # åˆå¹¶è¯¥å±‚æ‰€æœ‰å¤´çš„æ¿€æ´»å€¼
        layer_chosen = []
        layer_rejected = []
        
        for head_id in range(num_heads):
            head_key = f"layer-{layer_id}-head-{head_id}"
            if head_key in h_chosen and head_key in h_rejected:
                layer_chosen.append(h_chosen[head_key])
                layer_rejected.append(h_rejected[head_key])
        
        if layer_chosen:
            # æ‹¼æ¥æ‰€æœ‰å¤´ (N, hidden_size)
            layer_chosen = np.concatenate(layer_chosen, axis=1)
            layer_rejected = np.concatenate(layer_rejected, axis=1)
            
            # è®¡ç®—å·®åˆ†
            diff = layer_chosen - layer_rejected
            layer_diffs[layer_id] = diff
            
            print(f"   Layer {layer_id:2d}: diff shape = {diff.shape}")
    
    # 5. ä¿å­˜
    output_file = output_dir / f'{dimension}_feature_diff.npz'
    np.savez(
        output_file,
        **{f'layer_{i}': diff for i, diff in layer_diffs.items()},
        num_layers=num_layers,
        num_samples=len(chosen_samples),
        hidden_size=model.config.hidden_size
    )
    
    print(f"\nâœ… ç‰¹å¾å·®åˆ†å·²ä¿å­˜: {output_file}")
    print(f"   åŒ…å« {len(layer_diffs)} å±‚çš„ç‰¹å¾å·®åˆ†")
    
    # æ¸…ç†
    del model
    torch.cuda.empty_cache()
    
    return output_file


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='æå–åå¥½ç‰¹å¾å·®åˆ†')
    parser.add_argument('--model_name', type=str, required=True,
                        help='æ¨¡å‹åç§°æˆ–è·¯å¾„')
    parser.add_argument('--data_dir', type=str, required=True,
                        help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--dimension', type=str, required=True,
                        choices=['safety', 'helpfulness', 'correctness', 'coherence'],
                        help='åå¥½ç»´åº¦')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max_samples', type=int, default=None,
                        help='æœ€å¤§æ ·æœ¬æ•°(ç”¨äºæµ‹è¯•)')
    parser.add_argument('--device', type=str, default='cuda:0',
                        help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    extract_and_save_features(
        model_name=args.model_name,
        data_dir=args.data_dir,
        dimension=args.dimension,
        output_dir=args.output_dir,
        max_samples=args.max_samples,
        device=args.device
    )
