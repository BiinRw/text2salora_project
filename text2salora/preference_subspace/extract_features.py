"""
åå¥½å­ç©ºé—´ç‰¹å¾æå– (æ”¯æŒåˆ†æŠ•å½±å±‚æå–)
æå– chosen/rejected æ ·æœ¬çš„æ¿€æ´»å€¼,è®¡ç®—ç‰¹å¾å·®åˆ†
v2: æ”¯æŒä¸ºæ¯ä¸ªæŠ•å½±å±‚(q_proj, k_proj, v_proj, o_proj, up_proj, down_proj)åˆ†åˆ«æå–å­ç©ºé—´
"""

import os
import json
import torch
import numpy as np
from tqdm import tqdm
from typing import Dict, List, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer
from pathlib import Path


class ActivationExtractor:
    """æå–æ¨¡å‹æ¿€æ´»å€¼çš„å·¥å…·ç±» (æ”¯æŒæŒ‡å®šæŠ•å½±å±‚)"""
    
    def __init__(self, model, tokenizer, projection_type='q_proj', device='cuda:0'):
        """
        Args:
            model: é¢„è®­ç»ƒæ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            projection_type: è¦æå–çš„æŠ•å½±å±‚ç±»å‹ (q_proj/k_proj/v_proj/o_proj/up_proj/down_proj)
            device: è®¾å¤‡
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.projection_type = projection_type  # æ–°å¢: æŠ•å½±å±‚ç±»å‹
        
        # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å±‚çš„æ­£ç¡®è·¯å¾„
        self.model_layers = self._get_model_layers()
        self.num_layers = model.config.num_hidden_layers
        
        self.activations = {}
        self.hooks = []
        
        print(f"   âœ… æå–æŠ•å½±å±‚: {projection_type}")
    
    def _get_model_layers(self):
        """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å±‚çš„æ­£ç¡®è®¿é—®è·¯å¾„"""
        possible_paths = [
            ('model.model.layers', lambda m: m.model.layers),
            ('model.model.model.layers', lambda m: m.model.model.layers),
            ('model.base_model.model.model.layers', lambda m: m.base_model.model.model.layers),
        ]
        
        for path_name, path_fn in possible_paths:
            try:
                layers = path_fn(self.model)
                if layers is not None and len(layers) > 0:
                    print(f"   âœ… æ£€æµ‹åˆ°æ¨¡å‹å±‚è·¯å¾„: {path_name}")
                    return layers
            except (AttributeError, TypeError):
                continue
        
        raise RuntimeError("æ— æ³•æ‰¾åˆ°æ¨¡å‹çš„å±‚ç»“æ„!")
    
    def _get_activation_hook(self, layer_id):
        """åˆ›å»ºhookå‡½æ•°æ¥æ•è·æ¿€æ´»å€¼"""
        def hook(module, input, output):
            key = f"layer-{layer_id}"
            if key not in self.activations:
                self.activations[key] = []
            # æå–æœ€åä¸€ä¸ªtokençš„æ¿€æ´»å€¼
            self.activations[key].append(output[:, -1, :].detach().cpu())
        return hook
    
    def register_hooks(self):
        """æ³¨å†Œhooksåˆ°æ‰€æœ‰å±‚çš„æŒ‡å®šæŠ•å½±å±‚
        
        æŠ•å½±å±‚ä½ç½®:
        - q_proj, k_proj, v_proj, o_proj: åœ¨ layer.self_attn ä¸­
        - up_proj, down_proj, gate_proj: åœ¨ layer.mlp ä¸­
        """
        print(f"ğŸ”§ æ³¨å†Œ {self.projection_type} hooks...")
        
        for layer_id in range(self.num_layers):
            layer = self.model_layers[layer_id]
            
            # æ ¹æ®æŠ•å½±ç±»å‹é€‰æ‹©è¦hookçš„æ¨¡å—
            if self.projection_type in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:
                # æ³¨æ„åŠ›å±‚æŠ•å½±
                try:
                    module = getattr(layer.self_attn, self.projection_type)
                except AttributeError:
                    raise ValueError(f"æ¨¡å‹å±‚æ²¡æœ‰ self_attn.{self.projection_type} å±æ€§!")
                    
            elif self.projection_type in ['up_proj', 'down_proj', 'gate_proj']:
                # MLPå±‚æŠ•å½± (gate_projæ˜¯æŸäº›æ¨¡å‹çš„é¢å¤–æŠ•å½±)
                try:
                    module = getattr(layer.mlp, self.projection_type)
                except AttributeError:
                    raise ValueError(f"æ¨¡å‹å±‚æ²¡æœ‰ mlp.{self.projection_type} å±æ€§!")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŠ•å½±ç±»å‹: {self.projection_type}. "
                               f"æ”¯æŒçš„ç±»å‹: q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj")
            
            # æ³¨å†Œhook
            hook = module.register_forward_hook(self._get_activation_hook(layer_id))
            self.hooks.append(hook)
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
    
    def format_conversation(self, prompt, response):
        """æ ¼å¼åŒ–å¯¹è¯ä¸ºæ¨¡å‹è¾“å…¥"""
        messages = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response}
        ]
        
        try:
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=False
            )
        except:
            text = f"User: {prompt}\nAssistant: {response}"
        
        return text
    
    def extract_activations(self, data_samples, max_samples=None):
        """æå–æ•°æ®æ ·æœ¬çš„æ¿€æ´»å€¼
        
        Args:
            data_samples: æ•°æ®æ ·æœ¬åˆ—è¡¨
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            
        Returns:
            dict: {layer_id: numpy_array} æ¯å±‚çš„æ¿€æ´»å€¼
        """
        self.activations = {}
        self.register_hooks()
        
        if max_samples:
            data_samples = data_samples[:max_samples]
        
        print(f"   æå– {len(data_samples)} ä¸ªæ ·æœ¬çš„æ¿€æ´»å€¼...")
        self.model.eval()
        
        with torch.no_grad():
            for sample in tqdm(data_samples, desc=f"   æå– {self.projection_type}"):
                text = self.format_conversation(sample['prompt'], sample['response'])
                inputs = self.tokenizer(
                    text,
                    return_tensors='pt',
                    truncation=True,
                    max_length=2048
                ).to(self.device)
                
                # å‰å‘ä¼ æ’­
                _ = self.model(**inputs)
        
        # ç§»é™¤hookså¹¶è¿”å›ç»“æœ
        self.remove_hooks()
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        activations_np = {}
        for key, values in self.activations.items():
            activations_np[key] = torch.cat(values, dim=0).numpy()
        
        return activations_np


def load_preference_data(data_dir: str, dimension: str) -> Tuple[List, List]:
    """åŠ è½½åå¥½æ•°æ®å¯¹
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        dimension: ç»´åº¦åç§° (safety, helpfulness, correctness, coherence)
        
    Returns:
        chosen_samples, rejected_samples
    """
    data_dir = Path(data_dir)
    
    if dimension == 'safety':
        # Safety ç»´åº¦: safe=chosen, harmful=rejected
        chosen_file = data_dir / 'safety_paired' / 'safe_pairs.json'
        rejected_file = data_dir / 'safety_paired' / 'harmful_pairs.json'
    else:
        # å…¶ä»–ç»´åº¦: good=chosen, bad=rejected
        chosen_file = data_dir / 'helpsteer_paired' / f'{dimension}_good_pairs.json'
        rejected_file = data_dir / 'helpsteer_paired' / f'{dimension}_bad_pairs.json'
    
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®:")
    print(f"   Chosen:   {chosen_file}")
    print(f"   Rejected: {rejected_file}")
    
    with open(chosen_file, 'r') as f:
        chosen_samples = json.load(f)
    
    with open(rejected_file, 'r') as f:
        rejected_samples = json.load(f)
    
    print(f"   âœ… Chosen: {len(chosen_samples)} æ ·æœ¬")
    print(f"   âœ… Rejected: {len(rejected_samples)} æ ·æœ¬")
    
    return chosen_samples, rejected_samples


def extract_and_save_features(
    model_name: str,
    data_dir: str,
    dimension: str,
    projection_type: str,  # æ–°å¢: æŠ•å½±å±‚ç±»å‹
    output_dir: str,
    max_samples: int = None,
    device: str = 'cuda:0'
):
    """æå–å¹¶ä¿å­˜ç‰¹å¾å·®åˆ†
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        data_dir: æ•°æ®ç›®å½•
        dimension: åå¥½ç»´åº¦
        projection_type: æŠ•å½±å±‚ç±»å‹ (q_proj/k_proj/v_proj/o_proj/up_proj/down_proj)
        output_dir: è¾“å‡ºç›®å½•
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        device: è®¾å¤‡
    """
    print("=" * 80)
    print(f"ğŸš€ å¼€å§‹æå– {dimension} ç»´åº¦ - {projection_type} æŠ•å½±å±‚çš„ç‰¹å¾")
    print("=" * 80)
    
    # 1. åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map=device,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # 2. åŠ è½½æ•°æ®
    chosen_samples, rejected_samples = load_preference_data(data_dir, dimension)
    
    # é™åˆ¶æ ·æœ¬æ•°
    if max_samples:
        chosen_samples = chosen_samples[:max_samples]
        rejected_samples = rejected_samples[:max_samples]
        print(f"\nâš ï¸  é™åˆ¶æ ·æœ¬æ•°: {max_samples}")
    
    # 3. æå–æ¿€æ´»å€¼ (ä½¿ç”¨æŒ‡å®šçš„æŠ•å½±å±‚)
    extractor = ActivationExtractor(model, tokenizer, projection_type, device)
    
    print(f"\nğŸ“Š æå– Chosen æ ·æœ¬æ¿€æ´»å€¼ ({projection_type}):")
    h_chosen = extractor.extract_activations(chosen_samples, max_samples)
    
    print(f"\nğŸ“Š æå– Rejected æ ·æœ¬æ¿€æ´»å€¼ ({projection_type}):")
    h_rejected = extractor.extract_activations(rejected_samples, max_samples)
    
    # 4. è®¡ç®—ç‰¹å¾å·®åˆ†å¹¶æŒ‰å±‚ä¿å­˜
    print(f"\nğŸ’¾ è®¡ç®—å¹¶ä¿å­˜ {projection_type} çš„ç‰¹å¾å·®åˆ†...")
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æŒ‰å±‚ç»„ç»‡ç‰¹å¾å·®åˆ†
    layer_diffs = {}
    
    num_layers = model.config.num_hidden_layers
    
    for layer_id in range(num_layers):
        layer_key = f"layer-{layer_id}"
        
        if layer_key not in h_chosen or layer_key not in h_rejected:
            print(f"   âš ï¸  è·³è¿‡ {layer_key}: ç¼ºå°‘æ¿€æ´»å€¼")
            continue
        
        # è®¡ç®—ç‰¹å¾å·®åˆ†: Î”h = h_chosen - h_rejected
        diff = h_chosen[layer_key] - h_rejected[layer_key]
        layer_diffs[layer_id] = diff
        
        print(f"   âœ… Layer {layer_id:2d} | Shape: {diff.shape} | "
              f"Mean: {diff.mean():.4f} | Std: {diff.std():.4f}")
    
    # 5. ä¿å­˜åˆ°æ–‡ä»¶ (æ–‡ä»¶ååŒ…å«æŠ•å½±å±‚ç±»å‹)
    output_file = output_dir / f'{dimension}_{projection_type}_feature_diff.npz'
    np.savez_compressed(
        output_file,
        **{f'layer_{layer_id}': diff for layer_id, diff in layer_diffs.items()},
        num_layers=num_layers,
        num_samples=len(chosen_samples),
        hidden_size=list(layer_diffs.values())[0].shape[1] if layer_diffs else 0
    )
    
    print(f"\nâœ… ç‰¹å¾å·®åˆ†å·²ä¿å­˜åˆ°: {output_file}")
    print(f"   - æŠ•å½±å±‚: {projection_type}")
    print(f"   - å±‚æ•°: {len(layer_diffs)}")
    print(f"   - æ ·æœ¬æ•°: {len(chosen_samples)}")
    print(f"   - è¾“å‡ºç»´åº¦: {list(layer_diffs.values())[0].shape[1] if layer_diffs else 'N/A'}")
    print("=" * 80)


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='æå–åå¥½ç‰¹å¾å·®åˆ† (æ”¯æŒæŒ‡å®šæŠ•å½±å±‚)')
    parser.add_argument('--model_name', type=str, required=True,
                       help='æ¨¡å‹åç§°æˆ–è·¯å¾„')
    parser.add_argument('--data_dir', type=str, required=True,
                       help='æ•°æ®ç›®å½• (åŒ…å« {dimension}_chosen.jsonl å’Œ {dimension}_rejected.jsonl)')
    parser.add_argument('--dimension', type=str, required=True,
                       help='åå¥½ç»´åº¦ (safety/helpfulness/correctness/coherence)')
    parser.add_argument('--projection', type=str, required=True,
                       choices=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj'],
                       help='æŠ•å½±å±‚ç±»å‹ (q/k/v/oåœ¨self_attn, up/down/gateåœ¨mlp)')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤ä½¿ç”¨å…¨éƒ¨)')
    parser.add_argument('--device', type=str, default='cuda:0',
                       help='è®¾å¤‡ (cuda:0, cuda:1, etc.)')
    
    args = parser.parse_args()
    
    extract_and_save_features(
        model_name=args.model_name,
        data_dir=args.data_dir,
        dimension=args.dimension,
        projection_type=args.projection,
        output_dir=args.output_dir,
        max_samples=args.max_samples,
        device=args.device
    )