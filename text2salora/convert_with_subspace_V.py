#!/usr/bin/env python3
"""
ä»å­ç©ºé—´å‘é‡ V æ„å»ºçº¦æŸçŸ©é˜µ Cï¼Œç„¶åè½¬æ¢ä¸º SaLoRA ABC.pt æ ¼å¼

æ­£ç¡®æµç¨‹:
1. åŠ è½½å­ç©ºé—´å‘é‡ V (ä» preference_subspace/saved_subspaces/)
2. æ„å»ºçº¦æŸçŸ©é˜µ C = I - V @ V^T  
3. åŠ è½½ LoRA adapter (A, B çŸ©é˜µ)
4. ç»„åˆæˆ ABC.pt æ–‡ä»¶
"""

import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel
import argparse
import os
from pathlib import Path

def load_and_build_constraint_matrix(subspace_dir, dimension, layer_id, device='cpu'):
    """ä»å­ç©ºé—´å‘é‡ V æ„å»ºçº¦æŸçŸ©é˜µ C"""
    
    # å°è¯•åŠ è½½ fused å­ç©ºé—´æˆ–å±‚çº§å­ç©ºé—´
    fused_path = Path(subspace_dir) / f"{dimension}_fused_subspace.pt"
    layer_path = Path(subspace_dir) / f"{dimension}_layer{layer_id}_subspace.pt"
    
    if fused_path.exists():
        print(f"  âœ“ Layer {layer_id}: ä½¿ç”¨ fused å­ç©ºé—´")
        data = torch.load(fused_path, map_location=device)
    elif layer_path.exists():
        print(f"  âœ“ Layer {layer_id}: ä½¿ç”¨å±‚çº§å­ç©ºé—´")
        data = torch.load(layer_path, map_location=device)
    else:
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ° layer {layer_id} çš„å­ç©ºé—´æ–‡ä»¶:\n"
            f"  - {fused_path}\n"
            f"  - {layer_path}"
        )
    
    # æå–å­ç©ºé—´å‘é‡ V
    V = data['V']  # shape: [feature_dim, subspace_rank]
    
    # æ„å»ºçº¦æŸçŸ©é˜µ C = I - V @ V^T
    feature_dim = V.shape[0]
    I = torch.eye(feature_dim, dtype=V.dtype, device=V.device)
    C = I - V @ V.T
    
    return C, V

def load_lora_adapter(base_model_name, adapter_path):
    """åŠ è½½ LoRA adapter"""
    print(f"\nï¿½ï¿½ åŠ è½½ base model: {base_model_name}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="cpu",
        trust_remote_code=True
    )
    
    print(f"ğŸ“‚ åŠ è½½ LoRA adapter: {adapter_path}")
    model = PeftModel.from_pretrained(base_model, adapter_path)
    
    return model, base_model

def extract_lora_weights(model, base_model):
    """æå– LoRA A, B çŸ©é˜µå’Œ base model æƒé‡"""
    weight_list = {}
    
    print("\nğŸ” æå– LoRA æƒé‡...")
    
    # æå– LoRA A, B çŸ©é˜µ
    for name, module in model.named_modules():
        if 'lora_A' in name and 'default' in name and hasattr(module, 'weight'):
            if 'layers.' in name and ('q_proj' in name or 'v_proj' in name):
                parts = name.split('.')
                layer_id, proj_type = None, None
                
                for i, part in enumerate(parts):
                    if part == 'layers' and i + 1 < len(parts):
                        layer_id = parts[i + 1]
                    if part in ['q_proj', 'v_proj']:
                        proj_type = part
                
                if layer_id and proj_type:
                    key = f"{proj_type}_{layer_id}lora_A"
                    weight_list[key] = module.weight.data.cpu().clone()
        
        if 'lora_B' in name and 'default' in name and hasattr(module, 'weight'):
            if 'layers.' in name and ('q_proj' in name or 'v_proj' in name):
                parts = name.split('.')
                layer_id, proj_type = None, None
                
                for i, part in enumerate(parts):
                    if part == 'layers' and i + 1 < len(parts):
                        layer_id = parts[i + 1]
                    if part in ['q_proj', 'v_proj']:
                        proj_type = part
                
                if layer_id and proj_type:
                    key = f"{proj_type}_{layer_id}lora_B"
                    weight_list[key] = module.weight.data.cpu().clone()
    
    print(f"âœ“ æå–åˆ° {len([k for k in weight_list if 'lora_A' in k])} ä¸ª LoRA å±‚")
    
    
    # è®¡ç®— merged weights (base + LoRA delta)
    print("\nğŸ” è®¡ç®— merged weights (base + LoRA)...")
    
    # è·å– LoRA scaling factor
    try:
        lora_config = model.peft_config['default']
        lora_alpha = lora_config.lora_alpha
        lora_r = lora_config.r
        scaling = lora_alpha / lora_r
        print(f"   LoRA scaling: {scaling} (alpha={lora_alpha}, r={lora_r})")
    except:
        scaling = 1.0
        print(f"   ä½¿ç”¨é»˜è®¤ scaling: {scaling}")
    
    for name, module in base_model.named_modules():
        if 'layers.' in name and ('q_proj' in name or 'v_proj' in name):
            if hasattr(module, 'weight') and 'lora' not in name.lower():
                if name.endswith('q_proj') or name.endswith('v_proj'):
                    parts = name.split('.')
                    layer_id, proj_type = None, None
                    
                    for i, part in enumerate(parts):
                        if part == 'layers' and i + 1 < len(parts):
                            layer_id = parts[i + 1]
                        if part in ['q_proj', 'v_proj']:
                            proj_type = part
                    
                    if layer_id and proj_type:
                        # Base weight
                        base_weight = module.weight.data.cpu().clone()
                        
                        # æŸ¥æ‰¾å¯¹åº”çš„ LoRA A, B
                        key_A = f"{proj_type}_{layer_id}lora_A"
                        key_B = f"{proj_type}_{layer_id}lora_B"
                        
                        if key_A in weight_list and key_B in weight_list:
                            # è®¡ç®— LoRA delta: B @ A * scaling
                            lora_A = weight_list[key_A]
                            lora_B = weight_list[key_B]
                            lora_delta = (lora_B @ lora_A) * scaling
                            
                            # Merged weight = base + delta
                            merged_weight = base_weight + lora_delta
                            
                            key = f"{proj_type}_{layer_id}weight"
                            weight_list[key] = merged_weight
                        else:
                            # æ²¡æœ‰ LoRA,ä½¿ç”¨åŸå§‹ base weight
                            key = f"{proj_type}_{layer_id}weight"
                            weight_list[key] = base_weight
    
    num_weights = len([k for k in weight_list if 'weight' in k])
    print(f"âœ“ ç”Ÿæˆ {num_weights} ä¸ª merged weights")
    return weight_list

def add_constraint_from_subspace(weight_list, subspace_dir, dimension, num_layers=28):
    """ä»å­ç©ºé—´å‘é‡ V æ„å»ºçº¦æŸçŸ©é˜µ C å¹¶æ·»åŠ åˆ° weight_list"""
    print(f"\nğŸ”§ ä»å­ç©ºé—´æ„å»ºçº¦æŸçŸ©é˜µ (ç»´åº¦: {dimension})...")
    print(f"   å­ç©ºé—´ç›®å½•: {subspace_dir}")
    
    added_count = 0
    for layer_id in range(num_layers):
        try:
            # ä¸ºæ¯å±‚æ„å»ºçº¦æŸçŸ©é˜µ
            C, V = load_and_build_constraint_matrix(subspace_dir, dimension, layer_id)
            
            for proj_type in ['q_proj', 'v_proj']:
                key_prefix = f"{proj_type}_{layer_id}"
                
                # æ£€æŸ¥è¯¥å±‚æ˜¯å¦æœ‰ LoRA B
                if f"{key_prefix}lora_B" in weight_list:
                    out_dim = weight_list[f"{key_prefix}lora_B"].shape[0]
                    
                    # è£å‰ªåˆ°åŒ¹é…ç»´åº¦
                    C_block = C[:out_dim, :out_dim].clone()
                    V_block = V[:out_dim, :].clone()
                    
                    weight_list[f"{key_prefix}lora_C"] = C_block
                    weight_list[f"{key_prefix}_V"] = V_block
                    
                    added_count += 1
        
        except FileNotFoundError as e:
            print(f"  âš ï¸ Layer {layer_id}: æœªæ‰¾åˆ°å­ç©ºé—´æ–‡ä»¶ï¼Œè·³è¿‡")
            continue
    
    print(f"âœ“ æˆåŠŸæ·»åŠ  {added_count} ä¸ªçº¦æŸçŸ©é˜µ")
    
    # æ·»åŠ å…ƒæ•°æ®
    weight_list['divide_num'] = 2
    
    return weight_list

def save_abc_file(weight_list, output_path):
    """ä¿å­˜ä¸º ABC.pt æ–‡ä»¶"""
    # è‡ªåŠ¨åˆ›å»ºçˆ¶ç›®å½•(å¦‚æœä¸å­˜åœ¨)
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ä¿å­˜ ABC.pt æ–‡ä»¶: {output_path}")
    torch.save(weight_list, str(output_path))
    
    # ç»Ÿè®¡
    stats = {
        'total': len(weight_list),
        'lora_A': sum(1 for k in weight_list if 'lora_A' in k),
        'lora_B': sum(1 for k in weight_list if 'lora_B' in k),
        'lora_C': sum(1 for k in weight_list if 'lora_C' in k),
        'V': sum(1 for k in weight_list if '_V' in k and 'lora' not in k),
        'weight': sum(1 for k in weight_list if 'weight' in k)
    }
    
    print(f"âœ“ ä¿å­˜æˆåŠŸ!")
    print(f"  - æ€»é”®æ•°: {stats['total']}")
    print(f"  - lora_A: {stats['lora_A']}")
    print(f"  - lora_B: {stats['lora_B']}")
    print(f"  - lora_C: {stats['lora_C']}")
    print(f"  - V (subspace): {stats['V']}")
    print(f"  - weight: {stats['weight']}")
    
    file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"  - æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")

def main():
    parser = argparse.ArgumentParser(description="ä»å­ç©ºé—´ V æ„å»º C å¹¶è½¬æ¢ä¸º ABC.pt")
    parser.add_argument('--lora_adapter_path', type=str, required=True,
                        help='LoRA adapter è·¯å¾„')
    parser.add_argument('--subspace_dir', type=str, required=True,
                        help='å­ç©ºé—´æ–‡ä»¶ç›®å½•')
    parser.add_argument('--dimension', type=str, default='safety',
                        help='åå¥½ç»´åº¦ (safety, helpfulness, etc.)')
    parser.add_argument('--base_model_name', type=str, required=True,
                        help='Base model åç§°')
    parser.add_argument('--output_path', type=str, required=True,
                        help='è¾“å‡º ABC.pt æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_layers', type=int, default=28,
                        help='æ¨¡å‹å±‚æ•°')
    
    args = parser.parse_args()
    
    print("="*70)
    print("ğŸ”„ Text2SaLoRA â†’ SaLoRA ABC.pt è½¬æ¢å™¨ (ä»å­ç©ºé—´ V æ„å»º C)")
    print("="*70)
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"  - LoRA adapter: {args.lora_adapter_path}")
    print(f"  - å­ç©ºé—´ç›®å½•: {args.subspace_dir}")
    print(f"  - åå¥½ç»´åº¦: {args.dimension}")
    print(f"  - Base model: {args.base_model_name}")
    print(f"  - è¾“å‡ºæ–‡ä»¶: {args.output_path}")
    print(f"  - å±‚æ•°: {args.num_layers}")
    print()
    
    # 1. åŠ è½½ LoRA adapter
    model, base_model = load_lora_adapter(args.base_model_name, args.lora_adapter_path)
    
    # 2. æå– LoRA A, B çŸ©é˜µå’Œ base weights
    weight_list = extract_lora_weights(model, base_model)
    
    # 3. ä»å­ç©ºé—´ V æ„å»ºçº¦æŸçŸ©é˜µ C
    weight_list = add_constraint_from_subspace(
        weight_list, 
        args.subspace_dir, 
        args.dimension, 
        args.num_layers
    )
    
    # 4. ä¿å­˜ä¸º ABC.pt
    save_abc_file(weight_list, args.output_path)
    
    print("\n" + "="*70)
    print("âœ… è½¬æ¢å®Œæˆ!")
    print(f"\nğŸ’¡ çº¦æŸçŸ©é˜µ C æ˜¯ä»å­ç©ºé—´å‘é‡ V æ„å»ºçš„:")
    print(f"   C = I - V @ V^T")
    print("="*70)

if __name__ == "__main__":
    main()
