"""
æ•°æ®é›†åŠ è½½æ¨¡å—
æ”¯æŒåŠ è½½ UltraFeedback Binarized åå¥½æ•°æ®é›†
"""

import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from torch.utils.data import Dataset
import random


class PreferenceDataset(Dataset):
    """åå¥½æ•°æ®é›† - åŠ è½½ chosen/rejected å¯¹"""
    
    def __init__(
        self,
        data_path: str,
        tokenizer,
        max_length: int = 512,
        use_chosen_only: bool = True,  # åªç”¨ chosen åš SFT
        format_type: str = "instruction"  # instruction æˆ– conversation
    ):
        """
        Args:
            data_path: JSONL æ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: tokenizer
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            use_chosen_only: æ˜¯å¦åªä½¿ç”¨ chosen å“åº”ï¼ˆç”¨äº SFTï¼‰
            format_type: æ ¼å¼åŒ–ç±»å‹
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_chosen_only = use_chosen_only
        self.format_type = format_type
        
        # åŠ è½½æ•°æ®
        self.data = self._load_jsonl(data_path)
        print(f"âœ… åŠ è½½äº† {len(self.data)} æ¡æ•°æ®")
    
    def _load_jsonl(self, path: str) -> List[Dict]:
        """åŠ è½½ JSONL æ–‡ä»¶"""
        data = []
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                data.append(item)
        return data
    
    def _format_instruction(self, prompt: str, response: str) -> str:
        """æ ¼å¼åŒ–ä¸ºæŒ‡ä»¤æ ¼å¼"""
        return f"### Instruction:\n{prompt}\n\n### Response:\n{response}"
    
    def _format_conversation(self, prompt: str, response: str) -> str:
        """æ ¼å¼åŒ–ä¸ºå¯¹è¯æ ¼å¼"""
        # Qwen æ ¼å¼
        return f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n{response}<|im_end|>"
    
    def format_text(self, prompt: str, response: str) -> str:
        """æ ¼å¼åŒ–æ–‡æœ¬"""
        if self.format_type == "instruction":
            return self._format_instruction(prompt, response)
        elif self.format_type == "conversation":
            return self._format_conversation(prompt, response)
        else:
            return f"{prompt}\n{response}"
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        prompt = item['prompt']
        
        # é€‰æ‹©å“åº”
        if self.use_chosen_only:
            response = item['chosen']
        else:
            # éšæœºé€‰æ‹© chosen æˆ– rejectedï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
            response = random.choice([item['chosen'], item['rejected']])
        
        # æ ¼å¼åŒ–æ–‡æœ¬
        text = self.format_text(prompt, response)
        
        # Tokenize
        encoding = self.tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': encoding['input_ids'].squeeze(0)
        }


class UltraFeedbackLoader:
    """UltraFeedback æ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, base_dir: str = "../datasets/ultrafeedback_binarized"):
        self.base_dir = Path(base_dir)
        if not self.base_dir.exists():
            raise FileNotFoundError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {self.base_dir}")
    
    def get_available_datasets(self) -> Dict[str, Path]:
        """è·å–å¯ç”¨çš„æ•°æ®é›†"""
        datasets = {}
        
        # è®­ç»ƒé›†
        train_files = {
            'train_100': 'train_prefs_ultrafeedback_binarized_100.jsonl',
            'train_1k': 'train_prefs_ultrafeedback_binarized_1k.jsonl',
            'train_3k': 'train_prefs_ultrafeedback_binarized_3k.jsonl',
            'train_1w': 'train_prefs_ultrafeedback_binarized_1w.jsonl',
            'train_full': 'train_prefs_ultrafeedback_binarized.jsonl',
        }
        
        # æµ‹è¯•é›†
        test_files = {
            'test_100': 'test_prefs_ultrafeedback_binarized_100.jsonl',
            'test_full': 'test_prefs_ultrafeedback_binarized.jsonl',
        }
        
        all_files = {**train_files, **test_files}
        
        for name, filename in all_files.items():
            path = self.base_dir / filename
            if path.exists():
                datasets[name] = path
        
        return datasets
    
    def load_dataset(
        self,
        dataset_name: str,
        tokenizer,
        max_length: int = 512,
        use_chosen_only: bool = True,
        format_type: str = "instruction"
    ) -> PreferenceDataset:
        """åŠ è½½æŒ‡å®šçš„æ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§° (train_100, train_1k, test_100, ç­‰)
            tokenizer: tokenizer
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            use_chosen_only: æ˜¯å¦åªä½¿ç”¨ chosen å“åº”
            format_type: æ ¼å¼åŒ–ç±»å‹
        """
        available = self.get_available_datasets()
        
        if dataset_name not in available:
            raise ValueError(
                f"æ•°æ®é›† '{dataset_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨çš„æ•°æ®é›†:\n" + 
                "\n".join(f"  - {name}" for name in available.keys())
            )
        
        data_path = available[dataset_name]
        print(f"ğŸ“š åŠ è½½æ•°æ®é›†: {dataset_name}")
        print(f"   è·¯å¾„: {data_path}")
        
        dataset = PreferenceDataset(
            data_path=str(data_path),
            tokenizer=tokenizer,
            max_length=max_length,
            use_chosen_only=use_chosen_only,
            format_type=format_type
        )
        
        return dataset
    
    def print_sample(self, dataset_name: str, num_samples: int = 2):
        """æ‰“å°æ•°æ®æ ·æœ¬ï¼ˆä¸éœ€è¦tokenizerï¼‰"""
        available = self.get_available_datasets()
        if dataset_name not in available:
            print(f"âŒ æ•°æ®é›† '{dataset_name}' ä¸å­˜åœ¨")
            return
        
        data_path = available[dataset_name]
        
        print(f"\n{'='*80}")
        print(f"æ•°æ®é›†: {dataset_name}")
        print(f"è·¯å¾„: {data_path}")
        print(f"{'='*80}\n")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= num_samples:
                    break
                
                item = json.loads(line.strip())
                print(f"æ ·æœ¬ {i+1}:")
                print(f"  Prompt: {item['prompt'][:100]}...")
                print(f"  Chosen length: {len(item['chosen'])} chars")
                print(f"  Rejected length: {len(item['rejected'])} chars")
                print()


def load_ultrafeedback_data(
    dataset_size: str = "100",  # "100", "1k", "3k", "1w", "full"
    tokenizer = None,
    max_length: int = 512,
    use_chosen_only: bool = True,
    format_type: str = "instruction",
    split: str = "train"  # "train" or "test"
) -> Tuple[PreferenceDataset, Optional[PreferenceDataset]]:
    """ä¾¿æ·å‡½æ•°ï¼šåŠ è½½ UltraFeedback æ•°æ®
    
    Args:
        dataset_size: æ•°æ®é›†å¤§å° ("100", "1k", "3k", "1w", "full")
        tokenizer: tokenizer
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        use_chosen_only: æ˜¯å¦åªä½¿ç”¨ chosen å“åº”
        format_type: æ ¼å¼åŒ–ç±»å‹
        split: "train", "test", æˆ– "both"
    
    Returns:
        (train_dataset, test_dataset) å¦‚æœ split="both"
        train_dataset å¦‚æœ split="train"
        test_dataset å¦‚æœ split="test"
    """
    loader = UltraFeedbackLoader()
    
    train_dataset = None
    test_dataset = None
    
    if split in ["train", "both"]:
        train_name = f"train_{dataset_size}"
        train_dataset = loader.load_dataset(
            train_name, tokenizer, max_length, use_chosen_only, format_type
        )
    
    if split in ["test", "both"]:
        test_name = "test_100" if dataset_size in ["100", "1k"] else "test_full"
        test_dataset = loader.load_dataset(
            test_name, tokenizer, max_length, use_chosen_only, format_type
        )
    
    if split == "both":
        return train_dataset, test_dataset
    elif split == "train":
        return train_dataset
    else:
        return test_dataset


# å‘½ä»¤è¡Œæµ‹è¯•
if __name__ == '__main__':
    print("UltraFeedback æ•°æ®é›†åŠ è½½å™¨æµ‹è¯•\n")
    
    loader = UltraFeedbackLoader()
    
    print("ğŸ“‹ å¯ç”¨çš„æ•°æ®é›†:")
    available = loader.get_available_datasets()
    for name, path in available.items():
        size = path.stat().st_size / (1024 * 1024)  # MB
        print(f"  - {name:20s} ({size:.1f} MB)")
    
    print("\n" + "="*80)
    print("æŸ¥çœ‹æ ·æœ¬æ•°æ®:")
    print("="*80)
    
    # æ‰“å°ä¸€äº›æ ·æœ¬
    loader.print_sample("train_100", num_samples=2)
