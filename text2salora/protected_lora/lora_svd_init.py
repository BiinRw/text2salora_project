"""
LoRA SVD åˆå§‹åŒ–æ¨¡å—

å®ç° SaLoRA è®ºæ–‡ä¸­çš„åˆå§‹åŒ–æ–¹æ³•ï¼š
1. SVD åˆ†è§£åŸå§‹æƒé‡
2. æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´ï¼ˆæˆ–å­ç©ºé—´å†…ï¼‰
3. æ›´æ–°åŸºç¡€æƒé‡
"""

import torch
from typing import Optional
import sys
import re
from pathlib import Path

def parse_projection_from_module_name(module_name: str) -> Optional[str]:
    """
    ä»æ¨¡å—åè§£ææŠ•å½±ç±»å‹
    
    Args:
        module_name: å¦‚ "model.layers.0.self_attn.q_proj.lora_A.weight"
        
    Returns:
        projection_type: å¦‚ "q_proj", "k_proj", ç­‰,æˆ– None
    """
    if 'q_proj' in module_name:
        return 'q_proj'
    elif 'k_proj' in module_name:
        return 'k_proj'
    elif 'v_proj' in module_name:
        return 'v_proj'
    elif 'o_proj' in module_name:
        return 'o_proj'
    elif 'up_proj' in module_name:
        return 'up_proj'
    elif 'down_proj' in module_name:
        return 'down_proj'
    elif 'gate_proj' in module_name:
        return 'gate_proj'
    else:
        return None


def load_projection_subspace(subspace_dir: str, dimension: str, projection: str, layer_id: int, device: str) -> Optional[torch.Tensor]:
    """
    åŠ è½½æŒ‡å®šæŠ•å½±å±‚çš„å­ç©ºé—´çŸ©é˜µ C
    
    Args:
        subspace_dir: å­ç©ºé—´æ–‡ä»¶ç›®å½•
        dimension: åå¥½ç»´åº¦ (safety/helpfulness/etc.)
        projection: æŠ•å½±ç±»å‹ (q_proj/k_proj/etc.)
        layer_id: å±‚ç¼–å·
        device: è®¾å¤‡
        
    Returns:
        C: çº¦æŸçŸ©é˜µ (d_out, d_out) æˆ– None
    """
    subspace_dir = Path(subspace_dir)
    
    # å°è¯•åŠ è½½ per-layer æ–‡ä»¶: {dimension}_{projection}_layer{id}_subspace.pt
    subspace_file = subspace_dir / f"{dimension}_{projection}_layer{layer_id}_subspace.pt"
    
    # å¦‚æœä¸å­˜åœ¨,å°è¯• fused æ–‡ä»¶
    if not subspace_file.exists():
        subspace_file = subspace_dir / f"{dimension}_{projection}_fused_subspace.pt"
        
    if not subspace_file.exists():
        return None
    
    try:
        data = torch.load(subspace_file, map_location=device)
        V = data['V'].to(device)  # (d_out, k)
        
        # è®¡ç®—çº¦æŸçŸ©é˜µ: C = I - V @ V^T
        d_out = V.shape[0]
        I = torch.eye(d_out, device=device, dtype=V.dtype)
        C = I - torch.mm(V, V.t())
        
        return C
        
    except Exception as e:
        print(f"   âš ï¸  åŠ è½½å­ç©ºé—´å¤±è´¥: {subspace_file} | {e}")
        return None

def initialize_lora_weights(
    model,
    constraint=None,
    rank: int = 16,
    method: str = 'random',
    niter: int = 30,
    verbose: bool = True
):
    """
    åˆå§‹åŒ– LoRA æƒé‡
    
    Args:
        model: PEFT LoRA æ¨¡å‹
        constraint: OrthogonalConstraint å¯¹è±¡ï¼ŒåŒ…å«å­ç©ºé—´æŠ•å½±çŸ©é˜µ C
        rank: LoRA çš„ç§©
        method: åˆå§‹åŒ–æ–¹æ³•
            - 'random': PEFT é»˜è®¤éšæœºåˆå§‹åŒ–ï¼ˆä¸åšä»»ä½•å¤„ç†ï¼‰
            - 'svd': PiSSA æ–¹æ³•ï¼ˆSVD åˆ†è§£ï¼Œä¸æŠ•å½±ï¼‰
            - 'svd_ortho': SVD + æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´ï¼ˆæ¨èï¼‰
            - 'svd_salora': SaLoRA åŸå§‹æ–¹æ³•ï¼ˆSVD + æŠ•å½±åˆ°å­ç©ºé—´å†…ï¼‰
        niter: SVD è¿­ä»£æ¬¡æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        åˆå§‹åŒ–çš„æ¨¡å—æ•°é‡
    """
    
    if method == 'random':
        if verbose:
            print("âœ… ä½¿ç”¨é»˜è®¤éšæœºåˆå§‹åŒ–")
        return 0
    
    if method in ['svd_ortho', 'svd_salora'] and constraint is None:
        raise ValueError(f"æ–¹æ³• '{method}' éœ€è¦æä¾› constraint å¯¹è±¡")
    
    if verbose:
        print(f"\nğŸ”§ ä½¿ç”¨ {method} æ–¹æ³•åˆå§‹åŒ– LoRA...")
        print(f"   Rank: {rank}, SVDè¿­ä»£æ¬¡æ•°: {niter}")
    
    initialized_count = 0
    
    for name, module in model.named_modules():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA æ¨¡å—
        if not (hasattr(module, 'lora_A') and hasattr(module, 'lora_B')):
            continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ base_layer
        if not hasattr(module, 'base_layer'):
            if verbose:
                print(f"âš ï¸ {name} æ²¡æœ‰ base_layerï¼Œè·³è¿‡")
            continue
        
        try:
            # è·å–åŸºç¡€æƒé‡
            base_weight = module.base_layer.weight.data
            d_out, d_in = base_weight.shape
            
            if verbose:
                print(f"\nå¤„ç†æ¨¡å—: {name}")
                print(f"  æƒé‡å½¢çŠ¶: {base_weight.shape}")
            
            # SVD åˆ†è§£
            if verbose:
                print(f"  æ‰§è¡Œ SVD åˆ†è§£ (rank={rank})...")
            
            U, S, V = torch.svd_lowrank(
                base_weight.float(), 
                q=min(rank, min(d_out, d_in)), 
                niter=niter
            )
            
            # è½¬æ¢å›åŸå§‹æ•°æ®ç±»å‹
            U = U.to(base_weight.dtype)
            S = S.to(base_weight.dtype)
            V = V.to(base_weight.dtype)
            
            # è®¡ç®— sqrt(S)
            sqrt_S = torch.sqrt(S)
            
            # åˆå§‹åŒ– B å’Œ A
            # B: (d_out, r)
            # A: (r, d_in)
            B_init = U @ torch.diag(sqrt_S)
            A_init = torch.diag(sqrt_S) @ V.T
            
            if verbose:
                print(f"  B_init å½¢çŠ¶: {B_init.shape}, A_init å½¢çŠ¶: {A_init.shape}")
            
            # æ ¹æ®æ–¹æ³•é€‰æ‹©æŠ•å½±æ–¹å¼
            if method == 'svd_ortho':
                # ğŸ†• æ–°ç‰ˆæœ¬: ä»projectionç‰¹å®šçš„å­ç©ºé—´æ–‡ä»¶åŠ è½½CçŸ©é˜µ
                if verbose:
                    print(f"  æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´...")
                
                # æå–å±‚ç¼–å·
                layer_match = re.search(r'\.layers\.(\d+)\.', name)
                if not layer_match:
                    if verbose:
                        print(f"   âš ï¸  æ— æ³•æå–å±‚ç¼–å·: {name}, ä½¿ç”¨å•ä½çŸ©é˜µ")
                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                else:
                    layer_id = int(layer_match.group(1))
                    
                    # è§£ææŠ•å½±ç±»å‹
                    projection = parse_projection_from_module_name(name)
                    
                    if projection is None:
                        if verbose:
                            print(f"   âš ï¸  æ— æ³•è¯†åˆ«æŠ•å½±ç±»å‹: {name}, ä½¿ç”¨å•ä½çŸ©é˜µ")
                        C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                    else:
                        # ä»æ–‡ä»¶åŠ è½½projectionç‰¹å®šçš„CçŸ©é˜µ
                        # éœ€è¦ä»constraintå¯¹è±¡è·å–subspace_dirå’Œdimension
                        if hasattr(constraint, 'subspace_dir') and hasattr(constraint, 'dimension'):
                            C = load_projection_subspace(
                                constraint.subspace_dir,
                                constraint.dimension,
                                projection,
                                layer_id,
                                B_init.device
                            )
                            
                            if C is not None:
                                # æ£€æŸ¥ç»´åº¦åŒ¹é…
                                if C.shape[0] != d_out:
                                    if verbose:
                                        print(f"   âš ï¸  C å½¢çŠ¶ {C.shape} ä¸ d_out {d_out} ä¸åŒ¹é… ({projection} Layer {layer_id}), ä½¿ç”¨å•ä½çŸ©é˜µ")
                                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                                elif verbose:
                                    print(f"   âœ… åŠ è½½å­ç©ºé—´: {projection} Layer {layer_id} | C shape: {C.shape}")
                            else:
                                if verbose:
                                    print(f"   â„¹ï¸  æœªæ‰¾åˆ°å­ç©ºé—´æ–‡ä»¶: {projection} Layer {layer_id}, ä½¿ç”¨å•ä½çŸ©é˜µ")
                                C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                        else:
                            # å…¼å®¹æ—§ç‰ˆconstraintå¯¹è±¡(ç›´æ¥ä¼ PçŸ©é˜µ)
                            dim = constraint.dimensions[0] if hasattr(constraint, 'dimensions') and constraint.dimensions else "safety"
                            P_data = constraint.projection_matrices.get(dim) if hasattr(constraint, 'projection_matrices') else None
                            if isinstance(P_data, dict):
                                C = P_data.get(layer_id)
                                if C is None:
                                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                            else:
                                C = P_data if P_data is not None else torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                
                # ç¡®ä¿Cåœ¨æ­£ç¡®çš„è®¾å¤‡å’Œç±»å‹ä¸Š
                if C is not None:
                    if not isinstance(C, torch.Tensor):
                        C = torch.tensor(C)
                    C = C.to(device=B_init.device, dtype=B_init.dtype)
                else:
                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                
                # ç¡®ä¿ C çš„å½¢çŠ¶ä¸ B_init åŒ¹é…
                if C.shape[0] != d_out:
                    if verbose:
                        print(f"  âš ï¸ C å½¢çŠ¶ {C.shape} ä¸ d_out {d_out} ä¸åŒ¹é…ï¼Œè·³è¿‡æŠ•å½±")
                    continue
                else:
                    I_minus_C = torch.eye(d_out, device=C.device, dtype=C.dtype) - C
                    # æŠ•å½±åˆ°æ­£äº¤è¡¥ç©ºé—´: B' = (I - C) @ B
                    B_init = I_minus_C @ B_init
                    
                    if verbose:
                        print(f"  æŠ•å½±å B_init å½¢çŠ¶: {B_init.shape}")
            
            elif method == 'svd_salora':
                # ğŸ†• æ–°ç‰ˆæœ¬: ä»projectionç‰¹å®šçš„å­ç©ºé—´æ–‡ä»¶åŠ è½½CçŸ©é˜µ
                if verbose:
                    print(f"  æŠ•å½±åˆ°å­ç©ºé—´å†… (SaLoRA æ–¹æ³•)...")
                
                # æå–å±‚ç¼–å·
                layer_match = re.search(r'\.layers\.(\d+)\.', name)
                if not layer_match:
                    if verbose:
                        print(f"   âš ï¸  æ— æ³•æå–å±‚ç¼–å·: {name}, ä½¿ç”¨å•ä½çŸ©é˜µ")
                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                else:
                    layer_id = int(layer_match.group(1))
                    
                    # è§£ææŠ•å½±ç±»å‹
                    projection = parse_projection_from_module_name(name)
                    
                    if projection is None:
                        if verbose:
                            print(f"   âš ï¸  æ— æ³•è¯†åˆ«æŠ•å½±ç±»å‹: {name}, ä½¿ç”¨å•ä½çŸ©é˜µ")
                        C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                    else:
                        # ä»æ–‡ä»¶åŠ è½½projectionç‰¹å®šçš„CçŸ©é˜µ
                        if hasattr(constraint, 'subspace_dir') and hasattr(constraint, 'dimension'):
                            C = load_projection_subspace(
                                constraint.subspace_dir,
                                constraint.dimension,
                                projection,
                                layer_id,
                                B_init.device
                            )
                            
                            if C is not None:
                                # æ£€æŸ¥ç»´åº¦åŒ¹é…
                                if C.shape[0] != d_out:
                                    if verbose:
                                        print(f"   âš ï¸  C å½¢çŠ¶ {C.shape} ä¸ d_out {d_out} ä¸åŒ¹é… ({projection} Layer {layer_id}), ä½¿ç”¨å•ä½çŸ©é˜µ")
                                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                                elif verbose:
                                    print(f"   âœ… åŠ è½½å­ç©ºé—´: {projection} Layer {layer_id} | C shape: {C.shape}")
                            else:
                                if verbose:
                                    print(f"   â„¹ï¸  æœªæ‰¾åˆ°å­ç©ºé—´æ–‡ä»¶: {projection} Layer {layer_id}, ä½¿ç”¨å•ä½çŸ©é˜µ")
                                C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                        else:
                            # å…¼å®¹æ—§ç‰ˆconstraintå¯¹è±¡
                            dim = constraint.dimensions[0] if hasattr(constraint, 'dimensions') and constraint.dimensions else "safety"
                            P_data = constraint.projection_matrices.get(dim) if hasattr(constraint, 'projection_matrices') else None
                            if isinstance(P_data, dict):
                                C = P_data.get(layer_id)
                                if C is None:
                                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                            else:
                                C = P_data if P_data is not None else torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                
                # ç¡®ä¿Cåœ¨æ­£ç¡®çš„è®¾å¤‡å’Œç±»å‹ä¸Š
                if C is not None:
                    if not isinstance(C, torch.Tensor):
                        C = torch.tensor(C)
                    C = C.to(device=B_init.device, dtype=B_init.dtype)
                else:
                    C = torch.eye(d_out, device=B_init.device, dtype=B_init.dtype)
                
                if C.shape[0] != d_out:
                    if verbose:
                        print(f"  âš ï¸ C å½¢çŠ¶ {C.shape} ä¸ d_out {d_out} ä¸åŒ¹é…ï¼Œè·³è¿‡æŠ•å½±")
                    continue
                else:
                    # æŠ•å½±åˆ°å­ç©ºé—´å†…: B' = C @ B
                    B_init = C @ B_init
                    
                    if verbose:
                        print(f"  æŠ•å½±å B_init å½¢çŠ¶: {B_init.shape}")
                    
                    # âœ… å…³é”®æ­¥éª¤ï¼šæ›´æ–° base_weight (ä¸åŸç‰ˆ SaLoRA ä¸€è‡´)
                    # åŸç†ï¼šä½¿ LoRA ä» 0 è´¡çŒ®å¼€å§‹
                    # è¾“å‡º = (W - B@A) @ x + B @ A @ x = W @ x
                    # æ³¨æ„ï¼šå› ä¸º C @ B = B (B åœ¨å­ç©ºé—´å†…)ï¼Œæ‰€ä»¥å®é™…å‡å» B @ A
                    base_layer = module.base_layer
                    if hasattr(base_layer, 'weight'):
                        with torch.no_grad():
                            # è®¡ç®—è¦å‡å»çš„éƒ¨åˆ†ï¼šB @ A
                            delta = (B_init @ A_init).to(base_layer.weight.dtype)
                            base_layer.weight.data.sub_(delta)
                            
                            if verbose:
                                print(f"  âœ… å·²æ›´æ–° base_weight: å‡å» B@A (å½¢çŠ¶: {delta.shape})")
                                print(f"     ç¡®ä¿åˆå§‹åŒ–åæ¨¡å‹è¾“å‡ºä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€è‡´")
            
            # èµ‹å€¼ç»™ LoRA å‚æ•°
            # æ³¨æ„ï¼šPEFT çš„ lora_B å’Œ lora_A çš„ weight éœ€è¦è½¬ç½®
            # lora_B: Linear(r, d_out) -> weight: (d_out, r)
            # lora_A: Linear(d_in, r) -> weight: (r, d_in)
            
            if 'default' in module.lora_A:
                adapter_name = 'default'
            else:
                adapter_name = list(module.lora_A.keys())[0]
            
            # B_init: (d_out, r) -> lora_B.weight: (d_out, r)
            # ä½¿ç”¨ with torch.no_grad() é¿å…åˆ›å»ºè®¡ç®—å›¾ï¼ŒåŒæ—¶ä¿æŒ requires_grad
            with torch.no_grad():
                module.lora_B[adapter_name].weight.copy_(B_init.detach())
            # ç¡®ä¿ requires_grad=True
            module.lora_B[adapter_name].weight.requires_grad_(True)
            
            # A_init: (r, d_in) -> lora_A.weight: (r, d_in)
            with torch.no_grad():
                module.lora_A[adapter_name].weight.copy_(A_init.detach())
            # ç¡®ä¿ requires_grad=True
            module.lora_A[adapter_name].weight.requires_grad_(True)
            
            if verbose:
                print(f"  âœ… å·²èµ‹å€¼ lora_A å’Œ lora_B")
            
            # æ›´æ–°åŸºç¡€æƒé‡: W' = W - B @ A
            # è¿™æ ·ä¿è¯ W' + B @ A = Wï¼ˆåˆå§‹è¾“å‡ºä¸å˜ï¼‰
            # è®¡ç®— BA ä¹˜ç§¯å¹¶ detachï¼Œç¡®ä¿ä¸å½±å“è®¡ç®—å›¾
            BA_product = (B_init @ A_init).detach()
            module.base_layer.weight.data.sub_(BA_product)
            
            if verbose:
                print(f"  âœ… å·²æ›´æ–°åŸºç¡€æƒé‡")
                # éªŒè¯
                reconstructed = module.base_layer.weight.data + BA_product
                error = torch.norm(reconstructed - base_weight) / torch.norm(base_weight)
                print(f"  é‡æ„è¯¯å·®: {error.item():.6f}")
                
                # éªŒè¯æ¢¯åº¦çŠ¶æ€
                print(f"  ğŸ“Š æ¢¯åº¦çŠ¶æ€:")
                print(f"     lora_B.requires_grad: {module.lora_B[adapter_name].weight.requires_grad}")
                print(f"     lora_A.requires_grad: {module.lora_A[adapter_name].weight.requires_grad}")
                print(f"     base_layer.requires_grad: {module.base_layer.weight.requires_grad}")
            
            initialized_count += 1
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– {name} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if verbose:
        print(f"\nâœ… LoRA åˆå§‹åŒ–å®Œæˆï¼å…±åˆå§‹åŒ– {initialized_count} ä¸ªæ¨¡å—")
    
    return initialized_count




if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("LoRA SVD åˆå§‹åŒ–æ¨¡å—")
    print("æ”¯æŒçš„æ–¹æ³•:")
    print("  - random: é»˜è®¤éšæœºåˆå§‹åŒ–")
    print("  - svd: PiSSA æ–¹æ³•")
    print("  - svd_ortho: SVD + æ­£äº¤è¡¥æŠ•å½± (æ¨è)")
    print("  - svd_salora: SaLoRA åŸå§‹æ–¹æ³•")