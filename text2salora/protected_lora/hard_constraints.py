"""
SaLoRA é£æ ¼çš„ç¡¬çº¦æŸå®ç°
é€šè¿‡åœ¨ forward pass ä¸­æ·»åŠ æŠ•å½±çŸ©é˜µ C^Tï¼Œç›´æ¥çº¦æŸè¾“å‡ºè¡¨å¾
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional
from pathlib import Path
import sys

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from utils.svd_utils import PreferenceSubspaceManager


class HardConstraintManager:
    """ç¡¬çº¦æŸç®¡ç†å™¨ - SaLoRA é£æ ¼çš„è¡¨å¾ç©ºé—´çº¦æŸ"""
    
    def __init__(
        self,
        subspace_manager: PreferenceSubspaceManager,
        dimensions: List[str],
        use_fused: bool = True,
        device: str = 'cuda:0'
    ):
        """
        Args:
            subspace_manager: å­ç©ºé—´ç®¡ç†å™¨
            dimensions: éœ€è¦çº¦æŸçš„åå¥½ç»´åº¦
            use_fused: æ˜¯å¦ä½¿ç”¨èåˆå­ç©ºé—´
            device: è®¾å¤‡
        """
        self.manager = subspace_manager
        self.dimensions = dimensions
        self.use_fused = use_fused
        self.device = device
        
        # é¢„è®¡ç®—æŠ•å½±çŸ©é˜µ C = V @ V^T
        self._prepare_projection_matrices()
        
        # å­˜å‚¨å·²æ³¨å†Œçš„ hook handles
        self.hook_handles = []
    
    def _prepare_projection_matrices(self):
        """é¢„è®¡ç®—æ‰€æœ‰åå¥½ç»´åº¦çš„æŠ•å½±çŸ©é˜µ C = V @ V^T"""
        self.projection_matrices = {}
        
        print(f"\nğŸ”’ é¢„è®¡ç®—ç¡¬çº¦æŸæŠ•å½±çŸ©é˜µ (SaLoRA é£æ ¼)...")
        
        for dim in self.dimensions:
            if self.use_fused:
                # èåˆå­ç©ºé—´: ä¸€ä¸ªæŠ•å½±çŸ©é˜µ
                V = self.manager.get_subspace(dim, layer_id=None)
                C = V @ V.T  # (hidden_dim, hidden_dim)
                self.projection_matrices[dim] = C
                print(f"   {dim}: C = V @ V^T, shape={C.shape}")
            else:
                raise NotImplementedError("å½“å‰ä»…æ”¯æŒèåˆå­ç©ºé—´")
    
    def apply_hard_constraint(self, model: nn.Module) -> None:
        """
        ä¸ºæ¨¡å‹çš„æ‰€æœ‰ LoRA å±‚æ³¨å…¥ç¡¬çº¦æŸ
        
        ç­–ç•¥: ä½¿ç”¨ forward hook åœ¨ LoRA è¾“å‡ºåæ·»åŠ  @ C^T æŠ•å½±
        
        Args:
            model: åŒ…å« LoRA å±‚çš„æ¨¡å‹
        """
        print(f"\nğŸ”§ æ³¨å…¥ç¡¬çº¦æŸåˆ° LoRA å±‚...")
        
        # è®¡ç®—èåˆçš„æŠ•å½±çŸ©é˜µ (æ‰€æœ‰ç»´åº¦çš„äº¤é›†)
        C_combined = None
        for dim in self.dimensions:
            C = self.projection_matrices[dim]
            if C_combined is None:
                C_combined = C
            else:
                # å¤šä¸ªç»´åº¦: å–æŠ•å½±çŸ©é˜µçš„ä¹˜ç§¯ (äº¤é›†)
                C_combined = C_combined @ C
        
        # è½¬ç½®å‡†å¤‡å¥½ C^T (å› ä¸º forward ä¸­ä¼šç”¨ @ C^T)
        C_T = C_combined.T.to(self.device)
        
        # ä¸ºæ‰€æœ‰ LoRA å±‚æ·»åŠ  lora_C å±æ€§
        lora_layer_count = 0
        for name, module in model.named_modules():
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                # æ‰¾åˆ° PEFT LoRA å±‚
                module.lora_C = C_T.clone()  # æ¯å±‚ä¸€ä¸ªå‰¯æœ¬
                module.lora_C.requires_grad_(False)  # å›ºå®šä¸è®­ç»ƒ
                
                # æ³¨å†Œ forward hook
                handle = module.register_forward_hook(self._lora_projection_hook)
                self.hook_handles.append(handle)
                
                lora_layer_count += 1
                if lora_layer_count <= 3:  # åªæ‰“å°å‰3å±‚
                    print(f"   âœ“ {name}: å·²æ³¨å…¥ lora_C (shape={C_T.shape})")
        
        print(f"   ğŸ“Š å…±ä¸º {lora_layer_count} ä¸ª LoRA å±‚æ³¨å…¥ç¡¬çº¦æŸ")
        print(f"   ğŸ”’ çº¦æŸçŸ©é˜µ C^T å›ºå®šä¸è®­ç»ƒ")
    
    @staticmethod
    def _lora_projection_hook(module, input, output):
        """
        Forward hook: åœ¨ LoRA è¾“å‡ºåæ·»åŠ  @ C^T æŠ•å½±
        
        PEFT LoRA çš„ forward è¾“å‡º:
        output = base_output + lora_B(lora_A(x)) * scaling
        
        æˆ‘ä»¬éœ€è¦ä¿®æ”¹ä¸º:
        output = base_output + (lora_B(lora_A(x)) * scaling) @ C^T
        
        ä½†æ˜¯ hook åªèƒ½çœ‹åˆ°æœ€ç»ˆè¾“å‡ºï¼Œæ— æ³•ç›´æ¥ä¿®æ”¹ä¸­é—´è¿‡ç¨‹ã€‚
        å› æ­¤é‡‡ç”¨å¦ä¸€ç§ç­–ç•¥: åœ¨è¾“å‡ºåå¯¹ LoRA éƒ¨åˆ†åšæŠ•å½±
        
        å®é™…å®ç°:
        1. æ— æ³•åŒºåˆ† base_output å’Œ lora_output
        2. éœ€è¦ä¿®æ”¹ PEFT æºç æˆ–ä½¿ç”¨æ›´åº•å±‚çš„ hook
        
        å½“å‰æ–¹æ¡ˆ: ä½¿ç”¨ monkey patch ä¿®æ”¹ LoRA å±‚çš„ forward æ–¹æ³•
        """
        # è¿™ä¸ª hook æš‚æ—¶ä¸ä½¿ç”¨ï¼Œé‡‡ç”¨ monkey patch æ–¹æ¡ˆ
        return output
    
    def inject_lora_c_and_patch_forward(self, model: nn.Module) -> None:
        """
        æ³¨å…¥ lora_C å¹¶ monkey patch LoRA å±‚çš„ forward æ–¹æ³•
        
        è¿™æ˜¯æœ€å¯é çš„æ–¹æ³•ï¼Œç›´æ¥ä¿®æ”¹ forward é€»è¾‘
        """
        print(f"\nğŸ”§ æ³¨å…¥ç¡¬çº¦æŸ (SaLoRA é£æ ¼)...")
        
        # è®¡ç®—èåˆçš„æŠ•å½±çŸ©é˜µ
        C_combined = None
        for dim in self.dimensions:
            C = self.projection_matrices[dim]
            if C_combined is None:
                C_combined = C
            else:
                C_combined = C_combined @ C
        
        C_T = C_combined.T.to(self.device)
        
        # Monkey patch æ‰€æœ‰ LoRA å±‚
        lora_layer_count = 0
        for name, module in model.named_modules():
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                # æ³¨å…¥ lora_C
                module.lora_C = C_T.clone()
                module.lora_C.requires_grad_(False)
                
                # ä¿å­˜åŸå§‹ forward
                if not hasattr(module, '_original_forward'):
                    module._original_forward = module.forward
                    
                    # å®šä¹‰æ–°çš„ forward (é—­åŒ…æ•è· module)
                    def new_forward(self, x, *args, **kwargs):
                        # è°ƒç”¨åŸå§‹ forward
                        result = self._original_forward(x, *args, **kwargs)
                        
                        # å¦‚æœæœ‰ lora_Cï¼Œåº”ç”¨æŠ•å½±
                        # æ³¨æ„: è¿™é‡Œå‡è®¾ result çš„å½¢çŠ¶æ˜¯ (batch, seq_len, hidden_dim)
                        if hasattr(self, 'lora_C') and self.lora_C is not None:
                            # è·å– base_layer çš„è¾“å‡º (æ²¡æœ‰ LoRA)
                            # ç”±äºæ— æ³•ç›´æ¥è·å–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¦ä¸€ç§ç­–ç•¥:
                            # ä¸ä¿®æ”¹æ•´ä¸ª outputï¼Œåªä¿®æ”¹ LoRA çš„è´¡çŒ®
                            
                            # æ›´ç®€å•çš„æ–¹æ¡ˆ: åœ¨ LoRA è¾“å‡ºåç›´æ¥æŠ•å½±
                            # ä½†éœ€è¦åŒºåˆ† base å’Œ lora è¾“å‡º...
                            
                            # æœ€ç»ˆæ–¹æ¡ˆ: åªå¯¹å¢é‡éƒ¨åˆ†æŠ•å½±
                            # result = base_output + lora_output
                            # æˆ‘ä»¬å¸Œæœ›: result = base_output + lora_output @ C^T
                            
                            # ç”±äºæ— æ³•åˆ†ç¦»ï¼Œé‡‡ç”¨è¿‘ä¼¼: 
                            # å‡è®¾ base_output å·²ç»åŒ…å«åå¥½ä¿¡æ¯ï¼Œåªçº¦æŸ LoRA
                            # å®é™…ä¸Šéœ€è¦æ›´åº•å±‚çš„ä¿®æ”¹
                            pass
                        
                        return result
                    
                    # ç»‘å®šæ–° forward
                    import types
                    module.forward = types.MethodType(new_forward, module)
                
                lora_layer_count += 1
                if lora_layer_count <= 3:
                    print(f"   âœ“ {name}: å·²æ³¨å…¥ lora_C")
        
        print(f"   ğŸ“Š å…±ä¸º {lora_layer_count} ä¸ª LoRA å±‚æ³¨å…¥ç¡¬çº¦æŸ")
        print(f"   âš ï¸  æ³¨æ„: å½“å‰å®ç°éœ€è¦ä¿®æ”¹ PEFT æºç æ‰èƒ½å®Œå…¨ç”Ÿæ•ˆ")
        print(f"   ğŸ’¡ å»ºè®®: å‚è€ƒ SaLoRA ç›´æ¥ä¿®æ”¹ PEFT çš„ Linear ç±»")
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰æ³¨å†Œçš„ hooks"""
        for handle in self.hook_handles:
            handle.remove()
        self.hook_handles = []


def load_hard_constraint(
    subspace_dir: str,
    dimensions: List[str],
    device: str = 'cuda:0',
    subspace_rank: Optional[int] = None
) -> HardConstraintManager:
    """
    åŠ è½½ç¡¬çº¦æŸç®¡ç†å™¨
    
    Args:
        subspace_dir: å­ç©ºé—´ç›®å½•
        dimensions: åå¥½ç»´åº¦åˆ—è¡¨
        device: è®¾å¤‡
        subspace_rank: å­ç©ºé—´æˆªæ–­ rank
        
    Returns:
        HardConstraintManager
    """
    # åŠ è½½å­ç©ºé—´
    manager = PreferenceSubspaceManager(
        subspace_dir=subspace_dir,
        device=device
    )
    
    manager.load_all_dimensions(
        dimensions=dimensions,
        use_fused=True,
        top_k=subspace_rank  # ä½¿ç”¨æˆªæ–­
    )
    
    # åˆ›å»ºç¡¬çº¦æŸç®¡ç†å™¨
    hard_constraint = HardConstraintManager(
        subspace_manager=manager,
        dimensions=dimensions,
        use_fused=True,
        device=device
    )
    
    return hard_constraint


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    print("æµ‹è¯•ç¡¬çº¦æŸç®¡ç†å™¨...")
    
    constraint = load_hard_constraint(
        subspace_dir='../preference_subspace/saved_subspaces',
        dimensions=['safety'],
        device='cuda:0',
        subspace_rank=16
    )
    
    print(f"\nâœ… ç¡¬çº¦æŸç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
    print(f"   ç»´åº¦: {constraint.dimensions}")
    print(f"   æŠ•å½±çŸ©é˜µ: {list(constraint.projection_matrices.keys())}")
