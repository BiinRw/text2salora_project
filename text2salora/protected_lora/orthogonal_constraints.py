"""
æ­£äº¤çº¦æŸæ ¸å¿ƒå®ç°
ç”¨äºåœ¨ LoRA è®­ç»ƒæ—¶çº¦æŸæƒé‡æ›´æ–°æ–¹å‘ä¸åå¥½å­ç©ºé—´æ­£äº¤
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Union
from pathlib import Path
import sys

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from utils.svd_utils import PreferenceSubspaceManager, compute_projection_matrix


class OrthogonalConstraint:
    """æ­£äº¤çº¦æŸè®¡ç®—å™¨"""
    
    def __init__(
        self,
        subspace_manager: PreferenceSubspaceManager,
        dimensions: List[str],
        layer_ids: Optional[List[int]] = None,
        use_fused: bool = True,
        device: str = 'cuda:0'
    ):
        """
        Args:
            subspace_manager: å­ç©ºé—´ç®¡ç†å™¨
            dimensions: éœ€è¦çº¦æŸçš„åå¥½ç»´åº¦
            layer_ids: å¯¹åº”çš„å±‚ID (None=ä½¿ç”¨èåˆå­ç©ºé—´)
            use_fused: æ˜¯å¦ä½¿ç”¨èåˆå­ç©ºé—´
            device: è®¾å¤‡
        """
        self.manager = subspace_manager
        self.dimensions = dimensions
        self.layer_ids = layer_ids
        self.use_fused = use_fused
        self.device = device
        
        # é¢„è®¡ç®—æŠ•å½±çŸ©é˜µ P = V @ V^T
        self._prepare_projection_matrices()
    
    def _prepare_projection_matrices(self):
        """é¢„è®¡ç®—æ‰€æœ‰åå¥½ç»´åº¦çš„æŠ•å½±çŸ©é˜µ"""
        self.projection_matrices = {}  # {dimension: P or {layer_id: P}}
        
        print(f"\nğŸ“ é¢„è®¡ç®—æŠ•å½±çŸ©é˜µ...")
        
        for dim in self.dimensions:
            if self.use_fused:
                # èåˆå­ç©ºé—´: ä¸€ä¸ªæŠ•å½±çŸ©é˜µ
                V = self.manager.get_subspace(dim, layer_id=None)
                P = compute_projection_matrix(V)
                self.projection_matrices[dim] = P
                print(f"   {dim}: P shape={P.shape}")
            
            else:
                # å¤šå±‚å­ç©ºé—´: æ¯å±‚ä¸€ä¸ªæŠ•å½±çŸ©é˜µ
                self.projection_matrices[dim] = {}
                for layer_id in self.layer_ids:
                    V = self.manager.get_subspace(dim, layer_id=layer_id)
                    P = compute_projection_matrix(V)
                    self.projection_matrices[dim][layer_id] = P
                    print(f"   {dim} Layer {layer_id}: P shape={P.shape}")
    
    def compute_orthogonal_loss(
        self,
        lora_deltas: Dict[str, torch.Tensor],
        lambda_orth: float = 1.0,
        dimension_weights: Optional[Dict[str, float]] = None
    ) -> torch.Tensor:
        """è®¡ç®—æ­£äº¤æŸå¤±
        
        æ­£äº¤æŸå¤±å®šä¹‰: L_orth = ||Î”W @ P||Â²_F
        å…¶ä¸­ Î”W = LoRA weight delta, P = V @ V^T
        
        æœ€å°åŒ–æ­¤æŸå¤± â†’ Î”W æ­£äº¤äºå­ç©ºé—´ V
        
        Args:
            lora_deltas: LoRA æƒé‡æ›´æ–° {layer_name: Î”W}
            lambda_orth: æ­£äº¤æŸå¤±ç³»æ•°
            dimension_weights: å„åå¥½ç»´åº¦æƒé‡ {dimension: weight}
            
        Returns:
            loss: æ€»æ­£äº¤æŸå¤±
        """
        if dimension_weights is None:
            dimension_weights = {dim: 1.0 for dim in self.dimensions}
        
        total_loss = 0.0
        loss_details = {}
        
        for dim in self.dimensions:
            dim_weight = dimension_weights.get(dim, 1.0)
            dim_loss = 0.0
            
            for layer_name, delta_W in lora_deltas.items():
                # delta_W: (out_dim, in_dim) æˆ– (out_dim, rank) @ (rank, in_dim)
                
                # è·å–å¯¹åº”çš„æŠ•å½±çŸ©é˜µ
                if self.use_fused:
                    P = self.projection_matrices[dim]
                else:
                    # ä» layer_name æå– layer_id
                    layer_id = self._extract_layer_id(layer_name)
                    P = self.projection_matrices[dim].get(layer_id)
                    if P is None:
                        continue  # è¯¥å±‚æœªçº¦æŸ
                
                # è®¡ç®— ||Î”W @ P||Â²_F
                # ä¸ºäº†é¿å…å¤§çŸ©é˜µä¹˜æ³•,æ”¹å†™ä¸º trace((Î”W @ P) @ (Î”W @ P)^T)
                delta_P = delta_W @ P  # (out_dim, in_dim) @ (in_dim, in_dim) = (out_dim, in_dim)
                loss_term = torch.sum(delta_P ** 2)
                
                dim_loss += loss_term
            
            # åŠ æƒ
            dim_loss = dim_weight * dim_loss
            loss_details[dim] = dim_loss.item()
            total_loss += dim_loss
        
        # åº”ç”¨ç³»æ•°
        total_loss = lambda_orth * total_loss
        
        return total_loss, loss_details
    
    def compute_orthogonal_loss_efficient(
        self,
        lora_A: Dict[str, torch.Tensor],
        lora_B: Dict[str, torch.Tensor],
        lambda_orth: float = 1.0,
        dimension_weights: Optional[Dict[str, float]] = None
    ) -> torch.Tensor:
        """é«˜æ•ˆè®¡ç®—æ­£äº¤æŸå¤± (é’ˆå¯¹ LoRA ä½ç§©åˆ†è§£)
        
        LoRA: Î”W = B @ A
        åˆ™ Î”W @ P = B @ A @ P
        ||Î”W @ P||Â²_F = ||B @ (A @ P)||Â²_F
        
        Args:
            lora_A: LoRA AçŸ©é˜µ {layer_name: A (rank, in_dim)}
            lora_B: LoRA BçŸ©é˜µ {layer_name: B (out_dim, rank)}
            lambda_orth: æ­£äº¤æŸå¤±ç³»æ•°
            dimension_weights: å„åå¥½ç»´åº¦æƒé‡
            
        Returns:
            loss: æ€»æ­£äº¤æŸå¤±
            loss_details: å„ç»´åº¦æŸå¤±è¯¦æƒ…
        """
        if dimension_weights is None:
            dimension_weights = {dim: 1.0 for dim in self.dimensions}
        
        total_loss = 0.0
        loss_details = {}
        
        for dim in self.dimensions:
            dim_weight = dimension_weights.get(dim, 1.0)
            dim_loss = 0.0
            
            for layer_name in lora_A.keys():
                A = lora_A[layer_name]  # (rank, in_dim)
                B = lora_B[layer_name]  # (out_dim, rank)
                
                # è·å–æŠ•å½±çŸ©é˜µ
                if self.use_fused:
                    P = self.projection_matrices[dim]
                else:
                    layer_id = self._extract_layer_id(layer_name)
                    P = self.projection_matrices[dim].get(layer_id)
                    if P is None:
                        continue
                
                # è®¡ç®— B @ (A @ P)
                AP = A @ P  # (rank, in_dim) @ (in_dim, in_dim) = (rank, in_dim)
                BAP = B @ AP  # (out_dim, rank) @ (rank, in_dim) = (out_dim, in_dim)
                
                # ||BAP||Â²_F
                loss_term = torch.sum(BAP ** 2)
                dim_loss += loss_term
            
            dim_loss = dim_weight * dim_loss
            loss_details[dim] = dim_loss.item()
            total_loss += dim_loss
        
        total_loss = lambda_orth * total_loss
        
        return total_loss, loss_details
    
    def _extract_layer_id(self, layer_name: str) -> int:
        """ä»å±‚åç§°æå–å±‚ID
        
        ä¾‹å¦‚: 'model.layers.15.self_attn.q_proj' -> 15
        """
        parts = layer_name.split('.')
        for i, p in enumerate(parts):
            if p == 'layers' and i + 1 < len(parts):
                return int(parts[i + 1])
        raise ValueError(f"æ— æ³•ä» {layer_name} æå–å±‚ID")


def collect_lora_deltas(model: nn.Module) -> Dict[str, torch.Tensor]:
    """ä»æ¨¡å‹ä¸­æ”¶é›† LoRA æƒé‡æ›´æ–° Î”W = B @ A
    
    Args:
        model: åŒ…å« LoRA å±‚çš„æ¨¡å‹
        
    Returns:
        {layer_name: Î”W}
    """
    lora_deltas = {}
    
    for name, module in model.named_modules():
        if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
            # PEFT LoRA ç»“æ„
            A = module.lora_A['default'].weight  # (rank, in_dim)
            B = module.lora_B['default'].weight  # (out_dim, rank)
            
            # è®¡ç®— Î”W = B @ A
            delta_W = B @ A
            lora_deltas[name] = delta_W
    
    return lora_deltas


def collect_lora_AB_matrices(model: nn.Module) -> tuple:
    """ä»æ¨¡å‹ä¸­æ”¶é›† LoRA A å’Œ B çŸ©é˜µ
    
    Args:
        model: åŒ…å« LoRA å±‚çš„æ¨¡å‹
        
    Returns:
        lora_A: {layer_name: A}
        lora_B: {layer_name: B}
    """
    lora_A = {}
    lora_B = {}
    
    for name, module in model.named_modules():
        if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
            A = module.lora_A['default'].weight  # (rank, in_dim)
            B = module.lora_B['default'].weight  # (out_dim, rank)
            
            lora_A[name] = A
            lora_B[name] = B
    
    return lora_A, lora_B


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    from utils.svd_utils import PreferenceSubspaceManager
    
    # åŠ è½½å­ç©ºé—´
    manager = PreferenceSubspaceManager(
        subspace_dir='../preference_subspace/saved_subspaces',
        device='cuda:0'
    )
    
    manager.load_all_dimensions(
        dimensions=['safety', 'helpfulness'],
        use_fused=True
    )
    
    # åˆ›å»ºçº¦æŸè®¡ç®—å™¨
    constraint = OrthogonalConstraint(
        subspace_manager=manager,
        dimensions=['safety', 'helpfulness'],
        use_fused=True,
        device='cuda:0'
    )
    
    # æ¨¡æ‹Ÿ LoRA æƒé‡
    lora_A = {
        'model.layers.15.self_attn.q_proj': torch.randn(8, 1024, device='cuda:0')
    }
    lora_B = {
        'model.layers.15.self_attn.q_proj': torch.randn(1024, 8, device='cuda:0')
    }
    
    # è®¡ç®—æŸå¤±
    loss, details = constraint.compute_orthogonal_loss_efficient(
        lora_A, lora_B, lambda_orth=0.1
    )
    
    print(f"\næ€»æŸå¤±: {loss.item():.6f}")
    print(f"è¯¦æƒ…: {details}")
