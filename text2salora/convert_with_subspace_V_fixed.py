#!/usr/bin/env python3
"""
å°†åŸºäº V (å­ç©ºé—´å‘é‡) çš„è®­ç»ƒç»“æœè½¬æ¢ä¸º SaLoRA æ ¼å¼çš„ ABC.pt æ–‡ä»¶

æ­¥éª¤:
1. åŠ è½½è®­ç»ƒå¾—åˆ°çš„å­ç©ºé—´ V
2. è®¡ç®—çº¦æŸçŸ©é˜µ C = I - V @ V^T
3. åŠ è½½ LoRA adapter (A, B çŸ©é˜µ)
4. ç»„åˆæˆ ABC.pt æ–‡ä»¶
"""

import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel
import argparse
import os
from pathlib import Path

def load_and_build_constraint_matrix(subspace_dir, dimension, layer_id, device='cpu'):
    """
    åŠ è½½å­ç©ºé—´ V å¹¶æ„å»ºçº¦æŸçŸ©é˜µ C = I - V @ V^T
    """
    # å…ˆå°è¯•åŠ è½½ fused æ–‡ä»¶
    fused_path = Path(subspace_dir) / f"{dimension}_layer{layer_id}_fused_subspace.pkl"
    layer_path = Path(subspace_dir) / f"{dimension}_layer{layer_id}_subspace.pkl"
    
    if fused_path.exists():
        data = torch.load(fused_path, map_location=device)
    elif layer_path.exists():
        data = torch.load(layer_path, map_location=device)
    else:
        return None, None
    
    V = data['V']  # shape: [feature_dim, subspace_dim]
    
    # æ„å»ºçº¦æŸçŸ©é˜µ C = I - V @ V^T
    feature_dim = V.shape[0]
    
    # ç¡®ä¿ V åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    V = V.to(device)
    
    # åˆ›å»ºå•ä½çŸ©é˜µ
    I = torch.eye(feature_dim, dtype=V.dtype, device=V.device)
    
    # è®¡ç®—çº¦æŸçŸ©é˜µ
    C = I - torch.matmul(V, V.transpose(0, 1))
    
    return C, V

def load_lora_adapter(base_model_name, adapter_path, device='cpu'):
    """åŠ è½½ LoRA adapter"""
    print(f"\nğŸ”§ åŠ è½½ base model: {base_model_name}")
    print(f"   è®¾å¤‡: {device}")
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map=device if device != 'cpu' else 'cpu',
        trust_remote_code=True
    )
    
    print(f"ğŸ“‚ åŠ è½½ LoRA adapter: {adapter_path}")
    model = PeftModel.from_pretrained(base_model, adapter_path)
    
    return model, base_model

def extract_lora_weights(model, base_model):
    """æå– LoRA A, B çŸ©é˜µå’Œ base model æƒé‡"""
    weight_dict = {}
    
    # è·å– LoRA æƒé‡
    lora_state_dict = model.state_dict()
    base_state_dict = base_model.state_dict()
    
    for name, param in lora_state_dict.items():
        # æå– LoRA A å’Œ B çŸ©é˜µ
        if 'lora_A' in name or 'lora_B' in name:
            # æ¸…ç†åç§°
            clean_name = name.replace('base_model.model.', '')
            clean_name = clean_name.replace('.default', '')
            weight_dict[clean_name] = param.cpu()
    
    # æå– base model æƒé‡
    for name, param in base_state_dict.items():
        if any(proj in name for proj in ['q_proj', 'k_proj', 'v_proj', 'o_proj']):
            if 'weight' in name:
                weight_dict[name] = param.cpu()
    
    return weight_dict

def add_constraint_from_subspace(weight_dict, subspace_dir, dimension, num_layers, device='cpu'):
    """
    ä¸ºæ¯ä¸€å±‚æ·»åŠ çº¦æŸçŸ©é˜µ C å’Œå­ç©ºé—´ V
    """
    print(f"\nğŸ“Š å¤„ç† {num_layers} å±‚çš„çº¦æŸçŸ©é˜µ...")
    
    for layer_id in range(num_layers):
        print(f"  å±‚ {layer_id}...", end=' ')
        
        C, V = load_and_build_constraint_matrix(subspace_dir, dimension, layer_id, device=device)
        
        if C is not None:
            # ä¸º q_proj, k_proj, v_proj æ·»åŠ çº¦æŸ
            for proj_name in ['q_proj', 'k_proj', 'v_proj']:
                key_c = f'model.layers.{layer_id}.self_attn.{proj_name}.lora_C'
                key_v = f'model.layers.{layer_id}.self_attn.{proj_name}.V'
                
                weight_dict[key_c] = C.cpu()
                weight_dict[key_v] = V.cpu()
            
            print("âœ“")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å­ç©ºé—´æ–‡ä»¶")
    
    return weight_dict

def compute_merged_weights(weight_dict, num_layers, lora_alpha=16, lora_rank=16):
    """
    è®¡ç®—åˆå¹¶æƒé‡: merged_weight = base_weight + (lora_B @ lora_A) * scaling
    """
    print(f"\nğŸ”— è®¡ç®—åˆå¹¶æƒé‡...")
    scaling = lora_alpha / lora_rank
    
    for layer_id in range(num_layers):
        for proj_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:
            base_key = f'model.layers.{layer_id}.self_attn.{proj_name}.weight'
            lora_a_key = f'model.layers.{layer_id}.self_attn.{proj_name}.lora_A.weight'
            lora_b_key = f'model.layers.{layer_id}.self_attn.{proj_name}.lora_B.weight'
            merged_key = f'model.layers.{layer_id}.self_attn.{proj_name}.merged_weight'
            
            if base_key in weight_dict and lora_a_key in weight_dict and lora_b_key in weight_dict:
                base_weight = weight_dict[base_key]
                lora_a = weight_dict[lora_a_key]
                lora_b = weight_dict[lora_b_key]
                
                # è®¡ç®— LoRA å¢é‡
                lora_delta = torch.matmul(lora_b, lora_a) * scaling
                
                # åˆå¹¶æƒé‡
                merged_weight = base_weight + lora_delta
                weight_dict[merged_key] = merged_weight
                
                print(f"  å±‚ {layer_id} {proj_name}: âœ“")
    
    return weight_dict

def save_abc_file(weight_dict, output_path):
    """ä¿å­˜ä¸º ABC.pt æ–‡ä»¶"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    print(f"   åŒ…å« {len(weight_dict)} ä¸ªæƒé‡")
    
    torch.save(weight_dict, output_path)
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
    file_size = output_path.stat().st_size / (1024 ** 2)
    print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

def count_keys_by_type(weight_dict):
    """ç»Ÿè®¡ä¸åŒç±»å‹çš„é”®"""
    lora_a_count = sum(1 for k in weight_dict.keys() if 'lora_A' in k)
    lora_b_count = sum(1 for k in weight_dict.keys() if 'lora_B' in k)
    lora_c_count = sum(1 for k in weight_dict.keys() if 'lora_C' in k)
    v_count = sum(1 for k in weight_dict.keys() if '.V' in k)
    merged_count = sum(1 for k in weight_dict.keys() if 'merged_weight' in k)
    base_count = sum(1 for k in weight_dict.keys() if 'weight' in k and 'lora' not in k and 'merged' not in k)
    
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"   LoRA A çŸ©é˜µ: {lora_a_count}")
    print(f"   LoRA B çŸ©é˜µ: {lora_b_count}")
    print(f"   çº¦æŸçŸ©é˜µ C: {lora_c_count}")
    print(f"   å­ç©ºé—´ V: {v_count}")
    print(f"   åˆå¹¶æƒé‡: {merged_count}")
    print(f"   åŸºç¡€æƒé‡: {base_count}")
    print(f"   æ€»è®¡: {len(weight_dict)}")

def main():
    parser = argparse.ArgumentParser(description='è½¬æ¢ V-based è®­ç»ƒç»“æœä¸º ABC.pt æ ¼å¼')
    parser.add_argument('--lora_adapter_path', type=str, required=True,
                        help='LoRA adapter è·¯å¾„')
    parser.add_argument('--subspace_dir', type=str, required=True,
                        help='å­ç©ºé—´æ–‡ä»¶ç›®å½•')
    parser.add_argument('--dimension', type=str, default='safety',
                        help='åå¥½ç»´åº¦ (safety, helpfulness, etc.)')
    parser.add_argument('--base_model_name', type=str, required=True,
                        help='Base model åç§°')
    parser.add_argument('--output_path', type=str, required=True,
                        help='è¾“å‡º ABC.pt æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_layers', type=int, default=28,
                        help='æ¨¡å‹å±‚æ•°')
    parser.add_argument('--device', type=str, default='cpu',
                        help='è®¾å¤‡ (cpu, cuda:0, cuda:1, ...)')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸš€ V-based â†’ ABC.pt è½¬æ¢å·¥å…·")
    print("=" * 70)
    print(f"LoRA Adapter: {args.lora_adapter_path}")
    print(f"å­ç©ºé—´ç›®å½•: {args.subspace_dir}")
    print(f"åå¥½ç»´åº¦: {args.dimension}")
    print(f"Base Model: {args.base_model_name}")
    print(f"è¾“å‡ºè·¯å¾„: {args.output_path}")
    print(f"å±‚æ•°: {args.num_layers}")
    print(f"è®¾å¤‡: {args.device}")
    print("=" * 70)
    
    # 1. åŠ è½½ LoRA adapter
    model, base_model = load_lora_adapter(args.base_model_name, args.lora_adapter_path, args.device)
    
    # 2. æå–æƒé‡
    print("\nğŸ“¦ æå– LoRA æƒé‡...")
    weight_dict = extract_lora_weights(model, base_model)
    
    # 3. æ·»åŠ çº¦æŸçŸ©é˜µ
    weight_dict = add_constraint_from_subspace(
        weight_dict, 
        args.subspace_dir, 
        args.dimension, 
        args.num_layers,
        args.device
    )
    
    # 4. è®¡ç®—åˆå¹¶æƒé‡
    weight_dict = compute_merged_weights(weight_dict, args.num_layers)
    
    # 5. ç»Ÿè®¡
    count_keys_by_type(weight_dict)
    
    # 6. ä¿å­˜
    save_abc_file(weight_dict, args.output_path)
    
    # æ¸…ç†å†…å­˜
    del model
    del base_model
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    print("\nâœ… è½¬æ¢å®Œæˆ!")

if __name__ == "__main__":
    main()
