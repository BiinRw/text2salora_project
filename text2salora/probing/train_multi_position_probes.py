"""
å¤šä½ç½®æ¢é’ˆè®­ç»ƒè„šæœ¬
æ”¯æŒåŒæ—¶è®­ç»ƒå¤šä¸ª token ä½ç½®çš„æ¢é’ˆï¼Œç”¨äºå¯¹æ¯”ä¸åŒä½ç½®çš„æ•ˆæœ

Token ä½ç½®ç­–ç•¥:
1. user_last: ç”¨æˆ·é—®é¢˜çš„æœ€åä¸€ä¸ª token (è¾“å…¥ç†è§£é˜¶æ®µ)
2. assistant_first: åŠ©æ‰‹å›ç­”çš„ç¬¬ä¸€ä¸ª token (å†³ç­–èµ·ç‚¹)
3. assistant_last: åŠ©æ‰‹å›ç­”çš„æœ€åä¸€ä¸ª token (æ•´ä½“è¡¨å¾, æ ‡å‡†åšæ³•)
4. assistant_mean: åŠ©æ‰‹å›ç­”çš„æ‰€æœ‰ token å¹³å‡ (ç”Ÿæˆè¿‡ç¨‹)
"""

import torch
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import cross_val_score
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import json
import argparse
import os
import pickle
from datetime import datetime
from typing import Dict, List, Tuple


class MultiPositionActivationExtractor:
    """æå–å¤šä¸ª token ä½ç½®çš„æ¿€æ´»å€¼"""
    
    def __init__(self, model, tokenizer, device='cuda:0'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.num_layers = model.config.num_hidden_layers
        self.num_heads = model.config.num_attention_heads
        self.hidden_size = model.config.hidden_size
        self.head_dim = self.hidden_size // self.num_heads
        
        # å­˜å‚¨å®Œæ•´çš„æ¿€æ´»å€¼åºåˆ— (ä¸åªæ˜¯æœ€åä¸€ä¸ª token)
        self.activations = {}
        self.hooks = []
        
        print(f"ğŸ“Š æ¨¡å‹é…ç½®:")
        print(f"   å±‚æ•°: {self.num_layers}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {self.num_heads}")
        print(f"   éšè—å±‚ç»´åº¦: {self.hidden_size}")
        print(f"   æ¯ä¸ªå¤´ç»´åº¦: {self.head_dim}")
    
    def _get_activation_hook(self, layer_id):
        """åˆ›å»º hook å‡½æ•°æ¥æ•è·å®Œæ•´åºåˆ—çš„æ¿€æ´»å€¼"""
        def hook(module, input, output):
            key = f"layer-{layer_id}"
            if key not in self.activations:
                self.activations[key] = []
            # ä¿å­˜å®Œæ•´åºåˆ— (batch, seq_len, hidden_dim)
            self.activations[key].append(output.detach().cpu())
        return hook
    
    def register_hooks(self):
        """æ³¨å†Œ hooks åˆ°æ‰€æœ‰å±‚çš„ Q æŠ•å½±"""
        print("ğŸ”§ æ³¨å†Œæ¿€æ´»å€¼æå– hooks...")
        for layer_id in range(self.num_layers):
            layer = self.model.model.layers[layer_id]
            hook = layer.self_attn.q_proj.register_forward_hook(
                self._get_activation_hook(layer_id)
            )
            self.hooks.append(hook)
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰ hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
    
    def find_token_positions(self, text: str, inputs) -> Dict[str, int]:
        """
        å®šä½å…³é”® token ä½ç½®
        
        è¿”å›æ ¼å¼: {
            'user_last': int,      # ç”¨æˆ·æœ€åä¸€ä¸ªtokenä½ç½®
            'assistant_first': int, # åŠ©æ‰‹ç¬¬ä¸€ä¸ªtokenä½ç½®
            'assistant_last': int,  # åŠ©æ‰‹æœ€åä¸€ä¸ªtokenä½ç½®
            'assistant_range': (start, end)  # åŠ©æ‰‹tokenèŒƒå›´
        }
        """
        # è·å–å®Œæ•´ token æ–‡æœ¬
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        
        # æŸ¥æ‰¾å…³é”®ä½ç½®
        positions = {}
        
        # å¯»æ‰¾ <|im_start|>assistant çš„ä½ç½®
        assistant_markers = ['assistant', '<|assistant|>', 'Assistant']
        assistant_start = -1
        
        for i, token in enumerate(tokens):
            for marker in assistant_markers:
                if marker.lower() in token.lower():
                    assistant_start = i
                    break
            if assistant_start != -1:
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°åŠ©æ‰‹æ ‡è®°ï¼Œä½¿ç”¨ç®€å•çš„åˆ†å‰²ç­–ç•¥
        if assistant_start == -1:
            # å‡è®¾å‰åŠéƒ¨åˆ†æ˜¯ç”¨æˆ·ï¼ŒååŠéƒ¨åˆ†æ˜¯åŠ©æ‰‹
            seq_len = len(tokens)
            assistant_start = seq_len // 2
        
        # è®¡ç®—å„ä¸ªä½ç½®
        positions['user_last'] = max(0, assistant_start - 1)
        positions['assistant_first'] = min(assistant_start + 1, len(tokens) - 1)
        positions['assistant_last'] = len(tokens) - 1
        positions['assistant_range'] = (assistant_start + 1, len(tokens))
        
        return positions
    
    def format_conversation(self, prompt, response):
        """æ ¼å¼åŒ–å¯¹è¯ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
        messages = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response}
        ]
        
        try:
            text = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=False
            )
        except:
            text = f"User: {prompt}\nAssistant: {response}"
        
        return text
    
    def extract_from_pairs(self, pairs, max_samples=None, 
                          positions=['user_last', 'assistant_first', 'assistant_last', 'assistant_mean']):
        """
        ä»é…å¯¹æ•°æ®ä¸­æå–å¤šä¸ªä½ç½®çš„æ¿€æ´»å€¼
        
        Args:
            pairs: é…å¯¹æ•°æ®åˆ—è¡¨
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            positions: è¦æå–çš„ä½ç½®åˆ—è¡¨
        
        Returns:
            Dict[position_name, Dict[head_key, activations]]
        """
        self.activations = {}
        self.register_hooks()
        
        print(f"ğŸ“¥ æå– {len(pairs)} ä¸ªé…å¯¹çš„æ¿€æ´»å€¼...")
        print(f"ğŸ“ æå–ä½ç½®: {', '.join(positions)}")
        self.model.eval()
        
        if max_samples:
            pairs = pairs[:max_samples]
        
        # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ä½ç½®ä¿¡æ¯
        position_indices = []
        
        with torch.no_grad():
            for pair in tqdm(pairs, desc="æå–æ¿€æ´»"):
                text = self.format_conversation(pair['prompt'], pair['response'])
                inputs = self.tokenizer(
                    text,
                    return_tensors='pt',
                    truncation=True,
                    max_length=512
                ).to(self.device)
                
                # è®°å½•ä½ç½®
                pos_info = self.find_token_positions(text, inputs)
                position_indices.append(pos_info)
                
                self.model(**inputs)
        
        self.remove_hooks()
        
        # æ•´ç†æ¿€æ´»å€¼: æŒ‰ä½ç½®å’Œæ³¨æ„åŠ›å¤´ç»„ç»‡
        print("ğŸ”„ æ•´ç†æ¿€æ´»å€¼...")
        result = {pos: {} for pos in positions}
        
        for layer_id in tqdm(range(self.num_layers), desc="å¤„ç†å±‚"):
            key = f"layer-{layer_id}"
            if key not in self.activations:
                continue
            
            # layer_acts: List[Tensor(1, seq_len, hidden_dim)]
            layer_acts_list = self.activations[key]
            
            for head_id in range(self.num_heads):
                start_idx = head_id * self.head_dim
                end_idx = (head_id + 1) * self.head_dim
                head_key = f"layer-{layer_id}-head-{head_id}"
                
                # ä¸ºæ¯ä¸ªä½ç½®æå–æ¿€æ´»
                for pos_name in positions:
                    acts_for_position = []
                    
                    for sample_idx, (act_tensor, pos_info) in enumerate(zip(layer_acts_list, position_indices)):
                        # act_tensor: (1, seq_len, hidden_dim)
                        act = act_tensor[0, :, start_idx:end_idx].numpy()  # (seq_len, head_dim)
                        
                        if pos_name == 'user_last':
                            token_idx = pos_info['user_last']
                            acts_for_position.append(act[token_idx])
                        
                        elif pos_name == 'assistant_first':
                            token_idx = pos_info['assistant_first']
                            acts_for_position.append(act[token_idx])
                        
                        elif pos_name == 'assistant_last':
                            token_idx = pos_info['assistant_last']
                            acts_for_position.append(act[token_idx])
                        
                        elif pos_name == 'assistant_mean':
                            start, end = pos_info['assistant_range']
                            mean_act = act[start:end].mean(axis=0)
                            acts_for_position.append(mean_act)
                    
                    result[pos_name][head_key] = np.array(acts_for_position)
        
        print(f"âœ… æå–å®Œæˆ! æ¯ä¸ªä½ç½®å…± {len(result[positions[0]])} ä¸ªæ³¨æ„åŠ›å¤´")
        return result


def train_linear_probes_with_cv(acts_good, acts_bad, test_split=0.2, cv_folds=5,
                                  max_iter=2000, C=1.0):
    """ä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´è®­ç»ƒçº¿æ€§æ¢é’ˆ"""
    results = {}
    probes = {}
    detailed_results = {}
    
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒçº¿æ€§æ¢é’ˆ")
    print(f"   æµ‹è¯•é›†æ¯”ä¾‹: {test_split}")
    print(f"   äº¤å‰éªŒè¯æŠ˜æ•°: {cv_folds}")
    print(f"   æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iter}")
    print(f"   æ­£åˆ™åŒ–å‚æ•°C: {C}")
    print()
    
    for key in tqdm(acts_good.keys(), desc="è®­ç»ƒæ¢é’ˆ"):
        X_good = acts_good[key]  # label=1
        X_bad = acts_bad[key]    # label=0
        
        X = np.vstack([X_good, X_bad])
        y = np.hstack([
            np.ones(len(X_good)),
            np.zeros(len(X_bad))
        ])
        
        # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
        n_train = int((1 - test_split) * len(X))
        indices = np.random.permutation(len(X))
        train_idx, test_idx = indices[:n_train], indices[n_train:]
        
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨
        clf = LogisticRegression(
            max_iter=max_iter,
            random_state=42,
            solver='lbfgs',
            C=C
        )
        clf.fit(X_train, y_train)
        
        # æµ‹è¯•é›†è¯„ä¼°
        y_pred = clf.predict(X_test)
        test_accuracy = accuracy_score(y_test, y_pred)
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(clf, X_train, y_train, cv=cv_folds)
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        
        results[key] = {
            'test_accuracy': float(test_accuracy),
            'cv_mean': float(cv_mean),
            'cv_std': float(cv_std),
            'train_size': len(X_train),
            'test_size': len(X_test)
        }
        
        probes[key] = clf
        
        if test_accuracy >= 0.8:
            detailed_results[key] = {
                'accuracy': float(test_accuracy),
                'cv_mean': float(cv_mean),
                'coefficients': clf.coef_.tolist(),
                'intercept': float(clf.intercept_[0])
            }
    
    return results, probes, detailed_results


def load_paired_data(file_path):
    """åŠ è½½é…å¯¹æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def print_summary(all_results: Dict[str, Dict]):
    """æ‰“å°å„ä½ç½®çš„ç»Ÿè®¡æ‘˜è¦"""
    print("\n" + "="*80)
    print("ğŸ“Š å¤šä½ç½®æ¢é’ˆè®­ç»ƒç»“æœæ±‡æ€»")
    print("="*80)
    
    for pos_name, results in all_results.items():
        test_accs = [r['test_accuracy'] for r in results.values()]
        cv_means = [r['cv_mean'] for r in results.values()]
        
        print(f"\nğŸ“ ä½ç½®: {pos_name}")
        print(f"   æ€»æ³¨æ„åŠ›å¤´æ•°: {len(results)}")
        print(f"   å¹³å‡æµ‹è¯•å‡†ç¡®ç‡: {np.mean(test_accs):.4f} Â± {np.std(test_accs):.4f}")
        print(f"   æœ€é«˜æµ‹è¯•å‡†ç¡®ç‡: {np.max(test_accs):.4f}")
        print(f"   å¹³å‡CVå‡†ç¡®ç‡: {np.mean(cv_means):.4f}")
        print(f"   å‡†ç¡®ç‡ >= 0.8: {sum(1 for a in test_accs if a >= 0.8)} ä¸ª")
        print(f"   å‡†ç¡®ç‡ >= 0.9: {sum(1 for a in test_accs if a >= 0.9)} ä¸ª")
        
        # Top 3
        top_3 = sorted(results.items(), key=lambda x: x[1]['test_accuracy'], reverse=True)[:3]
        print(f"   ğŸ† Top 3:")
        for i, (head, metrics) in enumerate(top_3, 1):
            print(f"      {i}. {head}: {metrics['test_accuracy']:.4f}")


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒå¤šä½ç½®æ¢é’ˆ')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_name', type=str, required=True, help='æ¨¡å‹åç§°æˆ–è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--good_pairs', type=str, required=True, help='å¥½æ ·æœ¬é…å¯¹æ•°æ®è·¯å¾„')
    parser.add_argument('--bad_pairs', type=str, required=True, help='åæ ·æœ¬é…å¯¹æ•°æ®è·¯å¾„')
    parser.add_argument('--max_samples', type=int, default=None, help='æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°')
    
    # ä½ç½®å‚æ•°
    parser.add_argument('--positions', type=str, nargs='+', 
                       default=['user_last', 'assistant_first', 'assistant_last', 'assistant_mean'],
                       choices=['user_last', 'assistant_first', 'assistant_last', 'assistant_mean'],
                       help='è¦æå–çš„tokenä½ç½®')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--test_split', type=float, default=0.2, help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--cv_folds', type=int, default=5, help='äº¤å‰éªŒè¯æŠ˜æ•°')
    parser.add_argument('--max_iter', type=int, default=2000, help='é€»è¾‘å›å½’æœ€å¤§è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--reg_C', type=float, default=1.0, help='æ­£åˆ™åŒ–å‚æ•°C')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--dimension', type=str, required=True, help='ç»´åº¦åç§° (ç”¨äºå‘½å)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("="*80)
    print("ğŸš€ å¤šä½ç½®æ¢é’ˆè®­ç»ƒ")
    print("="*80)
    print(f"æ¨¡å‹: {args.model_name}")
    print(f"ç»´åº¦: {args.dimension}")
    print(f"ä½ç½®: {', '.join(args.positions)}")
    print(f"è®¾å¤‡: {args.device}")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.float16,
        device_map=args.device
    )
    model.eval()
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    good_pairs = load_paired_data(args.good_pairs)
    bad_pairs = load_paired_data(args.bad_pairs)
    print(f"   å¥½æ ·æœ¬: {len(good_pairs)} å¯¹")
    print(f"   åæ ·æœ¬: {len(bad_pairs)} å¯¹")
    
    # æå–æ¿€æ´»å€¼
    extractor = MultiPositionActivationExtractor(model, tokenizer, args.device)
    
    print("\nğŸ” æå–å¥½æ ·æœ¬æ¿€æ´»å€¼...")
    acts_good_multi = extractor.extract_from_pairs(good_pairs, args.max_samples, args.positions)
    
    print("\nğŸ” æå–åæ ·æœ¬æ¿€æ´»å€¼...")
    acts_bad_multi = extractor.extract_from_pairs(bad_pairs, args.max_samples, args.positions)
    
    # ä¸ºæ¯ä¸ªä½ç½®è®­ç»ƒæ¢é’ˆ
    all_results = {}
    all_probes = {}
    
    for pos_name in args.positions:
        print(f"\n{'='*80}")
        print(f"ğŸ“ è®­ç»ƒä½ç½®: {pos_name}")
        print(f"{'='*80}")
        
        acts_good = acts_good_multi[pos_name]
        acts_bad = acts_bad_multi[pos_name]
        
        results, probes, detailed = train_linear_probes_with_cv(
            acts_good, acts_bad,
            test_split=args.test_split,
            cv_folds=args.cv_folds,
            max_iter=args.max_iter,
            C=args.reg_C
        )
        
        all_results[pos_name] = results
        all_probes[pos_name] = probes
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(args.output_dir, f"{args.dimension}_{pos_name}_results.json")
        with open(result_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
        
        # ä¿å­˜æ¢é’ˆ
        probe_dir = os.path.join(args.output_dir, f"{args.dimension}_{pos_name}_probes")
        os.makedirs(probe_dir, exist_ok=True)
        for head_key, probe in probes.items():
            probe_file = os.path.join(probe_dir, f"{head_key}.pkl")
            with open(probe_file, 'wb') as f:
                pickle.dump(probe, f)
        print(f"ğŸ’¾ æ¢é’ˆå·²ä¿å­˜: {probe_dir}/")
    
    # æ‰“å°æ±‡æ€»
    print_summary(all_results)
    
    # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
    report_file = os.path.join(args.output_dir, f"{args.dimension}_position_comparison.txt")
    with open(report_file, 'w') as f:
        f.write("="*80 + "\n")
        f.write(f"ğŸ“Š {args.dimension} ç»´åº¦ - å¤šä½ç½®æ¢é’ˆå¯¹æ¯”æŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        
        for pos_name, results in all_results.items():
            test_accs = [r['test_accuracy'] for r in results.values()]
            f.write(f"\nğŸ“ {pos_name}\n")
            f.write(f"   å¹³å‡å‡†ç¡®ç‡: {np.mean(test_accs):.4f}\n")
            f.write(f"   æœ€é«˜å‡†ç¡®ç‡: {np.max(test_accs):.4f}\n")
            f.write(f"   å‡†ç¡®ç‡>=0.8: {sum(1 for a in test_accs if a >= 0.8)}\n")
            f.write(f"   å‡†ç¡®ç‡>=0.9: {sum(1 for a in test_accs if a >= 0.9)}\n")
    
    print(f"\nğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print("\nâœ… è®­ç»ƒå®Œæˆ!")


if __name__ == '__main__':
    main()
