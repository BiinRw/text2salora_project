"""
æ•´åˆ HelpSteer å’Œ HelpSteer2 æ•°æ®é›†

ç­–ç•¥:
1. ä½¿ç”¨ä¸ ultra-extreme ç›¸åŒçš„é€»è¾‘: åªé€‰æ‹© 0 vs 4 åˆ†çš„æç«¯å¯¹æ¯”
2. åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†çš„æç«¯æ ·æœ¬
3. ä¸ºæ¯ä¸ªç»´åº¦ç”Ÿæˆæ•´åˆåçš„ good/bad pairs
"""

import json
from datasets import load_dataset
from pathlib import Path
from collections import Counter

def build_ultra_extreme_pairs_from_dataset(dataset, dataset_name):
    """
    ä»æ•°æ®é›†ä¸­æ„å»º ultra-extreme pairs (åªé€‰æ‹© 0 vs 4 åˆ†)
    
    Returns:
        dict: {dimension: {'good': [...], 'bad': [...]}}
    """
    dimensions = ['helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity']
    results = {dim: {'good': [], 'bad': []} for dim in dimensions}
    
    for dimension in dimensions:
        # æ”¶é›† 0 åˆ†å’Œ 4 åˆ†çš„æ ·æœ¬
        score_0_samples = []
        score_4_samples = []
        
        for item in dataset:
            score = item.get(dimension)
            if score is None:
                continue
            
            if score == 0.0 or score == 0:
                score_0_samples.append(item)
            elif score == 4.0 or score == 4:
                score_4_samples.append(item)
        
        # é…å¯¹: ä¸ºæ¯ä¸ª prompt åˆ›å»º good (4åˆ†) å’Œ bad (0åˆ†) å¯¹
        # å¦‚æœæ˜¯åŒä¸€ä¸ª prompt æœ‰ä¸åŒå“åº”,ä¼˜å…ˆé…å¯¹
        # å¦åˆ™åˆ†åˆ«ä½œä¸ºç‹¬ç«‹æ ·æœ¬
        
        # ç®€åŒ–ç­–ç•¥: åˆ†åˆ«å¤„ç† good å’Œ bad
        for item in score_4_samples:
            results[dimension]['good'].append({
                'prompt': item['prompt'],
                'response': item['response'],
                'source': dataset_name,
                'original_score': 4,
                'label': 0
            })
        
        for item in score_0_samples:
            results[dimension]['bad'].append({
                'prompt': item['prompt'],
                'response': item['response'],
                'source': dataset_name,
                'original_score': 0,
                'label': 1
            })
        
        print(f"\n  {dimension}:")
        print(f"    4åˆ†æ ·æœ¬: {len(score_4_samples)}")
        print(f"    0åˆ†æ ·æœ¬: {len(score_0_samples)}")
    
    return results


def merge_helpsteer_datasets(output_dir="data/helpsteer_merged_ultra"):
    """
    æ•´åˆ HelpSteer å’Œ HelpSteer2 æ•°æ®é›†
    """
    print("=" * 70)
    print("ğŸ“¥ åŠ è½½ HelpSteer å’Œ HelpSteer2 æ•°æ®é›†")
    print("=" * 70)
    
    # åŠ è½½ HelpSteer (åŸå§‹)
    print("\næ­£åœ¨åŠ è½½ HelpSteer...")
    helpsteer1 = load_dataset("nvidia/HelpSteer", split="train")
    print(f"âœ… HelpSteer: {len(helpsteer1)} æ¡")
    
    # åŠ è½½ HelpSteer2
    print("æ­£åœ¨åŠ è½½ HelpSteer2...")
    helpsteer2 = load_dataset("nvidia/HelpSteer2", split="train")
    print(f"âœ… HelpSteer2: {len(helpsteer2)} æ¡")
    
    print(f"\næ€»æ•°æ®é‡: {len(helpsteer1) + len(helpsteer2)} æ¡")
    
    # æ„å»º ultra-extreme pairs
    print("\n" + "=" * 70)
    print("ğŸ—ï¸  æ„å»º Ultra-Extreme Pairs (0 vs 4 åˆ†)")
    print("=" * 70)
    
    print("\n--- HelpSteer ---")
    pairs_hs1 = build_ultra_extreme_pairs_from_dataset(helpsteer1, "helpsteer")
    
    print("\n--- HelpSteer2 ---")
    pairs_hs2 = build_ultra_extreme_pairs_from_dataset(helpsteer2, "helpsteer2")
    
    # åˆå¹¶æ•°æ®
    print("\n" + "=" * 70)
    print("ğŸ”— åˆå¹¶æ•°æ®")
    print("=" * 70)
    
    dimensions = ['helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity']
    merged_pairs = {}
    
    for dim in dimensions:
        merged_pairs[dim] = {
            'good': pairs_hs1[dim]['good'] + pairs_hs2[dim]['good'],
            'bad': pairs_hs1[dim]['bad'] + pairs_hs2[dim]['bad']
        }
        
        print(f"\n  {dim.upper()}:")
        print(f"    HelpSteer  - Good: {len(pairs_hs1[dim]['good']):4d}, Bad: {len(pairs_hs1[dim]['bad']):4d}")
        print(f"    HelpSteer2 - Good: {len(pairs_hs2[dim]['good']):4d}, Bad: {len(pairs_hs2[dim]['bad']):4d}")
        print(f"    åˆå¹¶å     - Good: {len(merged_pairs[dim]['good']):4d}, Bad: {len(merged_pairs[dim]['bad']):4d}")
    
    # ä¿å­˜æ•°æ®
    print("\n" + "=" * 70)
    print("ğŸ’¾ ä¿å­˜æ•´åˆæ•°æ®")
    print("=" * 70)
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    for dim in dimensions:
        good_file = output_path / f"{dim}_good_pairs.json"
        bad_file = output_path / f"{dim}_bad_pairs.json"
        
        with open(good_file, 'w', encoding='utf-8') as f:
            json.dump(merged_pairs[dim]['good'], f, indent=2, ensure_ascii=False)
        
        with open(bad_file, 'w', encoding='utf-8') as f:
            json.dump(merged_pairs[dim]['bad'], f, indent=2, ensure_ascii=False)
        
        print(f"  âœ… {dim}: {good_file.name}, {bad_file.name}")
    
    # ç»Ÿè®¡æ‘˜è¦
    print("\n" + "=" * 70)
    print("ğŸ“ˆ æ•´åˆæ•°æ®ç»Ÿè®¡æ‘˜è¦")
    print("=" * 70)
    
    print("\nç»´åº¦æ•°æ®é‡å¯¹æ¯”:")
    print("-" * 70)
    print(f"{'ç»´åº¦':<15} {'åŸHelpSteer':<15} {'æ•´åˆå':<15} {'å¢é•¿':<10}")
    print("-" * 70)
    
    # è¯»å–åŸå§‹ ultra-extreme æ•°æ®è¿›è¡Œå¯¹æ¯”
    original_dir = Path("data/helpsteer_ultra_extreme")
    for dim in dimensions:
        original_good = 0
        if (original_dir / f"{dim}_good_pairs.json").exists():
            with open(original_dir / f"{dim}_good_pairs.json", 'r') as f:
                original_good = len(json.load(f))
        
        merged_good = len(merged_pairs[dim]['good'])
        increase = merged_good - original_good
        increase_pct = (increase / original_good * 100) if original_good > 0 else 0
        
        print(f"{dim:<15} {original_good:<15} {merged_good:<15} +{increase} (+{increase_pct:.0f}%)")
    
    print("\n" + "=" * 70)
    print("âœ… HelpSteer æ•°æ®æ•´åˆå®Œæˆ!")
    print("=" * 70)
    print(f"\nè¾“å‡ºç›®å½•: {output_dir}/")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  bash train_extreme_single.sh helpfulness")
    print("  (è®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨ä½¿ç”¨ helpsteer_merged_ultra ç›®å½•ä¸‹çš„æ•°æ®)")


if __name__ == "__main__":
    merge_helpsteer_datasets()
