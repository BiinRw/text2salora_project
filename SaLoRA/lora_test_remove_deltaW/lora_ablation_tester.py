"""
LoRA â–³W å±‚çº§æ¶ˆèæµ‹è¯•æ¡†æ¶

æ”¯æŒåœ¨æ¨ç†æ—¶é€‰æ‹©æ€§ç¦ç”¨æŸäº›å±‚çš„LoRAæƒé‡æ›´æ–°,ç”¨äºéªŒè¯:
1. æ¨¡å‹å®‰å…¨è¯­ä¹‰æ˜¯å¦é›†ä¸­åœ¨ç‰¹å®šå±‚
2. è¡¨å¾å¯åˆ†æ€§(æ¢é’ˆå‡†ç¡®åº¦) vs æ¨¡å‹è¡Œä¸ºå®‰å…¨æ€§çš„å…³ç³»
"""

import torch
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm
from datetime import datetime
import logging

# é…ç½®logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LoRAAblusionTester:
    """
    LoRAæƒé‡æ¶ˆèæµ‹è¯•å™¨
    
    æ ¸å¿ƒåŠŸèƒ½:
    - ä¿å­˜åŸå§‹æ¨¡å‹æƒé‡å’ŒLoRAåº”ç”¨åçš„æƒé‡
    - æ”¯æŒè¿è¡Œæ—¶ç¦ç”¨/æ¢å¤æŸäº›å±‚çš„LoRA
    - æ”¯æŒè‡ªåŠ¨åŒ–çš„æ¶ˆèå®éªŒå¾ªç¯
    - è®°å½•å’Œåˆ†æç»“æœ
    """
    
    def __init__(
        self,
        model,
        num_layers: int = 28,
        device: str = 'cuda:0'
    ):
        """
        åˆå§‹åŒ–æ¶ˆèæµ‹è¯•å™¨
        
        Args:
            model: å·²åŠ è½½LoRAçš„LLMæ¨¡å‹
            num_layers: æ¨¡å‹å±‚æ•° (Qwen2.5é»˜è®¤ä¸º28)
            device: è®¾å¤‡ ('cuda:0' æˆ– 'cpu')
        """
        self.model = model
        self.num_layers = num_layers
        self.device = device
        
        # æƒé‡å¤‡ä»½å­˜å‚¨
        self.original_weights: Dict[int, torch.Tensor] = {}  # åŸå§‹æƒé‡
        self.lora_weights: Dict[int, torch.Tensor] = {}      # LoRAåº”ç”¨åçš„æƒé‡
        
        # å®éªŒç»“æœ
        self.ablation_results: Dict = {}
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–LoRAAblusionTester (num_layers={num_layers})")
    
    def save_original_weights(self):
        """
        ä¿å­˜æ¨¡å‹çš„åŸå§‹æƒé‡ (æœªåº”ç”¨LoRAå‰)
        
        è¿™åº”è¯¥åœ¨åŠ è½½LoRAä¹‹å‰è°ƒç”¨
        """
        logger.info("ğŸ’¾ ä¿å­˜åŸå§‹æƒé‡...")
        
        for layer_id in tqdm(range(self.num_layers), desc="ä¿å­˜åŸå§‹æƒé‡"):
            try:
                layer = self.model.model.layers[layer_id]
                # ä¿å­˜QæŠ•å½±å±‚çš„æƒé‡ (å…¶ä»–å±‚å¯ç±»ä¼¼æ‰©å±•)
                self.original_weights[layer_id] = \
                    layer.self_attn.q_proj.weight.data.clone().detach()
            except Exception as e:
                logger.warning(f"âš ï¸ ä¿å­˜ç¬¬ {layer_id} å±‚æƒé‡å¤±è´¥: {e}")
                self.original_weights[layer_id] = None
        
        logger.info(f"âœ… å·²ä¿å­˜ {len(self.original_weights)} å±‚çš„åŸå§‹æƒé‡")
    
    def save_lora_weights(self):
        """
        ä¿å­˜åº”ç”¨LoRAåçš„æƒé‡
        
        è¿™åº”è¯¥åœ¨åº”ç”¨LoRAåè°ƒç”¨
        """
        logger.info("ğŸ’¾ ä¿å­˜LoRAåº”ç”¨åçš„æƒé‡...")
        
        for layer_id in tqdm(range(self.num_layers), desc="ä¿å­˜LoRAæƒé‡"):
            try:
                layer = self.model.model.layers[layer_id]
                self.lora_weights[layer_id] = \
                    layer.self_attn.q_proj.weight.data.clone().detach()
            except Exception as e:
                logger.warning(f"âš ï¸ ä¿å­˜ç¬¬ {layer_id} å±‚LoRAæƒé‡å¤±è´¥: {e}")
                self.lora_weights[layer_id] = None
        
        logger.info(f"âœ… å·²ä¿å­˜ {len(self.lora_weights)} å±‚çš„LoRAæƒé‡")
    
    def disable_lora_on_layers(self, layer_ids: List[int]):
        """
        ç¦ç”¨æŒ‡å®šå±‚çš„LoRA (æ¢å¤åˆ°åŸå§‹æƒé‡)
        
        Args:
            layer_ids: è¦ç¦ç”¨çš„å±‚IDåˆ—è¡¨, å¦‚ [16] æˆ– [0, 8, 16, 27]
            
        Note:
            è¿™å°†æŠŠæŒ‡å®šå±‚çš„æƒé‡æ¢å¤åˆ° W_orig, ç›¸å½“äº â–³W = 0
        """
        if not layer_ids:
            logger.info("â„¹ï¸ æ— éœ€ç¦ç”¨ä»»ä½•å±‚ (baselineé…ç½®)")
            return
        
        logger.info(f"ğŸ”‡ ç¦ç”¨LoRA on layers: {layer_ids}")
        
        for layer_id in layer_ids:
            if layer_id >= self.num_layers:
                logger.warning(f"âš ï¸ å±‚ID {layer_id} è¶…å‡ºèŒƒå›´ (æœ€å¤§ {self.num_layers-1})")
                continue
            
            if self.original_weights.get(layer_id) is None:
                logger.warning(f"âš ï¸ ç¬¬ {layer_id} å±‚çš„åŸå§‹æƒé‡æœªä¿å­˜")
                continue
            
            try:
                layer = self.model.model.layers[layer_id]
                # æ¢å¤åˆ°åŸå§‹æƒé‡
                layer.self_attn.q_proj.weight.data = \
                    self.original_weights[layer_id].clone().to(self.device)
                logger.debug(f"   Layer {layer_id}: W = W_orig")
            except Exception as e:
                logger.error(f"âŒ ç¦ç”¨ç¬¬ {layer_id} å±‚å¤±è´¥: {e}")
    
    def restore_lora_on_layers(self, layer_ids: List[int]):
        """
        æ¢å¤æŒ‡å®šå±‚çš„LoRAæƒé‡
        
        Args:
            layer_ids: è¦æ¢å¤çš„å±‚IDåˆ—è¡¨
            
        Note:
            è¿™å°†æŠŠæŒ‡å®šå±‚çš„æƒé‡æ¢å¤åˆ° W_lora, ç›¸å½“äº â–³W = B @ A @ C
        """
        if not layer_ids:
            logger.info("â„¹ï¸ æ— éœ€æ¢å¤ä»»ä½•å±‚")
            return
        
        logger.info(f"ğŸ”Š æ¢å¤LoRA on layers: {layer_ids}")
        
        for layer_id in layer_ids:
            if layer_id >= self.num_layers:
                logger.warning(f"âš ï¸ å±‚ID {layer_id} è¶…å‡ºèŒƒå›´")
                continue
            
            if self.lora_weights.get(layer_id) is None:
                logger.warning(f"âš ï¸ ç¬¬ {layer_id} å±‚çš„LoRAæƒé‡æœªä¿å­˜")
                continue
            
            try:
                layer = self.model.model.layers[layer_id]
                # æ¢å¤LoRAæƒé‡
                layer.self_attn.q_proj.weight.data = \
                    self.lora_weights[layer_id].clone().to(self.device)
                logger.debug(f"   Layer {layer_id}: W = W_lora")
            except Exception as e:
                logger.error(f"âŒ æ¢å¤ç¬¬ {layer_id} å±‚å¤±è´¥: {e}")
    
    def get_lora_delta_w(self, layer_id: int) -> Optional[torch.Tensor]:
        """
        è®¡ç®—æŸä¸€å±‚çš„â–³W (LoRAæƒé‡æ›´æ–°)
        
        Args:
            layer_id: å±‚ID
            
        Returns:
            â–³W = W_lora - W_orig, shapeå–å†³äºq_projæƒé‡
        """
        if (self.original_weights.get(layer_id) is None or 
            self.lora_weights.get(layer_id) is None):
            return None
        
        delta_w = self.lora_weights[layer_id] - self.original_weights[layer_id]
        return delta_w
    
    def get_lora_importance(self, layer_id: int) -> float:
        """
        ä¼°è®¡æŸä¸€å±‚LoRAçš„é‡è¦æ€§ (åŸºäºæƒé‡æ›´æ–°çš„å¤§å°)
        
        Args:
            layer_id: å±‚ID
            
        Returns:
            FrobeniusèŒƒæ•° ||â–³W||_F
        """
        delta_w = self.get_lora_delta_w(layer_id)
        if delta_w is None:
            return 0.0
        
        return torch.norm(delta_w, p='fro').item()
    
    def run_inference(self, prompt: str, max_tokens: int = 100) -> str:
        """
        è¿è¡Œå•æ¬¡æ¨ç† (éœ€è¦ç”±è°ƒç”¨è€…å®ç°å…·ä½“é€»è¾‘)
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
            
        Note:
            è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦,å…·ä½“å®ç°å–å†³äºä½¿ç”¨çš„æ¨ç†å¼•æ“
        """
        raise NotImplementedError("éœ€è¦ç”±å­ç±»æˆ–è°ƒç”¨è€…å®ç°æ¨ç†é€»è¾‘")
    
    def prepare_ablation_configs(self) -> Dict[str, List[int]]:
        """
        å‡†å¤‡æ ‡å‡†çš„æ¶ˆèé…ç½®
        
        Returns:
            é…ç½®å­—å…¸: {'config_name': [layer_ids]}
        """
        configs = {
            'baseline': [],                          # å…¨å±‚LoRA
            'disable_layer_16': [16],               # åªç¦ç”¨16å±‚
            'disable_early_layers_0_8': list(range(0, 9)),      # ç¦ç”¨0-8
            'disable_mid_layers_8_16': list(range(8, 17)),      # ç¦ç”¨8-16
            'disable_late_layers_17_27': list(range(17, 28)),   # ç¦ç”¨17-27
        }
        return configs
    
    def run_ablation_test(
        self,
        ablation_configs: Optional[Dict[str, List[int]]] = None,
        save_results: bool = True,
        output_dir: Optional[Path] = None
    ) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒå¾ªç¯
        
        Args:
            ablation_configs: æ¶ˆèé…ç½®å­—å…¸
            save_results: æ˜¯å¦ä¿å­˜ç»“æœåˆ°JSON
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            å®éªŒç»“æœå­—å…¸
            
        Note:
            éœ€è¦ç”±è°ƒç”¨è€…æä¾›evaluate()æ–¹æ³•æ¥è®¡ç®—æŒ‡æ ‡
        """
        if ablation_configs is None:
            ablation_configs = self.prepare_ablation_configs()
        
        logger.info(f"ğŸ§ª å¼€å§‹æ¶ˆèå®éªŒ ({len(ablation_configs)} ä¸ªé…ç½®)")
        
        self.ablation_results = {}
        
        for config_name, disabled_layers in ablation_configs.items():
            logger.info(f"\n{'='*60}")
            logger.info(f"é…ç½®: {config_name}")
            logger.info(f"ç¦ç”¨å±‚: {disabled_layers if disabled_layers else '[æ— ] (baseline)'}")
            logger.info(f"{'='*60}")
            
            try:
                # åº”ç”¨æ¶ˆèé…ç½®
                self.disable_lora_on_layers(disabled_layers)
                
                # è¿™é‡Œéœ€è¦ç”±è°ƒç”¨è€…å®ç°å…·ä½“çš„è¯„ä¼°é€»è¾‘
                # ä¾‹å¦‚: harmfulness_rate, probe_accuracy = self.evaluate()
                result = {
                    'disabled_layers': disabled_layers,
                    'timestamp': datetime.now().isoformat(),
                    # éœ€è¦å¡«å……: harmfulness_rate, probe_accuracyç­‰æŒ‡æ ‡
                }
                
                self.ablation_results[config_name] = result
                logger.info(f"âœ… é…ç½® {config_name} å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ é…ç½® {config_name} å¤±è´¥: {e}")
                self.ablation_results[config_name] = {'error': str(e)}
            
            finally:
                # æ¢å¤LoRAæƒé‡
                self.restore_lora_on_layers(disabled_layers)
        
        # ä¿å­˜ç»“æœ
        if save_results and output_dir:
            self.save_results(output_dir)
        
        return self.ablation_results
    
    def save_results(self, output_dir: Path):
        """
        ä¿å­˜æ¶ˆèå®éªŒç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        results_file = output_dir / "ablation_results.json"
        
        # è½¬æ¢torchå¼ é‡ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        results_to_save = {}
        for config, result in self.ablation_results.items():
            results_to_save[config] = {
                k: v.tolist() if isinstance(v, torch.Tensor) else v
                for k, v in result.items()
            }
        
        with open(results_file, 'w') as f:
            json.dump(results_to_save, f, indent=2)
        
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    def analyze_results(self) -> Dict:
        """
        åˆ†ææ¶ˆèå®éªŒç»“æœ
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.info("\nğŸ“Š æ¶ˆèå®éªŒç»“æœåˆ†æ:")
        logger.info("="*60)
        
        analysis = {}
        
        # è®¡ç®—â–³Wçš„å¤§å° (é‡è¦æ€§ä¼°è®¡)
        logger.info("\nğŸ”¢ å„å±‚LoRAé‡è¦æ€§ (||â–³W||_F):")
        importance_scores = {}
        for layer_id in range(self.num_layers):
            importance = self.get_lora_importance(layer_id)
            importance_scores[layer_id] = importance
            if layer_id % 5 == 0:
                logger.info(f"   Layer {layer_id:2d}: {importance:.4f}")
        
        analysis['layer_importance'] = importance_scores
        
        # æ‰¾å‡ºæœ€é‡è¦çš„å±‚
        top_k = 5
        top_layers = sorted(
            importance_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        logger.info(f"\nğŸ† Top {top_k} é‡è¦å±‚:")
        for rank, (layer_id, score) in enumerate(top_layers, 1):
            logger.info(f"   {rank}. Layer {layer_id}: {score:.4f}")
        
        analysis['top_important_layers'] = [l for l, _ in top_layers]
        
        return analysis
    
    def print_summary(self):
        """æ‰“å°æ¶ˆèå®éªŒçš„æ€»ç»“"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ æ¶ˆèå®éªŒæ€»ç»“")
        logger.info("="*60)
        logger.info(f"æ¨¡å‹å±‚æ•°: {self.num_layers}")
        logger.info(f"åŸå§‹æƒé‡å¤‡ä»½: {len(self.original_weights)} å±‚")
        logger.info(f"LoRAæƒé‡å¤‡ä»½: {len(self.lora_weights)} å±‚")
        logger.info(f"æ¶ˆèé…ç½®æ•°: {len(self.ablation_results)}")
        logger.info("="*60 + "\n")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("""
    LoRAæ¶ˆèæµ‹è¯•æ¡†æ¶ä½¿ç”¨ç¤ºä¾‹:
    
    ```python
    # 1. åŠ è½½æ¨¡å‹ (ä½¿ç”¨ç°æœ‰çš„load_model_with_abcå‡½æ•°)
    model, tokenizer, _ = load_model_with_abc(
        model_path, lora_path, subspace_dir, dimension
    )
    
    # 2. åˆ›å»ºæ¶ˆèæµ‹è¯•å™¨
    tester = LoRAAblusionTester(model, num_layers=28)
    
    # 3. ä¿å­˜æƒé‡å¤‡ä»½
    tester.save_original_weights()  # æ³¨æ„: åº”åœ¨åº”ç”¨LoRAå‰è°ƒç”¨
    model = load_model_with_abc(...)  # åº”ç”¨LoRA
    tester.save_lora_weights()  # åº”ç”¨LoRAåè°ƒç”¨
    
    # 4. å®šä¹‰æ¶ˆèé…ç½®
    ablation_configs = {
        'baseline': [],
        'disable_layer_16': [16],
        'disable_layers_0_8': list(range(0, 9)),
    }
    
    # 5. è¿è¡Œæ¶ˆèå®éªŒ (éœ€è¦å®ç°evaluateæ–¹æ³•)
    results = tester.run_ablation_test(ablation_configs)
    
    # 6. åˆ†æç»“æœ
    analysis = tester.analyze_results()
    ```
    """)
